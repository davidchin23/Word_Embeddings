{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# $$Word Embeddings$$\n",
        "\n",
        "In this notebook, we try to build our very own Word Embedding model from scratch."
      ],
      "metadata": {
        "id": "COJ_MNBhq27x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N80_a7_YqvzN"
      },
      "outputs": [],
      "source": [
        "# installation of necessary packages\n",
        "!pip install datasets\n",
        "!pip install spacy torch wordninja contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing of all necessary packages for this notebook, as well as connecting to Google Drive, for easier access to saved datasets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import wordninja\n",
        "import contractions\n",
        "from collections import defaultdict, Counter\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, top_k_accuracy_score, v_measure_score, adjusted_rand_score, classification_report\n",
        "from sklearn.datasets import load_files\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jCT-n0WEq2H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get our raw data from a dataset called bookcorpus on Huggingface. This dataset has sentences from 7185 unique books, with a wide range of genres, which can help our model to learn more semantics. The dataset includes preamble, which might be helpful, as those sections do not follow a certain theme / style of writing, and it is more conventional, thus giving our model a wider variety of data to train on."
      ],
      "metadata": {
        "id": "_pyPG89wrC5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing of dataset\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/WE_book_corpus_data.csv')\n",
        "print(df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "gyixePV7q6Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the size of the dataset and our computational constraints, we subset 1% of the dataset out."
      ],
      "metadata": {
        "id": "H9P1vFFgrLzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = df.sample(frac = 0.01, random_state = 42)\n",
        "df_data.to_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset.csv', index = False)"
      ],
      "metadata": {
        "id": "7o1L2bQ5rK52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.read_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset.csv')"
      ],
      "metadata": {
        "id": "YWPXWnIorQbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in this cell, we conduct preprocessing on our text\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+n't\\b\", r\"\\1n't\", text)  # Handles \"is n't\" → \"isn't\"\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+'ll\\b\", r\"\\1'll\", text)  # Handles \"I 'll\" → \"I'll\"\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+'ve\\b\", r\"\\1've\", text)  # Handles \"I 've\" → \"I've\"\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+'re\\b\", r\"\\1're\", text)  # Handles \"you 're\" → \"you're\"\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+'m\\b\", r\"\\1'm\", text)    # Handles \"I 'm\" → \"I'm\"\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+'d\\b\", r\"\\1'd\", text)    # Handles \"I 'd\" → \"I'd\"\n",
        "    text = re.sub(r\"\\b(\\w+)\\s+'s\\b\", r\"\\1's\", text)    # Handles \"it 's\" → \"it's\"\n",
        "    text = contractions.fix(text)\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = ' '.join(wordninja.split(text)) # Split concatenated words\n",
        "    lemmatized_tokens = [token.lemma_ for token in nlp(text)]  # Lemmatize\n",
        "    lemma_text = re.sub(r'[^a-zA-Z\\s]', '', ' '.join(lemmatized_tokens)) # Join tokens and clean the full string\n",
        "    return lemma_text\n",
        "\n",
        "tqdm.pandas(desc=\"Preprocessing filtered and limited text\")\n",
        "df_data['processed_text'] = df_data['text'].progress_apply(preprocess_text)\n",
        "\n",
        "df_data.head()"
      ],
      "metadata": {
        "id": "hgUrTB_XrRwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.to_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset_processed.csv', index = False)"
      ],
      "metadata": {
        "id": "fd6oJQwMrRpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can further sample our data to get 50% and 25% of the full dataset respectively. The intention of this is to compare the performance of the models when trained on different sizes of the dataset, thus we have three different sizes of the dataset: 100%, 50% and 25%."
      ],
      "metadata": {
        "id": "UyMgTL_irbFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.read_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset_processed.csv')\n",
        "\n",
        "df_data = df_data[df_data['processed_text'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "df_quarter = df_data.sample(frac = 0.25, random_state = 42)\n",
        "df_half = df_data.sample(frac = 0.5, random_state = 42)\n",
        "df_quarter.to_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset_processed_quarter.csv', index = False)\n",
        "df_half.to_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset_processed_half.csv', index = False)\n",
        "df_data.to_csv('/content/drive/MyDrive/WE_book_corpus_final_dataset_processed.csv', index = False)"
      ],
      "metadata": {
        "id": "7EoR5efQrRif"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}