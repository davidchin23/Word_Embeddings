{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10243953,"sourceType":"datasetVersion","datasetId":6335246},{"sourceId":10250289,"sourceType":"datasetVersion","datasetId":6340021},{"sourceId":10250293,"sourceType":"datasetVersion","datasetId":6340024},{"sourceId":211326,"sourceType":"modelInstanceVersion","modelInstanceId":180177,"modelId":202439}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets spacy wordninja contractions\n!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:17:34.059545Z","iopub.execute_input":"2024-12-27T06:17:34.059842Z","iopub.status.idle":"2024-12-27T06:17:38.365609Z","shell.execute_reply.started":"2024-12-27T06:17:34.059813Z","shell.execute_reply":"2024-12-27T06:17:38.364778Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport ast\nfrom tqdm import tqdm\nfrom google.colab import drive\nfrom datasets import load_dataset\nimport os\nimport re\nimport requests\nimport tarfile\nimport random\nimport spacy\nimport wordninja\nimport contractions\nfrom collections import defaultdict, Counter\nimport itertools\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, top_k_accuracy_score, v_measure_score, adjusted_rand_score, classification_report\nfrom sklearn.datasets import load_files\nfrom scipy.spatial.distance import cosine\nfrom scipy.stats import spearmanr\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:09:36.889194Z","iopub.execute_input":"2025-01-21T11:09:36.889582Z","iopub.status.idle":"2025-01-21T11:09:40.408316Z","shell.execute_reply.started":"2025-01-21T11:09:36.889557Z","shell.execute_reply":"2025-01-21T11:09:40.407420Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def build_vocab(df, text_column):\n  \"\"\"\n  This function takes in a dataframe and the column where the text is stored.\n  It returns a unique set of key-value pairs, where the key is the word, \n  and the value is its unique corresponding numerical value.\n  \"\"\"\n  unique_words = {word for text in df[text_column] for word in text.split()}\n  return {word: idx for idx, word in enumerate(sorted(unique_words))}\n\ndef create_context_target_pairs(df, text_column, window_size = 5):\n  \"\"\"\n  This function intends to formulate the context-target pairs across the \n  entire text column of a dataframe, with the window size having a default\n  value of 5. It returns a list of tuples, where each tuple is represented \n  by a list of context words, followed by the corresponding target word.\n  \"\"\"\n  pairs = []\n  for text in df[text_column]:\n    tokens = text.split()\n    for i, target in enumerate(tokens):\n      start = max(0, i - window_size)\n      end = min(len(tokens), i + window_size + 1)\n      context_words = [tokens[j] for j in range(start, end) if j != i]\n      pairs.append((context_words, target))\n  return pairs\n\ndef encode_pairs(pairs, vocab):\n  \"\"\"\n  This function takes in context-target pairs that is produced using \n  create_context_target_pairs, and encodes all the words into its unique\n  integer representation, which will be used as input for our model later on.\n  \"\"\"\n  encoded_pairs = []\n  for context, target in pairs:\n    encoded_context = [vocab[word] for word in context]\n    encoded_target = vocab[target]\n    encoded_pairs.append((encoded_context, encoded_target))\n  return encoded_pairs\n\nclass CBOWDataset(Dataset):\n  def __init__(self, pairs, vocab_size, window_size):\n    self.pairs = pairs\n    self.vocab_size = vocab_size\n    self.window_size = window_size\n\n  def __len__(self):\n    return len(self.pairs)\n\n  def __getitem__(self, idx):\n    context, target = self.pairs[idx]\n    padded = context + [0] * (2 * self.window_size - len(context)) #padding is necessary to ensure that all input tensors have the same dimensions\n    return torch.tensor(padded, dtype = torch.long), torch.tensor(target, dtype = torch.long)\n\nclass CBOWModel(nn.Module):\n  def __init__(self, vocab_size, embedding_dim):\n    super(CBOWModel, self).__init__()\n    self.embedding_dim = embedding_dim\n    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n    self.softmax = nn.Softmax(dim = 1) #use softmax to convert logits into probabilities\n    self.fc = nn.Linear(embedding_dim, vocab_size)\n\n  def forward(self, context):\n    embedded = self.embeddings(context)  # (batch_size, context_size, embedding_dim)\n    avg_embedding = embedded.mean(dim=1)  # (batch_size, embedding_dim)\n    out = self.fc(avg_embedding)  # (batch_size, vocab_size)\n    return self.softmax(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:09:43.295312Z","iopub.execute_input":"2025-01-21T11:09:43.295790Z","iopub.status.idle":"2025-01-21T11:09:43.305152Z","shell.execute_reply.started":"2025-01-21T11:09:43.295765Z","shell.execute_reply":"2025-01-21T11:09:43.304137Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df_quarter = pd.read_csv('/kaggle/input/we-final-quarter/WE_book_corpus_final_dataset_processed_quarter.csv')\ndf_quarter.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:09:48.960219Z","iopub.execute_input":"2025-01-21T11:09:48.960509Z","iopub.status.idle":"2025-01-21T11:09:49.490679Z","shell.execute_reply.started":"2025-01-21T11:09:48.960489Z","shell.execute_reply":"2025-01-21T11:09:49.489877Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0                 we were all young once , he said .   \n1  it was as she washing her hands in the sink af...   \n2  for the illiterate soldiers , it is a trip bac...   \n3  names , characters , places , brands , media ,...   \n4                          it was all a masquerade .   \n\n                                      processed_text  \n0                        we be all young once he say  \n1  it be as she wash her hand in the sink after u...  \n2  for the illiterate soldier it be a trip back t...  \n3  name character place brand medium and incident...  \n4                             it be all a masquerade  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>we were all young once , he said .</td>\n      <td>we be all young once he say</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>it was as she washing her hands in the sink af...</td>\n      <td>it be as she wash her hand in the sink after u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>for the illiterate soldiers , it is a trip bac...</td>\n      <td>for the illiterate soldier it be a trip back t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>names , characters , places , brands , media ,...</td>\n      <td>name character place brand medium and incident...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>it was all a masquerade .</td>\n      <td>it be all a masquerade</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"vocab_quarter = build_vocab(df_quarter, 'processed_text')\npairs_quarter = create_context_target_pairs(df_quarter, 'processed_text', 5)\nencoded_pairs_quarter = encode_pairs(pairs_quarter, vocab_quarter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:09:51.623501Z","iopub.execute_input":"2025-01-21T11:09:51.623790Z","iopub.status.idle":"2025-01-21T11:10:03.868700Z","shell.execute_reply.started":"2025-01-21T11:09:51.623770Z","shell.execute_reply":"2025-01-21T11:10:03.867796Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#use gpu if possible to accelerate the speed of model training and evaluation\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:10:12.207580Z","iopub.execute_input":"2025-01-21T11:10:12.207857Z","iopub.status.idle":"2025-01-21T11:10:12.212558Z","shell.execute_reply.started":"2025-01-21T11:10:12.207835Z","shell.execute_reply":"2025-01-21T11:10:12.211659Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def calculate_metrics(output, target, top_ks = [3, 5, 10]):\n  \"\"\"\n  This function takes in the entire output vector and the true target word,\n  and returns accuracies for top-k accuracies where k = 3, 5 and 10 \n  respectively. Eg. if the true target word is in the top 10 highest\n  probabilities for k = 10, this will count as a true positive towards \n  the computation of accuracy.\n  \"\"\"\n  top_k_accuracies = {}\n  for k in top_ks:\n    _, top_k_preds = output.topk(k, dim = 1)\n    top_k_acc = (top_k_preds == target.view(-1, 1)).any(dim = 1).float().mean().item()\n    top_k_accuracies[f\"top_{k}_accuracy\"] = top_k_acc\n  return top_k_accuracies\n\ndef tune_hyperparams_quarter(param_grid, train_dataset, val_dataset):\n  \"\"\"\n  This function takes in the parameter grid that we want to train our model \n  on, along with the respective train and validation datasets. The train and \n  validation loop is nested in this function.\n  \"\"\"     \n  results = []\n  for params in param_grid:\n    start_time = time.time()\n    batch_size = params['batch_size']\n    embedding_dim = params['embedding_dim']\n    learning_rate = params['learning_rate']\n    num_epochs = params['num_epochs']\n\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n\n    model = CBOWModel(len(vocab_quarter), embedding_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n      model.train()\n      total_loss = 0.0\n      for context, target in train_loader:\n        context, target = context.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(context)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n      avg_train_loss = total_loss / len(train_loader)\n      train_losses.append(avg_train_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Training Loss: {avg_train_loss:.4f}\")\n\n      model.eval()\n      val_loss = 0.0\n      val_accuracy = 0.0\n      val_top_3_accuracy = 0.0\n      val_top_5_accuracy = 0.0\n      val_top_10_accuracy = 0.0\n      with torch.no_grad():\n        for context, target in val_loader:\n          context, target = context.to(device), target.to(device)\n          output = model(context)\n          loss = criterion(output, target)\n          val_loss += loss.item()\n\n          predictions = torch.argmax(output, dim = 1)\n          val_accuracy += accuracy_score(target.cpu(), predictions.cpu())\n\n          top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n          val_top_3_accuracy += top_k_accuracies['top_3_accuracy']\n          val_top_5_accuracy += top_k_accuracies['top_5_accuracy']\n          val_top_10_accuracy += top_k_accuracies['top_10_accuracy']\n\n      avg_val_loss = val_loss / len(val_loader)\n      avg_val_accuracy = val_accuracy / len(val_loader)\n      avg_top_3_accuracy = val_top_3_accuracy / len(val_loader)\n      avg_top_5_accuracy = val_top_5_accuracy / len(val_loader)\n      avg_top_10_accuracy = val_top_10_accuracy / len(val_loader)\n      val_losses.append(avg_val_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Validation Loss: {avg_val_loss:.4f}\")\n      print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n      print(f\"Top-3 Accuracy: {avg_top_3_accuracy:.4f}\")\n      print(f\"Top-5 Accuracy: {avg_top_5_accuracy:.4f}\")\n      print(f\"Top-10 Accuracy: {avg_top_10_accuracy:.4f}\")\n\n      if abs(best_val_loss - avg_val_loss) < 1e-3:\n        print(\"Early stopping triggered\") #early stopping is triggered to prevent misinformation and to improve computational time\n        break\n      else:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), \"best_cbow_model_quarter.pth\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    results.append({\n        'params': params,\n        'val_accuracy': avg_val_accuracy,\n        'val_top_3_accuracy': avg_top_3_accuracy,\n        'val_top_5_accuracy': avg_top_5_accuracy,\n        'val_top_10_accuracy': avg_top_10_accuracy,\n        'total time': total_time\n    })\n  return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T14:58:19.665088Z","iopub.execute_input":"2025-01-20T14:58:19.665404Z","iopub.status.idle":"2025-01-20T14:58:19.676965Z","shell.execute_reply.started":"2025-01-20T14:58:19.665378Z","shell.execute_reply":"2025-01-20T14:58:19.675894Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_pairs_quarter, val_test_pairs_quarter = train_test_split(encoded_pairs_quarter, test_size = 0.70, random_state = 42)\nval_pairs_quarter, test_pairs_quarter = train_test_split(val_test_pairs_quarter, test_size = 0.5, random_state = 42)\n\ntrain_dataset_quarter = CBOWDataset(train_pairs_quarter, len(vocab_quarter), 5)\nval_dataset_quarter = CBOWDataset(val_pairs_quarter, len(vocab_quarter), 5)\ntest_dataset_quarter = CBOWDataset(test_pairs_quarter, len(vocab_quarter), 5)\n\nparam_grid = {\n    'batch_size': [64, 128],\n    'embedding_dim': [50, 100],\n    'learning_rate': [0.001, 0.01],\n    'num_epochs': [10, 20]\n}\n\nparam_combs = list(itertools.product(\n    param_grid['batch_size'],\n    param_grid['embedding_dim'],\n    param_grid['learning_rate'],\n    param_grid['num_epochs']\n))\n\nparam_grid_dicts = [\n    {'batch_size': bs, 'embedding_dim': ed, 'learning_rate': lr, 'num_epochs': ne}\n    for bs, ed, lr, ne in param_combs\n]\n\nresults_quarter = tune_hyperparams_quarter(param_grid_dicts, train_dataset_quarter, val_dataset_quarter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T00:45:57.613924Z","iopub.execute_input":"2024-12-20T00:45:57.614218Z","iopub.status.idle":"2024-12-20T02:14:43.487984Z","shell.execute_reply.started":"2024-12-20T00:45:57.614198Z","shell.execute_reply":"2024-12-20T02:14:43.486954Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:\nTraining Loss: 10.2977\nEpoch 1:\nValidation Loss: 10.2839\nValidation Accuracy: 0.0801\nTop-3 Accuracy: 0.1309\nTop-5 Accuracy: 0.1706\nTop-10 Accuracy: 0.2570\nEpoch 2:\nTraining Loss: 10.2790\nEpoch 2:\nValidation Loss: 10.2781\nValidation Accuracy: 0.0850\nTop-3 Accuracy: 0.1322\nTop-5 Accuracy: 0.1699\nTop-10 Accuracy: 0.2614\nEpoch 3:\nTraining Loss: 10.2735\nEpoch 3:\nValidation Loss: 10.2758\nValidation Accuracy: 0.0867\nTop-3 Accuracy: 0.1325\nTop-5 Accuracy: 0.1701\nTop-10 Accuracy: 0.2587\nEpoch 4:\nTraining Loss: 10.2700\nEpoch 4:\nValidation Loss: 10.2744\nValidation Accuracy: 0.0879\nTop-3 Accuracy: 0.1339\nTop-5 Accuracy: 0.1704\nTop-10 Accuracy: 0.2585\nEpoch 5:\nTraining Loss: 10.2673\nEpoch 5:\nValidation Loss: 10.2729\nValidation Accuracy: 0.0894\nTop-3 Accuracy: 0.1451\nTop-5 Accuracy: 0.1709\nTop-10 Accuracy: 0.2582\nEpoch 6:\nTraining Loss: 10.2638\nEpoch 6:\nValidation Loss: 10.2702\nValidation Accuracy: 0.0924\nTop-3 Accuracy: 0.1465\nTop-5 Accuracy: 0.1716\nTop-10 Accuracy: 0.2572\nEpoch 7:\nTraining Loss: 10.2596\nEpoch 7:\nValidation Loss: 10.2682\nValidation Accuracy: 0.0943\nTop-3 Accuracy: 0.1473\nTop-5 Accuracy: 0.1726\nTop-10 Accuracy: 0.2559\nEpoch 8:\nTraining Loss: 10.2563\nEpoch 8:\nValidation Loss: 10.2672\nValidation Accuracy: 0.0950\nTop-3 Accuracy: 0.1479\nTop-5 Accuracy: 0.1736\nTop-10 Accuracy: 0.2545\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2987\nEpoch 1:\nValidation Loss: 10.2845\nValidation Accuracy: 0.0799\nTop-3 Accuracy: 0.1308\nTop-5 Accuracy: 0.1698\nTop-10 Accuracy: 0.2653\nEpoch 2:\nTraining Loss: 10.2795\nEpoch 2:\nValidation Loss: 10.2782\nValidation Accuracy: 0.0850\nTop-3 Accuracy: 0.1399\nTop-5 Accuracy: 0.1699\nTop-10 Accuracy: 0.2597\nEpoch 3:\nTraining Loss: 10.2726\nEpoch 3:\nValidation Loss: 10.2732\nValidation Accuracy: 0.0904\nTop-3 Accuracy: 0.1457\nTop-5 Accuracy: 0.1703\nTop-10 Accuracy: 0.2520\nEpoch 4:\nTraining Loss: 10.2661\nEpoch 4:\nValidation Loss: 10.2698\nValidation Accuracy: 0.0934\nTop-3 Accuracy: 0.1470\nTop-5 Accuracy: 0.1708\nTop-10 Accuracy: 0.2501\nEpoch 5:\nTraining Loss: 10.2614\nEpoch 5:\nValidation Loss: 10.2682\nValidation Accuracy: 0.0948\nTop-3 Accuracy: 0.1477\nTop-5 Accuracy: 0.1716\nTop-10 Accuracy: 0.2491\nEpoch 6:\nTraining Loss: 10.2580\nEpoch 6:\nValidation Loss: 10.2670\nValidation Accuracy: 0.0957\nTop-3 Accuracy: 0.1482\nTop-5 Accuracy: 0.1726\nTop-10 Accuracy: 0.2470\nEpoch 7:\nTraining Loss: 10.2553\nEpoch 7:\nValidation Loss: 10.2664\nValidation Accuracy: 0.0960\nTop-3 Accuracy: 0.1485\nTop-5 Accuracy: 0.1737\nTop-10 Accuracy: 0.2468\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2872\nEpoch 1:\nValidation Loss: 10.2804\nValidation Accuracy: 0.0806\nTop-3 Accuracy: 0.1267\nTop-5 Accuracy: 0.1772\nTop-10 Accuracy: 0.2557\nEpoch 2:\nTraining Loss: 10.2686\nEpoch 2:\nValidation Loss: 10.2697\nValidation Accuracy: 0.0913\nTop-3 Accuracy: 0.1470\nTop-5 Accuracy: 0.1807\nTop-10 Accuracy: 0.2500\nEpoch 3:\nTraining Loss: 10.2599\nEpoch 3:\nValidation Loss: 10.2671\nValidation Accuracy: 0.0939\nTop-3 Accuracy: 0.1510\nTop-5 Accuracy: 0.1817\nTop-10 Accuracy: 0.2319\nEpoch 4:\nTraining Loss: 10.2546\nEpoch 4:\nValidation Loss: 10.2650\nValidation Accuracy: 0.0960\nTop-3 Accuracy: 0.1558\nTop-5 Accuracy: 0.1861\nTop-10 Accuracy: 0.2239\nEpoch 5:\nTraining Loss: 10.2496\nEpoch 5:\nValidation Loss: 10.2626\nValidation Accuracy: 0.0984\nTop-3 Accuracy: 0.1599\nTop-5 Accuracy: 0.1863\nTop-10 Accuracy: 0.2191\nEpoch 6:\nTraining Loss: 10.2454\nEpoch 6:\nValidation Loss: 10.2616\nValidation Accuracy: 0.0992\nTop-3 Accuracy: 0.1670\nTop-5 Accuracy: 0.1978\nTop-10 Accuracy: 0.2229\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2920\nEpoch 1:\nValidation Loss: 10.2845\nValidation Accuracy: 0.0765\nTop-3 Accuracy: 0.1236\nTop-5 Accuracy: 0.1725\nTop-10 Accuracy: 0.2607\nEpoch 2:\nTraining Loss: 10.2784\nEpoch 2:\nValidation Loss: 10.2773\nValidation Accuracy: 0.0836\nTop-3 Accuracy: 0.1354\nTop-5 Accuracy: 0.1672\nTop-10 Accuracy: 0.2483\nEpoch 3:\nTraining Loss: 10.2677\nEpoch 3:\nValidation Loss: 10.2732\nValidation Accuracy: 0.0877\nTop-3 Accuracy: 0.1409\nTop-5 Accuracy: 0.1624\nTop-10 Accuracy: 0.2223\nEpoch 4:\nTraining Loss: 10.2619\nEpoch 4:\nValidation Loss: 10.2725\nValidation Accuracy: 0.0883\nTop-3 Accuracy: 0.1405\nTop-5 Accuracy: 0.1667\nTop-10 Accuracy: 0.2317\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2973\nEpoch 1:\nValidation Loss: 10.2853\nValidation Accuracy: 0.0783\nTop-3 Accuracy: 0.1231\nTop-5 Accuracy: 0.1743\nTop-10 Accuracy: 0.2548\nEpoch 2:\nTraining Loss: 10.2800\nEpoch 2:\nValidation Loss: 10.2804\nValidation Accuracy: 0.0822\nTop-3 Accuracy: 0.1249\nTop-5 Accuracy: 0.1539\nTop-10 Accuracy: 0.2625\nEpoch 3:\nTraining Loss: 10.2742\nEpoch 3:\nValidation Loss: 10.2777\nValidation Accuracy: 0.0846\nTop-3 Accuracy: 0.1321\nTop-5 Accuracy: 0.1541\nTop-10 Accuracy: 0.2620\nEpoch 4:\nTraining Loss: 10.2696\nEpoch 4:\nValidation Loss: 10.2758\nValidation Accuracy: 0.0865\nTop-3 Accuracy: 0.1330\nTop-5 Accuracy: 0.1546\nTop-10 Accuracy: 0.2593\nEpoch 5:\nTraining Loss: 10.2658\nEpoch 5:\nValidation Loss: 10.2747\nValidation Accuracy: 0.0874\nTop-3 Accuracy: 0.1339\nTop-5 Accuracy: 0.1555\nTop-10 Accuracy: 0.2581\nEpoch 6:\nTraining Loss: 10.2625\nEpoch 6:\nValidation Loss: 10.2738\nValidation Accuracy: 0.0881\nTop-3 Accuracy: 0.1342\nTop-5 Accuracy: 0.1564\nTop-10 Accuracy: 0.2574\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2921\nEpoch 1:\nValidation Loss: 10.2777\nValidation Accuracy: 0.0871\nTop-3 Accuracy: 0.1480\nTop-5 Accuracy: 0.1780\nTop-10 Accuracy: 0.2596\nEpoch 2:\nTraining Loss: 10.2709\nEpoch 2:\nValidation Loss: 10.2700\nValidation Accuracy: 0.0939\nTop-3 Accuracy: 0.1527\nTop-5 Accuracy: 0.1783\nTop-10 Accuracy: 0.2630\nEpoch 3:\nTraining Loss: 10.2621\nEpoch 3:\nValidation Loss: 10.2658\nValidation Accuracy: 0.0974\nTop-3 Accuracy: 0.1552\nTop-5 Accuracy: 0.1793\nTop-10 Accuracy: 0.2625\nEpoch 4:\nTraining Loss: 10.2562\nEpoch 4:\nValidation Loss: 10.2637\nValidation Accuracy: 0.0992\nTop-3 Accuracy: 0.1560\nTop-5 Accuracy: 0.1794\nTop-10 Accuracy: 0.2574\nEpoch 5:\nTraining Loss: 10.2519\nEpoch 5:\nValidation Loss: 10.2626\nValidation Accuracy: 0.1000\nTop-3 Accuracy: 0.1566\nTop-5 Accuracy: 0.1801\nTop-10 Accuracy: 0.2541\nEpoch 6:\nTraining Loss: 10.2484\nEpoch 6:\nValidation Loss: 10.2619\nValidation Accuracy: 0.1004\nTop-3 Accuracy: 0.1570\nTop-5 Accuracy: 0.1815\nTop-10 Accuracy: 0.2536\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2911\nEpoch 1:\nValidation Loss: 10.2841\nValidation Accuracy: 0.0767\nTop-3 Accuracy: 0.1380\nTop-5 Accuracy: 0.1814\nTop-10 Accuracy: 0.2532\nEpoch 2:\nTraining Loss: 10.2710\nEpoch 2:\nValidation Loss: 10.2710\nValidation Accuracy: 0.0898\nTop-3 Accuracy: 0.1491\nTop-5 Accuracy: 0.1803\nTop-10 Accuracy: 0.2467\nEpoch 3:\nTraining Loss: 10.2637\nEpoch 3:\nValidation Loss: 10.2693\nValidation Accuracy: 0.0916\nTop-3 Accuracy: 0.1483\nTop-5 Accuracy: 0.1790\nTop-10 Accuracy: 0.2341\nEpoch 4:\nTraining Loss: 10.2609\nEpoch 4:\nValidation Loss: 10.2699\nValidation Accuracy: 0.0909\nTop-3 Accuracy: 0.1496\nTop-5 Accuracy: 0.1772\nTop-10 Accuracy: 0.2223\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.3112\nEpoch 1:\nValidation Loss: 10.3090\nValidation Accuracy: 0.0518\nTop-3 Accuracy: 0.1222\nTop-5 Accuracy: 0.1752\nTop-10 Accuracy: 0.2578\nEpoch 2:\nTraining Loss: 10.2922\nEpoch 2:\nValidation Loss: 10.2898\nValidation Accuracy: 0.0710\nTop-3 Accuracy: 0.1258\nTop-5 Accuracy: 0.1756\nTop-10 Accuracy: 0.2423\nEpoch 3:\nTraining Loss: 10.2814\nEpoch 3:\nValidation Loss: 10.2816\nValidation Accuracy: 0.0793\nTop-3 Accuracy: 0.1344\nTop-5 Accuracy: 0.1675\nTop-10 Accuracy: 0.2132\nEpoch 4:\nTraining Loss: 10.2734\nEpoch 4:\nValidation Loss: 10.2770\nValidation Accuracy: 0.0838\nTop-3 Accuracy: 0.1370\nTop-5 Accuracy: 0.1627\nTop-10 Accuracy: 0.2086\nEpoch 5:\nTraining Loss: 10.2693\nEpoch 5:\nValidation Loss: 10.2762\nValidation Accuracy: 0.0846\nTop-3 Accuracy: 0.1372\nTop-5 Accuracy: 0.1559\nTop-10 Accuracy: 0.1890\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.3028\nEpoch 1:\nValidation Loss: 10.2872\nValidation Accuracy: 0.0782\nTop-3 Accuracy: 0.1302\nTop-5 Accuracy: 0.1708\nTop-10 Accuracy: 0.2582\nEpoch 2:\nTraining Loss: 10.2820\nEpoch 2:\nValidation Loss: 10.2805\nValidation Accuracy: 0.0833\nTop-3 Accuracy: 0.1320\nTop-5 Accuracy: 0.1704\nTop-10 Accuracy: 0.2571\nEpoch 3:\nTraining Loss: 10.2757\nEpoch 3:\nValidation Loss: 10.2768\nValidation Accuracy: 0.0867\nTop-3 Accuracy: 0.1441\nTop-5 Accuracy: 0.1707\nTop-10 Accuracy: 0.2581\nEpoch 4:\nTraining Loss: 10.2707\nEpoch 4:\nValidation Loss: 10.2733\nValidation Accuracy: 0.0903\nTop-3 Accuracy: 0.1457\nTop-5 Accuracy: 0.1711\nTop-10 Accuracy: 0.2595\nEpoch 5:\nTraining Loss: 10.2658\nEpoch 5:\nValidation Loss: 10.2707\nValidation Accuracy: 0.0926\nTop-3 Accuracy: 0.1468\nTop-5 Accuracy: 0.1718\nTop-10 Accuracy: 0.2605\nEpoch 6:\nTraining Loss: 10.2618\nEpoch 6:\nValidation Loss: 10.2690\nValidation Accuracy: 0.0940\nTop-3 Accuracy: 0.1476\nTop-5 Accuracy: 0.1754\nTop-10 Accuracy: 0.2590\nEpoch 7:\nTraining Loss: 10.2584\nEpoch 7:\nValidation Loss: 10.2672\nValidation Accuracy: 0.0957\nTop-3 Accuracy: 0.1535\nTop-5 Accuracy: 0.1777\nTop-10 Accuracy: 0.2567\nEpoch 8:\nTraining Loss: 10.2551\nEpoch 8:\nValidation Loss: 10.2658\nValidation Accuracy: 0.0971\nTop-3 Accuracy: 0.1543\nTop-5 Accuracy: 0.1781\nTop-10 Accuracy: 0.2559\nEpoch 9:\nTraining Loss: 10.2522\nEpoch 9:\nValidation Loss: 10.2647\nValidation Accuracy: 0.0981\nTop-3 Accuracy: 0.1550\nTop-5 Accuracy: 0.1787\nTop-10 Accuracy: 0.2557\nEpoch 10:\nTraining Loss: 10.2497\nEpoch 10:\nValidation Loss: 10.2642\nValidation Accuracy: 0.0985\nTop-3 Accuracy: 0.1551\nTop-5 Accuracy: 0.1789\nTop-10 Accuracy: 0.2553\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.3068\nEpoch 1:\nValidation Loss: 10.2952\nValidation Accuracy: 0.0680\nTop-3 Accuracy: 0.1083\nTop-5 Accuracy: 0.1617\nTop-10 Accuracy: 0.2599\nEpoch 2:\nTraining Loss: 10.2914\nEpoch 2:\nValidation Loss: 10.2895\nValidation Accuracy: 0.0736\nTop-3 Accuracy: 0.1163\nTop-5 Accuracy: 0.1656\nTop-10 Accuracy: 0.2589\nEpoch 3:\nTraining Loss: 10.2855\nEpoch 3:\nValidation Loss: 10.2863\nValidation Accuracy: 0.0765\nTop-3 Accuracy: 0.1168\nTop-5 Accuracy: 0.1549\nTop-10 Accuracy: 0.2595\nEpoch 4:\nTraining Loss: 10.2816\nEpoch 4:\nValidation Loss: 10.2844\nValidation Accuracy: 0.0781\nTop-3 Accuracy: 0.1178\nTop-5 Accuracy: 0.1538\nTop-10 Accuracy: 0.2621\nEpoch 5:\nTraining Loss: 10.2786\nEpoch 5:\nValidation Loss: 10.2833\nValidation Accuracy: 0.0791\nTop-3 Accuracy: 0.1184\nTop-5 Accuracy: 0.1542\nTop-10 Accuracy: 0.2629\nEpoch 6:\nTraining Loss: 10.2753\nEpoch 6:\nValidation Loss: 10.2802\nValidation Accuracy: 0.0827\nTop-3 Accuracy: 0.1319\nTop-5 Accuracy: 0.1546\nTop-10 Accuracy: 0.2621\nEpoch 7:\nTraining Loss: 10.2708\nEpoch 7:\nValidation Loss: 10.2776\nValidation Accuracy: 0.0853\nTop-3 Accuracy: 0.1329\nTop-5 Accuracy: 0.1548\nTop-10 Accuracy: 0.2617\nEpoch 8:\nTraining Loss: 10.2669\nEpoch 8:\nValidation Loss: 10.2758\nValidation Accuracy: 0.0869\nTop-3 Accuracy: 0.1337\nTop-5 Accuracy: 0.1554\nTop-10 Accuracy: 0.2615\nEpoch 9:\nTraining Loss: 10.2641\nEpoch 9:\nValidation Loss: 10.2750\nValidation Accuracy: 0.0876\nTop-3 Accuracy: 0.1340\nTop-5 Accuracy: 0.1559\nTop-10 Accuracy: 0.2609\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2874\nEpoch 1:\nValidation Loss: 10.2717\nValidation Accuracy: 0.0898\nTop-3 Accuracy: 0.1449\nTop-5 Accuracy: 0.1762\nTop-10 Accuracy: 0.2607\nEpoch 2:\nTraining Loss: 10.2639\nEpoch 2:\nValidation Loss: 10.2680\nValidation Accuracy: 0.0932\nTop-3 Accuracy: 0.1486\nTop-5 Accuracy: 0.1825\nTop-10 Accuracy: 0.2626\nEpoch 3:\nTraining Loss: 10.2568\nEpoch 3:\nValidation Loss: 10.2655\nValidation Accuracy: 0.0956\nTop-3 Accuracy: 0.1583\nTop-5 Accuracy: 0.1885\nTop-10 Accuracy: 0.2621\nEpoch 4:\nTraining Loss: 10.2499\nEpoch 4:\nValidation Loss: 10.2623\nValidation Accuracy: 0.0989\nTop-3 Accuracy: 0.1611\nTop-5 Accuracy: 0.1882\nTop-10 Accuracy: 0.2459\nEpoch 5:\nTraining Loss: 10.2442\nEpoch 5:\nValidation Loss: 10.2614\nValidation Accuracy: 0.0995\nTop-3 Accuracy: 0.1614\nTop-5 Accuracy: 0.1878\nTop-10 Accuracy: 0.2293\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2814\nEpoch 1:\nValidation Loss: 10.2719\nValidation Accuracy: 0.0895\nTop-3 Accuracy: 0.1418\nTop-5 Accuracy: 0.1779\nTop-10 Accuracy: 0.2618\nEpoch 2:\nTraining Loss: 10.2621\nEpoch 2:\nValidation Loss: 10.2645\nValidation Accuracy: 0.0967\nTop-3 Accuracy: 0.1540\nTop-5 Accuracy: 0.1864\nTop-10 Accuracy: 0.2657\nEpoch 3:\nTraining Loss: 10.2518\nEpoch 3:\nValidation Loss: 10.2634\nValidation Accuracy: 0.0977\nTop-3 Accuracy: 0.1590\nTop-5 Accuracy: 0.1925\nTop-10 Accuracy: 0.2652\nEpoch 4:\nTraining Loss: 10.2466\nEpoch 4:\nValidation Loss: 10.2613\nValidation Accuracy: 0.0999\nTop-3 Accuracy: 0.1671\nTop-5 Accuracy: 0.2004\nTop-10 Accuracy: 0.2455\nEpoch 5:\nTraining Loss: 10.2418\nEpoch 5:\nValidation Loss: 10.2599\nValidation Accuracy: 0.1012\nTop-3 Accuracy: 0.1692\nTop-5 Accuracy: 0.2000\nTop-10 Accuracy: 0.2389\nEpoch 6:\nTraining Loss: 10.2373\nEpoch 6:\nValidation Loss: 10.2596\nValidation Accuracy: 0.1015\nTop-3 Accuracy: 0.1702\nTop-5 Accuracy: 0.2005\nTop-10 Accuracy: 0.2337\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2963\nEpoch 1:\nValidation Loss: 10.2819\nValidation Accuracy: 0.0828\nTop-3 Accuracy: 0.1374\nTop-5 Accuracy: 0.1638\nTop-10 Accuracy: 0.2597\nEpoch 2:\nTraining Loss: 10.2765\nEpoch 2:\nValidation Loss: 10.2748\nValidation Accuracy: 0.0892\nTop-3 Accuracy: 0.1495\nTop-5 Accuracy: 0.1780\nTop-10 Accuracy: 0.2603\nEpoch 3:\nTraining Loss: 10.2679\nEpoch 3:\nValidation Loss: 10.2696\nValidation Accuracy: 0.0940\nTop-3 Accuracy: 0.1530\nTop-5 Accuracy: 0.1789\nTop-10 Accuracy: 0.2616\nEpoch 4:\nTraining Loss: 10.2609\nEpoch 4:\nValidation Loss: 10.2664\nValidation Accuracy: 0.0968\nTop-3 Accuracy: 0.1545\nTop-5 Accuracy: 0.1787\nTop-10 Accuracy: 0.2620\nEpoch 5:\nTraining Loss: 10.2558\nEpoch 5:\nValidation Loss: 10.2646\nValidation Accuracy: 0.0984\nTop-3 Accuracy: 0.1554\nTop-5 Accuracy: 0.1789\nTop-10 Accuracy: 0.2617\nEpoch 6:\nTraining Loss: 10.2519\nEpoch 6:\nValidation Loss: 10.2634\nValidation Accuracy: 0.0995\nTop-3 Accuracy: 0.1560\nTop-5 Accuracy: 0.1793\nTop-10 Accuracy: 0.2588\nEpoch 7:\nTraining Loss: 10.2485\nEpoch 7:\nValidation Loss: 10.2624\nValidation Accuracy: 0.1001\nTop-3 Accuracy: 0.1563\nTop-5 Accuracy: 0.1796\nTop-10 Accuracy: 0.2572\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2968\nEpoch 1:\nValidation Loss: 10.2819\nValidation Accuracy: 0.0834\nTop-3 Accuracy: 0.1455\nTop-5 Accuracy: 0.1769\nTop-10 Accuracy: 0.2623\nEpoch 2:\nTraining Loss: 10.2749\nEpoch 2:\nValidation Loss: 10.2726\nValidation Accuracy: 0.0916\nTop-3 Accuracy: 0.1509\nTop-5 Accuracy: 0.1778\nTop-10 Accuracy: 0.2594\nEpoch 3:\nTraining Loss: 10.2659\nEpoch 3:\nValidation Loss: 10.2683\nValidation Accuracy: 0.0955\nTop-3 Accuracy: 0.1533\nTop-5 Accuracy: 0.1782\nTop-10 Accuracy: 0.2573\nEpoch 4:\nTraining Loss: 10.2599\nEpoch 4:\nValidation Loss: 10.2659\nValidation Accuracy: 0.0975\nTop-3 Accuracy: 0.1547\nTop-5 Accuracy: 0.1786\nTop-10 Accuracy: 0.2592\nEpoch 5:\nTraining Loss: 10.2552\nEpoch 5:\nValidation Loss: 10.2643\nValidation Accuracy: 0.0988\nTop-3 Accuracy: 0.1556\nTop-5 Accuracy: 0.1792\nTop-10 Accuracy: 0.2601\nEpoch 6:\nTraining Loss: 10.2514\nEpoch 6:\nValidation Loss: 10.2633\nValidation Accuracy: 0.0995\nTop-3 Accuracy: 0.1558\nTop-5 Accuracy: 0.1799\nTop-10 Accuracy: 0.2596\nEpoch 7:\nTraining Loss: 10.2483\nEpoch 7:\nValidation Loss: 10.2627\nValidation Accuracy: 0.0999\nTop-3 Accuracy: 0.1564\nTop-5 Accuracy: 0.1822\nTop-10 Accuracy: 0.2572\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2758\nEpoch 1:\nValidation Loss: 10.2662\nValidation Accuracy: 0.0952\nTop-3 Accuracy: 0.1543\nTop-5 Accuracy: 0.1890\nTop-10 Accuracy: 0.2525\nEpoch 2:\nTraining Loss: 10.2573\nEpoch 2:\nValidation Loss: 10.2651\nValidation Accuracy: 0.0960\nTop-3 Accuracy: 0.1575\nTop-5 Accuracy: 0.1930\nTop-10 Accuracy: 0.2670\nEpoch 3:\nTraining Loss: 10.2511\nEpoch 3:\nValidation Loss: 10.2632\nValidation Accuracy: 0.0978\nTop-3 Accuracy: 0.1620\nTop-5 Accuracy: 0.1999\nTop-10 Accuracy: 0.2661\nEpoch 4:\nTraining Loss: 10.2459\nEpoch 4:\nValidation Loss: 10.2580\nValidation Accuracy: 0.1029\nTop-3 Accuracy: 0.1724\nTop-5 Accuracy: 0.2072\nTop-10 Accuracy: 0.2625\nEpoch 5:\nTraining Loss: 10.2393\nEpoch 5:\nValidation Loss: 10.2568\nValidation Accuracy: 0.1041\nTop-3 Accuracy: 0.1755\nTop-5 Accuracy: 0.2085\nTop-10 Accuracy: 0.2521\nEpoch 6:\nTraining Loss: 10.2352\nEpoch 6:\nValidation Loss: 10.2554\nValidation Accuracy: 0.1055\nTop-3 Accuracy: 0.1759\nTop-5 Accuracy: 0.2089\nTop-10 Accuracy: 0.2498\nEpoch 7:\nTraining Loss: 10.2320\nEpoch 7:\nValidation Loss: 10.2547\nValidation Accuracy: 0.1062\nTop-3 Accuracy: 0.1779\nTop-5 Accuracy: 0.2103\nTop-10 Accuracy: 0.2497\nEarly stopping triggered\nEpoch 1:\nTraining Loss: 10.2892\nEpoch 1:\nValidation Loss: 10.2790\nValidation Accuracy: 0.0821\nTop-3 Accuracy: 0.1334\nTop-5 Accuracy: 0.1783\nTop-10 Accuracy: 0.2640\nEpoch 2:\nTraining Loss: 10.2701\nEpoch 2:\nValidation Loss: 10.2671\nValidation Accuracy: 0.0940\nTop-3 Accuracy: 0.1529\nTop-5 Accuracy: 0.1901\nTop-10 Accuracy: 0.2677\nEpoch 3:\nTraining Loss: 10.2564\nEpoch 3:\nValidation Loss: 10.2647\nValidation Accuracy: 0.0962\nTop-3 Accuracy: 0.1545\nTop-5 Accuracy: 0.1929\nTop-10 Accuracy: 0.2562\nEpoch 4:\nTraining Loss: 10.2511\nEpoch 4:\nValidation Loss: 10.2641\nValidation Accuracy: 0.0967\nTop-3 Accuracy: 0.1565\nTop-5 Accuracy: 0.1957\nTop-10 Accuracy: 0.2425\nEarly stopping triggered\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\"\"\"\nThe exported dataframe will have the following metrics: accuracy, \ntop-3 accuracy, top-5 accuracy, top-10 accuracy, as well as the total time \nneeded to finish the training loop. This is necessary as a tiebreaker, as\ncomputational time is a finite resource that we need to consider.\n\"\"\"\n\nresults_quarter_df = pd.DataFrame(results_quarter)\nresults_quarter_df.to_csv('model_quarter_hyperparam_results.csv', index = False)\nprint(results_quarter_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T02:14:43.489392Z","iopub.execute_input":"2024-12-20T02:14:43.489800Z","iopub.status.idle":"2024-12-20T02:14:43.504571Z","shell.execute_reply.started":"2024-12-20T02:14:43.489759Z","shell.execute_reply":"2024-12-20T02:14:43.503628Z"}},"outputs":[{"name":"stdout","text":"                                              params  val_accuracy  \\\n0  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.095047   \n1  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.096010   \n2  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.099198   \n3  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.088328   \n4  {'batch_size': 64, 'embedding_dim': 100, 'lear...      0.088123   \n\n   val_top_3_accuracy  val_top_5_accuracy  val_top_10_accuracy  total time  \n0            0.147940            0.173606             0.254494  462.096224  \n1            0.148532            0.173694             0.246759  402.416546  \n2            0.166956            0.197802             0.222861  346.485157  \n3            0.140491            0.166743             0.231708  232.955465  \n4            0.134215            0.156366             0.257379  384.411699  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\"repeat what we did with df_quarter, with the df_full and df_half\"\n\ndf_full = pd.read_csv('/kaggle/input/we-final/WE_book_corpus_final_dataset_processed.csv')\ndf_half = pd.read_csv('/kaggle/input/we-final-half/WE_book_corpus_final_dataset_processed_half.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:10:23.325746Z","iopub.execute_input":"2025-01-21T11:10:23.326051Z","iopub.status.idle":"2025-01-21T11:10:26.560586Z","shell.execute_reply.started":"2025-01-21T11:10:23.326028Z","shell.execute_reply":"2025-01-21T11:10:26.559913Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"vocab_full = build_vocab(df_full, 'processed_text')\npairs_full = create_context_target_pairs(df_full, 'processed_text', 5)\nencoded_pairs_full = encode_pairs(pairs_full, vocab_full)\n\nvocab_half = build_vocab(df_half, 'processed_text')\npairs_half = create_context_target_pairs(df_half, 'processed_text', 5)\nencoded_pairs_half = encode_pairs(pairs_half, vocab_half)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:11:38.024776Z","iopub.execute_input":"2025-01-21T11:11:38.025025Z","iopub.status.idle":"2025-01-21T11:12:44.160546Z","shell.execute_reply.started":"2025-01-21T11:11:38.025004Z","shell.execute_reply":"2025-01-21T11:12:44.159874Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def tune_hyperparams_full(param_grid, train_dataset, val_dataset):\n  results = []\n  for params in param_grid:\n    start_time = time.time()\n    batch_size = params['batch_size']\n    embedding_dim = params['embedding_dim']\n    learning_rate = params['learning_rate']\n    num_epochs = params['num_epochs']\n\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n\n    model = CBOWModel(len(vocab_full), embedding_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n      model.train()\n      total_loss = 0.0\n      for context, target in train_loader:\n        context, target = context.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(context)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n      avg_train_loss = total_loss / len(train_loader)\n      train_losses.append(avg_train_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Training Loss: {avg_train_loss:.4f}\")\n\n      model.eval()\n      val_loss = 0.0\n      val_accuracy = 0.0\n      val_top_3_accuracy = 0.0\n      val_top_5_accuracy = 0.0\n      val_top_10_accuracy = 0.0\n      with torch.no_grad():\n        for context, target in val_loader:\n          context, target = context.to(device), target.to(device)\n          output = model(context)\n          loss = criterion(output, target)\n          val_loss += loss.item()\n\n          predictions = torch.argmax(output, dim = 1)\n          val_accuracy += accuracy_score(target.cpu(), predictions.cpu())\n\n          top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n          val_top_3_accuracy += top_k_accuracies['top_3_accuracy']\n          val_top_5_accuracy += top_k_accuracies['top_5_accuracy']\n          val_top_10_accuracy += top_k_accuracies['top_10_accuracy']\n\n      avg_val_loss = val_loss / len(val_loader)\n      avg_val_accuracy = val_accuracy / len(val_loader)\n      avg_top_3_accuracy = val_top_3_accuracy / len(val_loader)\n      avg_top_5_accuracy = val_top_5_accuracy / len(val_loader)\n      avg_top_10_accuracy = val_top_10_accuracy / len(val_loader)\n      val_losses.append(avg_val_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Validation Loss: {avg_val_loss:.4f}\")\n      print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n      print(f\"Top-3 Accuracy: {avg_top_3_accuracy:.4f}\")\n      print(f\"Top-5 Accuracy: {avg_top_5_accuracy:.4f}\")\n      print(f\"Top-10 Accuracy; {avg_top_10_accuracy:.4f}\")\n\n      if abs(avg_val_loss - best_val_loss) < 1e-3:\n        print(\"Early Stopping Triggered\")\n        break\n      else:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), \"best_cbow_model_full.pth\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    results.append({\n        'params': params,\n        'val_accuracy': avg_val_accuracy,\n        'val_top_3_accuracy': avg_top_3_accuracy,\n        'val_top_5_accuracy': avg_top_5_accuracy,\n        'val_top_10_accuracy': avg_top_10_accuracy,\n        'total time': total_time\n    })\n  return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T15:01:10.502569Z","iopub.execute_input":"2025-01-20T15:01:10.502838Z","iopub.status.idle":"2025-01-20T15:01:10.512697Z","shell.execute_reply.started":"2025-01-20T15:01:10.502818Z","shell.execute_reply":"2025-01-20T15:01:10.511753Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_pairs_full, val_test_pairs_full = train_test_split(encoded_pairs_full, test_size = 0.70, random_state = 42)\nval_pairs_full, test_pairs_full = train_test_split(val_test_pairs_full, test_size = 0.5, random_state = 42)\n\ntrain_dataset_full = CBOWDataset(train_pairs_full, len(vocab_full), window_size = 5)\nval_dataset_full = CBOWDataset(val_pairs_full, len(vocab_full), window_size = 5)\ntest_dataset_full = CBOWDataset(test_pairs_full, len(vocab_full), window_size = 5)\n\nparam_grid = {\n    'batch_size': [64, 128],\n    'embedding_dim': [50, 100],\n    'learning_rate': [0.001, 0.01],\n    'num_epochs': [10, 20]\n}\n\nparam_combs = list(itertools.product(\n    param_grid['batch_size'],\n    param_grid['embedding_dim'],\n    param_grid['learning_rate'],\n    param_grid['num_epochs']\n))\n\nparam_grid_dicts = [\n    {'batch_size': bs, 'embedding_dim': ed, 'learning_rate': lr, 'num_epochs': ne}\n    for bs, ed, lr, ne in param_combs\n]\nresults_full = tune_hyperparams_full(param_grid_dicts, train_dataset_full, val_dataset_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:29:03.057743Z","iopub.execute_input":"2024-12-27T06:29:03.058049Z","iopub.status.idle":"2024-12-27T06:29:39.201560Z","shell.execute_reply.started":"2024-12-27T06:29:03.058028Z","shell.execute_reply":"2024-12-27T06:29:39.200338Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-2d33c04d5824>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_combs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m ]\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresults_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparams_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-fa99ffddce2d>\u001b[0m in \u001b[0;36mtune_hyperparams_full\u001b[0;34m(param_grid, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"results_full_df = pd.DataFrame(results_full)\nresults_full_df.to_csv('model_full_hyperparam_results.csv', index = False)\nprint(results_full_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:58:50.666997Z","iopub.execute_input":"2024-12-20T07:58:50.667309Z","iopub.status.idle":"2024-12-20T07:58:50.678174Z","shell.execute_reply.started":"2024-12-20T07:58:50.667287Z","shell.execute_reply":"2024-12-20T07:58:50.677257Z"}},"outputs":[{"name":"stdout","text":"                                              params  val_accuracy  \\\n0  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.091945   \n1  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.099652   \n2  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.095635   \n3  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.104104   \n4  {'batch_size': 64, 'embedding_dim': 100, 'lear...      0.091414   \n\n   val_top_3_accuracy  val_top_5_accuracy  val_top_10_accuracy   total time  \n0            0.134056            0.163004             0.261592  1131.510653  \n1            0.149471            0.172890             0.244863  1125.440634  \n2            0.153999            0.188280             0.214185  1713.419736  \n3            0.169909            0.193780             0.228524  1416.539697  \n4            0.136206            0.158654             0.259320  1256.018537  \n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def tune_hyperparams_half(param_grid, train_dataset, val_dataset):\n  results = []\n  for params in param_grid:\n    start_time = time.time()\n    batch_size = params['batch_size']\n    embedding_dim = params['embedding_dim']\n    learning_rate = params['learning_rate']\n    num_epochs = params['num_epochs']\n\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n\n    model = CBOWModel(len(vocab_half), embedding_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n      model.train()\n      total_loss = 0.0\n      for context, target in train_loader:\n        context, target = context.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(context)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n      avg_train_loss = total_loss / len(train_loader)\n      train_losses.append(avg_train_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Training Loss: {avg_train_loss:.4f}\")\n\n      model.eval()\n      val_loss = 0.0\n      val_accuracy = 0.0\n      val_top_3_accuracy = 0.0\n      val_top_5_accuracy = 0.0\n      val_top_10_accuracy = 0.0\n      with torch.no_grad():\n        for context, target in val_loader:\n          context, target = context.to(device), target.to(device)\n          output = model(context)\n          loss = criterion(output, target)\n          val_loss += loss.item()\n\n          predictions = torch.argmax(output, dim = 1)\n          val_accuracy += accuracy_score(target.cpu(), predictions.cpu())\n\n          top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n          val_top_3_accuracy += top_k_accuracies['top_3_accuracy']\n          val_top_5_accuracy += top_k_accuracies['top_5_accuracy']\n          val_top_10_accuracy += top_k_accuracies['top_10_accuracy']\n\n      avg_val_loss = val_loss / len(val_loader)\n      avg_val_accuracy = val_accuracy / len(val_loader)\n      avg_top_3_accuracy = val_top_3_accuracy / len(val_loader)\n      avg_top_5_accuracy = val_top_5_accuracy / len(val_loader)\n      avg_top_10_accuracy = val_top_10_accuracy / len(val_loader)\n      val_losses.append(avg_val_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Validation Loss: {avg_val_loss:.4f}\")\n      print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n      print(f\"Top-3 Accuracy: {avg_top_3_accuracy:.4f}\")\n      print(f\"Top-5 Accuracy: {avg_top_5_accuracy:.4f}\")\n      print(f\"Top-10 Accuracy; {avg_top_10_accuracy:.4f}\")\n\n      if abs(avg_val_loss - best_val_loss) < 1e-3:\n        print(\"Early Stopping Triggered\")\n        break\n      else:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), \"best_cbow_model_half.pth\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    results.append({\n        'params': params,\n        'val_accuracy': avg_val_accuracy,\n        'val_top_3_accuracy': avg_top_3_accuracy,\n        'val_top_5_accuracy': avg_top_5_accuracy,\n        'val_top_10_accuracy': avg_top_10_accuracy,\n        'total time': total_time\n    })\n  return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T15:01:15.497071Z","iopub.execute_input":"2025-01-20T15:01:15.497392Z","iopub.status.idle":"2025-01-20T15:01:15.507752Z","shell.execute_reply.started":"2025-01-20T15:01:15.497361Z","shell.execute_reply":"2025-01-20T15:01:15.506668Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_pairs_half, val_test_pairs_half = train_test_split(encoded_pairs_half, test_size = 0.70, random_state = 42)\nval_pairs_half, test_pairs_half = train_test_split(val_test_pairs_half, test_size = 0.5, random_state = 42)\n\ntrain_dataset_half = CBOWDataset(train_pairs_half, len(vocab_half), window_size = 5)\nval_dataset_half = CBOWDataset(val_pairs_half, len(vocab_half), window_size = 5)\ntest_dataset_half = CBOWDataset(test_pairs_half, len(vocab_half), window_size = 5)\n\nparam_grid = {\n    'batch_size': [64, 128],\n    'embedding_dim': [50, 100],\n    'learning_rate': [0.001, 0.01],\n    'num_epochs': [10, 20]\n}\n\nparam_combs = list(itertools.product(\n    param_grid['batch_size'],\n    param_grid['embedding_dim'],\n    param_grid['learning_rate'],\n    param_grid['num_epochs']\n))\n\nparam_grid_dicts = [\n    {'batch_size': bs, 'embedding_dim': ed, 'learning_rate': lr, 'num_epochs': ne}\n    for bs, ed, lr, ne in param_combs\n]\nresults_half = tune_hyperparams_half(param_grid_dicts, train_dataset_half, val_dataset_half)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T07:59:05.227399Z","iopub.execute_input":"2024-12-20T07:59:05.227731Z","iopub.status.idle":"2024-12-20T10:27:21.439979Z","shell.execute_reply.started":"2024-12-20T07:59:05.227704Z","shell.execute_reply":"2024-12-20T10:27:21.439137Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:\nTraining Loss: 10.4920\nEpoch 1:\nValidation Loss: 10.4824\nValidation Accuracy: 0.0753\nTop-3 Accuracy: 0.1162\nTop-5 Accuracy: 0.1533\nTop-10 Accuracy; 0.2594\nEpoch 2:\nTraining Loss: 10.4787\nEpoch 2:\nValidation Loss: 10.4782\nValidation Accuracy: 0.0788\nTop-3 Accuracy: 0.1174\nTop-5 Accuracy: 0.1535\nTop-10 Accuracy; 0.2611\nEpoch 3:\nTraining Loss: 10.4742\nEpoch 3:\nValidation Loss: 10.4764\nValidation Accuracy: 0.0803\nTop-3 Accuracy: 0.1180\nTop-5 Accuracy: 0.1540\nTop-10 Accuracy; 0.2519\nEpoch 4:\nTraining Loss: 10.4714\nEpoch 4:\nValidation Loss: 10.4755\nValidation Accuracy: 0.0810\nTop-3 Accuracy: 0.1185\nTop-5 Accuracy: 0.1547\nTop-10 Accuracy; 0.2476\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4843\nEpoch 1:\nValidation Loss: 10.4719\nValidation Accuracy: 0.0867\nTop-3 Accuracy: 0.1448\nTop-5 Accuracy: 0.1763\nTop-10 Accuracy; 0.2570\nEpoch 2:\nTraining Loss: 10.4648\nEpoch 2:\nValidation Loss: 10.4625\nValidation Accuracy: 0.0959\nTop-3 Accuracy: 0.1533\nTop-5 Accuracy: 0.1781\nTop-10 Accuracy; 0.2503\nEpoch 3:\nTraining Loss: 10.4560\nEpoch 3:\nValidation Loss: 10.4582\nValidation Accuracy: 0.0998\nTop-3 Accuracy: 0.1558\nTop-5 Accuracy: 0.1784\nTop-10 Accuracy; 0.2508\nEpoch 4:\nTraining Loss: 10.4509\nEpoch 4:\nValidation Loss: 10.4563\nValidation Accuracy: 0.1012\nTop-3 Accuracy: 0.1566\nTop-5 Accuracy: 0.1785\nTop-10 Accuracy; 0.2513\nEpoch 5:\nTraining Loss: 10.4475\nEpoch 5:\nValidation Loss: 10.4554\nValidation Accuracy: 0.1018\nTop-3 Accuracy: 0.1572\nTop-5 Accuracy: 0.1788\nTop-10 Accuracy; 0.2515\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4678\nEpoch 1:\nValidation Loss: 10.4598\nValidation Accuracy: 0.0962\nTop-3 Accuracy: 0.1545\nTop-5 Accuracy: 0.1904\nTop-10 Accuracy; 0.2664\nEpoch 2:\nTraining Loss: 10.4535\nEpoch 2:\nValidation Loss: 10.4577\nValidation Accuracy: 0.0982\nTop-3 Accuracy: 0.1561\nTop-5 Accuracy: 0.1922\nTop-10 Accuracy; 0.2594\nEpoch 3:\nTraining Loss: 10.4497\nEpoch 3:\nValidation Loss: 10.4573\nValidation Accuracy: 0.0985\nTop-3 Accuracy: 0.1565\nTop-5 Accuracy: 0.1907\nTop-10 Accuracy; 0.2535\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4844\nEpoch 1:\nValidation Loss: 10.4793\nValidation Accuracy: 0.0765\nTop-3 Accuracy: 0.1257\nTop-5 Accuracy: 0.1776\nTop-10 Accuracy; 0.2574\nEpoch 2:\nTraining Loss: 10.4719\nEpoch 2:\nValidation Loss: 10.4707\nValidation Accuracy: 0.0852\nTop-3 Accuracy: 0.1350\nTop-5 Accuracy: 0.1708\nTop-10 Accuracy; 0.2491\nEpoch 3:\nTraining Loss: 10.4591\nEpoch 3:\nValidation Loss: 10.4585\nValidation Accuracy: 0.0974\nTop-3 Accuracy: 0.1592\nTop-5 Accuracy: 0.1908\nTop-10 Accuracy; 0.2461\nEpoch 4:\nTraining Loss: 10.4508\nEpoch 4:\nValidation Loss: 10.4570\nValidation Accuracy: 0.0988\nTop-3 Accuracy: 0.1636\nTop-5 Accuracy: 0.1939\nTop-10 Accuracy; 0.2412\nEpoch 5:\nTraining Loss: 10.4476\nEpoch 5:\nValidation Loss: 10.4562\nValidation Accuracy: 0.0996\nTop-3 Accuracy: 0.1639\nTop-5 Accuracy: 0.1934\nTop-10 Accuracy; 0.2359\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4777\nEpoch 1:\nValidation Loss: 10.4649\nValidation Accuracy: 0.0938\nTop-3 Accuracy: 0.1524\nTop-5 Accuracy: 0.1783\nTop-10 Accuracy; 0.2649\nEpoch 2:\nTraining Loss: 10.4585\nEpoch 2:\nValidation Loss: 10.4588\nValidation Accuracy: 0.0990\nTop-3 Accuracy: 0.1555\nTop-5 Accuracy: 0.1783\nTop-10 Accuracy; 0.2495\nEpoch 3:\nTraining Loss: 10.4516\nEpoch 3:\nValidation Loss: 10.4560\nValidation Accuracy: 0.1014\nTop-3 Accuracy: 0.1570\nTop-5 Accuracy: 0.1791\nTop-10 Accuracy; 0.2497\nEpoch 4:\nTraining Loss: 10.4475\nEpoch 4:\nValidation Loss: 10.4550\nValidation Accuracy: 0.1020\nTop-3 Accuracy: 0.1573\nTop-5 Accuracy: 0.1790\nTop-10 Accuracy; 0.2513\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4767\nEpoch 1:\nValidation Loss: 10.4640\nValidation Accuracy: 0.0948\nTop-3 Accuracy: 0.1531\nTop-5 Accuracy: 0.1775\nTop-10 Accuracy; 0.2519\nEpoch 2:\nTraining Loss: 10.4576\nEpoch 2:\nValidation Loss: 10.4584\nValidation Accuracy: 0.0994\nTop-3 Accuracy: 0.1556\nTop-5 Accuracy: 0.1779\nTop-10 Accuracy; 0.2512\nEpoch 3:\nTraining Loss: 10.4511\nEpoch 3:\nValidation Loss: 10.4560\nValidation Accuracy: 0.1014\nTop-3 Accuracy: 0.1569\nTop-5 Accuracy: 0.1788\nTop-10 Accuracy; 0.2521\nEpoch 4:\nTraining Loss: 10.4471\nEpoch 4:\nValidation Loss: 10.4550\nValidation Accuracy: 0.1020\nTop-3 Accuracy: 0.1569\nTop-5 Accuracy: 0.1788\nTop-10 Accuracy; 0.2487\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4770\nEpoch 1:\nValidation Loss: 10.4648\nValidation Accuracy: 0.0910\nTop-3 Accuracy: 0.1513\nTop-5 Accuracy: 0.1861\nTop-10 Accuracy; 0.2574\nEpoch 2:\nTraining Loss: 10.4603\nEpoch 2:\nValidation Loss: 10.4628\nValidation Accuracy: 0.0929\nTop-3 Accuracy: 0.1513\nTop-5 Accuracy: 0.1792\nTop-10 Accuracy; 0.2279\nEpoch 3:\nTraining Loss: 10.4575\nEpoch 3:\nValidation Loss: 10.4624\nValidation Accuracy: 0.0934\nTop-3 Accuracy: 0.1519\nTop-5 Accuracy: 0.1788\nTop-10 Accuracy; 0.2190\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4755\nEpoch 1:\nValidation Loss: 10.4677\nValidation Accuracy: 0.0882\nTop-3 Accuracy: 0.1462\nTop-5 Accuracy: 0.1830\nTop-10 Accuracy; 0.2481\nEpoch 2:\nTraining Loss: 10.4613\nEpoch 2:\nValidation Loss: 10.4613\nValidation Accuracy: 0.0945\nTop-3 Accuracy: 0.1572\nTop-5 Accuracy: 0.1869\nTop-10 Accuracy; 0.2443\nEpoch 3:\nTraining Loss: 10.4551\nEpoch 3:\nValidation Loss: 10.4597\nValidation Accuracy: 0.0961\nTop-3 Accuracy: 0.1590\nTop-5 Accuracy: 0.1886\nTop-10 Accuracy; 0.2378\nEpoch 4:\nTraining Loss: 10.4510\nEpoch 4:\nValidation Loss: 10.4562\nValidation Accuracy: 0.0996\nTop-3 Accuracy: 0.1627\nTop-5 Accuracy: 0.1881\nTop-10 Accuracy; 0.2266\nEpoch 5:\nTraining Loss: 10.4475\nEpoch 5:\nValidation Loss: 10.4554\nValidation Accuracy: 0.1004\nTop-3 Accuracy: 0.1636\nTop-5 Accuracy: 0.1865\nTop-10 Accuracy; 0.2113\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4943\nEpoch 1:\nValidation Loss: 10.4851\nValidation Accuracy: 0.0723\nTop-3 Accuracy: 0.1090\nTop-5 Accuracy: 0.1698\nTop-10 Accuracy; 0.2521\nEpoch 2:\nTraining Loss: 10.4823\nEpoch 2:\nValidation Loss: 10.4821\nValidation Accuracy: 0.0748\nTop-3 Accuracy: 0.1092\nTop-5 Accuracy: 0.1757\nTop-10 Accuracy; 0.2541\nEpoch 3:\nTraining Loss: 10.4793\nEpoch 3:\nValidation Loss: 10.4810\nValidation Accuracy: 0.0756\nTop-3 Accuracy: 0.1093\nTop-5 Accuracy: 0.1600\nTop-10 Accuracy; 0.2525\nEpoch 4:\nTraining Loss: 10.4775\nEpoch 4:\nValidation Loss: 10.4804\nValidation Accuracy: 0.0761\nTop-3 Accuracy: 0.1096\nTop-5 Accuracy: 0.1554\nTop-10 Accuracy; 0.2498\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4894\nEpoch 1:\nValidation Loss: 10.4761\nValidation Accuracy: 0.0829\nTop-3 Accuracy: 0.1316\nTop-5 Accuracy: 0.1693\nTop-10 Accuracy; 0.2619\nEpoch 2:\nTraining Loss: 10.4716\nEpoch 2:\nValidation Loss: 10.4708\nValidation Accuracy: 0.0870\nTop-3 Accuracy: 0.1327\nTop-5 Accuracy: 0.1694\nTop-10 Accuracy; 0.2587\nEpoch 3:\nTraining Loss: 10.4661\nEpoch 3:\nValidation Loss: 10.4663\nValidation Accuracy: 0.0918\nTop-3 Accuracy: 0.1464\nTop-5 Accuracy: 0.1696\nTop-10 Accuracy; 0.2556\nEpoch 4:\nTraining Loss: 10.4599\nEpoch 4:\nValidation Loss: 10.4625\nValidation Accuracy: 0.0954\nTop-3 Accuracy: 0.1479\nTop-5 Accuracy: 0.1698\nTop-10 Accuracy; 0.2492\nEpoch 5:\nTraining Loss: 10.4556\nEpoch 5:\nValidation Loss: 10.4606\nValidation Accuracy: 0.0970\nTop-3 Accuracy: 0.1485\nTop-5 Accuracy: 0.1703\nTop-10 Accuracy; 0.2490\nEpoch 6:\nTraining Loss: 10.4528\nEpoch 6:\nValidation Loss: 10.4596\nValidation Accuracy: 0.0977\nTop-3 Accuracy: 0.1487\nTop-5 Accuracy: 0.1715\nTop-10 Accuracy; 0.2492\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4805\nEpoch 1:\nValidation Loss: 10.4712\nValidation Accuracy: 0.0849\nTop-3 Accuracy: 0.1346\nTop-5 Accuracy: 0.1787\nTop-10 Accuracy; 0.2639\nEpoch 2:\nTraining Loss: 10.4601\nEpoch 2:\nValidation Loss: 10.4578\nValidation Accuracy: 0.0983\nTop-3 Accuracy: 0.1551\nTop-5 Accuracy: 0.1878\nTop-10 Accuracy; 0.2622\nEpoch 3:\nTraining Loss: 10.4494\nEpoch 3:\nValidation Loss: 10.4564\nValidation Accuracy: 0.0995\nTop-3 Accuracy: 0.1570\nTop-5 Accuracy: 0.1897\nTop-10 Accuracy; 0.2576\nEpoch 4:\nTraining Loss: 10.4459\nEpoch 4:\nValidation Loss: 10.4560\nValidation Accuracy: 0.0998\nTop-3 Accuracy: 0.1590\nTop-5 Accuracy: 0.1906\nTop-10 Accuracy; 0.2543\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.5076\nEpoch 1:\nValidation Loss: 10.5038\nValidation Accuracy: 0.0520\nTop-3 Accuracy: 0.1143\nTop-5 Accuracy: 0.1642\nTop-10 Accuracy; 0.2562\nEpoch 2:\nTraining Loss: 10.4927\nEpoch 2:\nValidation Loss: 10.4830\nValidation Accuracy: 0.0728\nTop-3 Accuracy: 0.1182\nTop-5 Accuracy: 0.1712\nTop-10 Accuracy; 0.2496\nEpoch 3:\nTraining Loss: 10.4793\nEpoch 3:\nValidation Loss: 10.4757\nValidation Accuracy: 0.0802\nTop-3 Accuracy: 0.1306\nTop-5 Accuracy: 0.1596\nTop-10 Accuracy; 0.2354\nEpoch 4:\nTraining Loss: 10.4631\nEpoch 4:\nValidation Loss: 10.4604\nValidation Accuracy: 0.0955\nTop-3 Accuracy: 0.1524\nTop-5 Accuracy: 0.1773\nTop-10 Accuracy; 0.2262\nEpoch 5:\nTraining Loss: 10.4527\nEpoch 5:\nValidation Loss: 10.4584\nValidation Accuracy: 0.0975\nTop-3 Accuracy: 0.1545\nTop-5 Accuracy: 0.1781\nTop-10 Accuracy; 0.2137\nEpoch 6:\nTraining Loss: 10.4490\nEpoch 6:\nValidation Loss: 10.4578\nValidation Accuracy: 0.0980\nTop-3 Accuracy: 0.1547\nTop-5 Accuracy: 0.1787\nTop-10 Accuracy; 0.2129\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4828\nEpoch 1:\nValidation Loss: 10.4711\nValidation Accuracy: 0.0872\nTop-3 Accuracy: 0.1456\nTop-5 Accuracy: 0.1774\nTop-10 Accuracy; 0.2599\nEpoch 2:\nTraining Loss: 10.4639\nEpoch 2:\nValidation Loss: 10.4622\nValidation Accuracy: 0.0962\nTop-3 Accuracy: 0.1541\nTop-5 Accuracy: 0.1787\nTop-10 Accuracy; 0.2638\nEpoch 3:\nTraining Loss: 10.4550\nEpoch 3:\nValidation Loss: 10.4580\nValidation Accuracy: 0.1000\nTop-3 Accuracy: 0.1561\nTop-5 Accuracy: 0.1789\nTop-10 Accuracy; 0.2533\nEpoch 4:\nTraining Loss: 10.4499\nEpoch 4:\nValidation Loss: 10.4562\nValidation Accuracy: 0.1014\nTop-3 Accuracy: 0.1567\nTop-5 Accuracy: 0.1793\nTop-10 Accuracy; 0.2480\nEpoch 5:\nTraining Loss: 10.4464\nEpoch 5:\nValidation Loss: 10.4552\nValidation Accuracy: 0.1021\nTop-3 Accuracy: 0.1574\nTop-5 Accuracy: 0.1810\nTop-10 Accuracy; 0.2428\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4901\nEpoch 1:\nValidation Loss: 10.4747\nValidation Accuracy: 0.0840\nTop-3 Accuracy: 0.1315\nTop-5 Accuracy: 0.1697\nTop-10 Accuracy; 0.2593\nEpoch 2:\nTraining Loss: 10.4686\nEpoch 2:\nValidation Loss: 10.4659\nValidation Accuracy: 0.0924\nTop-3 Accuracy: 0.1467\nTop-5 Accuracy: 0.1698\nTop-10 Accuracy; 0.2580\nEpoch 3:\nTraining Loss: 10.4598\nEpoch 3:\nValidation Loss: 10.4621\nValidation Accuracy: 0.0956\nTop-3 Accuracy: 0.1479\nTop-5 Accuracy: 0.1705\nTop-10 Accuracy; 0.2493\nEpoch 4:\nTraining Loss: 10.4550\nEpoch 4:\nValidation Loss: 10.4602\nValidation Accuracy: 0.0973\nTop-3 Accuracy: 0.1485\nTop-5 Accuracy: 0.1717\nTop-10 Accuracy; 0.2491\nEpoch 5:\nTraining Loss: 10.4520\nEpoch 5:\nValidation Loss: 10.4593\nValidation Accuracy: 0.0977\nTop-3 Accuracy: 0.1492\nTop-5 Accuracy: 0.1739\nTop-10 Accuracy; 0.2497\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4737\nEpoch 1:\nValidation Loss: 10.4634\nValidation Accuracy: 0.0926\nTop-3 Accuracy: 0.1525\nTop-5 Accuracy: 0.1901\nTop-10 Accuracy; 0.2713\nEpoch 2:\nTraining Loss: 10.4557\nEpoch 2:\nValidation Loss: 10.4592\nValidation Accuracy: 0.0966\nTop-3 Accuracy: 0.1574\nTop-5 Accuracy: 0.1939\nTop-10 Accuracy; 0.2727\nEpoch 3:\nTraining Loss: 10.4509\nEpoch 3:\nValidation Loss: 10.4574\nValidation Accuracy: 0.0985\nTop-3 Accuracy: 0.1564\nTop-5 Accuracy: 0.1879\nTop-10 Accuracy; 0.2610\nEpoch 4:\nTraining Loss: 10.4482\nEpoch 4:\nValidation Loss: 10.4557\nValidation Accuracy: 0.1001\nTop-3 Accuracy: 0.1567\nTop-5 Accuracy: 0.1846\nTop-10 Accuracy; 0.2543\nEpoch 5:\nTraining Loss: 10.4454\nEpoch 5:\nValidation Loss: 10.4552\nValidation Accuracy: 0.1006\nTop-3 Accuracy: 0.1578\nTop-5 Accuracy: 0.1826\nTop-10 Accuracy; 0.2409\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 10.4843\nEpoch 1:\nValidation Loss: 10.4797\nValidation Accuracy: 0.0762\nTop-3 Accuracy: 0.1354\nTop-5 Accuracy: 0.1863\nTop-10 Accuracy; 0.2647\nEpoch 2:\nTraining Loss: 10.4634\nEpoch 2:\nValidation Loss: 10.4594\nValidation Accuracy: 0.0965\nTop-3 Accuracy: 0.1547\nTop-5 Accuracy: 0.1852\nTop-10 Accuracy; 0.2586\nEpoch 3:\nTraining Loss: 10.4522\nEpoch 3:\nValidation Loss: 10.4564\nValidation Accuracy: 0.0995\nTop-3 Accuracy: 0.1573\nTop-5 Accuracy: 0.1890\nTop-10 Accuracy; 0.2556\nEpoch 4:\nTraining Loss: 10.4476\nEpoch 4:\nValidation Loss: 10.4545\nValidation Accuracy: 0.1013\nTop-3 Accuracy: 0.1649\nTop-5 Accuracy: 0.1965\nTop-10 Accuracy; 0.2527\nEpoch 5:\nTraining Loss: 10.4447\nEpoch 5:\nValidation Loss: 10.4533\nValidation Accuracy: 0.1025\nTop-3 Accuracy: 0.1686\nTop-5 Accuracy: 0.1980\nTop-10 Accuracy; 0.2475\nEpoch 6:\nTraining Loss: 10.4423\nEpoch 6:\nValidation Loss: 10.4532\nValidation Accuracy: 0.1027\nTop-3 Accuracy: 0.1695\nTop-5 Accuracy: 0.1985\nTop-10 Accuracy; 0.2429\nEarly Stopping Triggered\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"results_half_df = pd.DataFrame(results_half)\nresults_half_df.to_csv('model_half_hyperparam_results.csv', index = False)\nprint(results_half_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T10:27:21.440998Z","iopub.execute_input":"2024-12-20T10:27:21.441282Z","iopub.status.idle":"2024-12-20T10:27:21.452621Z","shell.execute_reply.started":"2024-12-20T10:27:21.441246Z","shell.execute_reply":"2024-12-20T10:27:21.451788Z"}},"outputs":[{"name":"stdout","text":"                                              params  val_accuracy  \\\n0  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.081014   \n1  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.101805   \n2  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.098515   \n3  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.099565   \n4  {'batch_size': 64, 'embedding_dim': 100, 'lear...      0.102039   \n\n   val_top_3_accuracy  val_top_5_accuracy  val_top_10_accuracy  total time  \n0            0.118510            0.154749             0.247598  526.791273  \n1            0.157203            0.178764             0.251544  655.878617  \n2            0.156505            0.190732             0.253524  393.286634  \n3            0.163915            0.193381             0.235921  653.833550  \n4            0.157298            0.179036             0.251280  587.242343  \n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"After completing the training and evaluation loops with the standard model, \nwe repeat the process with the weighted model, where the lambda layer takes\na weighted average instead of the standard average that was employed in the\nprevious models.","metadata":{}},{"cell_type":"code","source":"class CBOWModelWeighted(nn.Module):\n  def __init__(self, vocab_size, embedding_dim, window_size):\n        super(CBOWModelWeighted, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.window_size = window_size\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.fc = nn.Linear(embedding_dim, vocab_size)\n        self.weights = self.compute_weights(window_size)\n\n  def compute_weights(self, window_size):\n        \"\"\"\n        Assign higher weights to context words closer to the target word.\n        Weights decrease linearly with distance from the target word.\n        \"\"\"\n        positions = torch.arange(-window_size, window_size + 1, dtype=torch.float32)\n        weights = 1.0 / (1.0 + torch.abs(positions))  # Larger weights for closer words\n        weights[window_size] = 0  # Exclude target position itself (distance=0)\n        return weights[:-1].softmax(0)  # Normalize weights to sum to 1\n\n  def forward(self, context):\n    embedded = self.embeddings(context)  # (batch_size, context_size, embedding_dim)\n\n    # Weighted averaging of embeddings\n    weights = self.weights.to(context.device)  # Ensure weights are on the same device\n    weighted_embeds = embedded * weights.view(1, -1, 1) # (batch_size, context_size, embedding_dim)\n    weighted_avg_embedding = weighted_embeds.sum(dim = 1) # (batch_size, embedding_dim)\n\n    # Fully connected layer\n    output = self.fc(weighted_avg_embedding) # (batch_size, vocab_size)\n    return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:14:18.583051Z","iopub.execute_input":"2025-01-21T11:14:18.583384Z","iopub.status.idle":"2025-01-21T11:14:18.589974Z","shell.execute_reply.started":"2025-01-21T11:14:18.583354Z","shell.execute_reply":"2025-01-21T11:14:18.589019Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def tune_hyperparams_half_weighted(param_grid, train_dataset, val_dataset):\n  results = []\n  for params in param_grid:\n    start_time = time.time()\n    batch_size = params['batch_size']\n    embedding_dim = params['embedding_dim']\n    learning_rate = params['learning_rate']\n    num_epochs = params['num_epochs']\n\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n    model = CBOWModelWeighted(len(vocab_half), embedding_dim, 5).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n      model.train()\n      total_loss = 0.0\n      for context, target in train_loader:\n        context, target = context.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(context)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n      avg_train_loss = total_loss / len(train_loader)\n      train_losses.append(avg_train_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Training Loss: {avg_train_loss:.4f}\")\n\n      model.eval()\n      val_loss = 0.0\n      val_accuracy = 0.0\n      val_top_3_accuracy = 0.0\n      val_top_5_accuracy = 0.0\n      val_top_10_accuracy = 0.0\n      with torch.no_grad():\n        for context, target in val_loader:\n          context, target = context.to(device), target.to(device)\n          output = model(context)\n          loss = criterion(output, target)\n          val_loss += loss.item()\n\n          predictions = torch.argmax(output, dim = 1)\n          val_accuracy += accuracy_score(target.cpu(), predictions.cpu())\n\n          top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n          val_top_3_accuracy += top_k_accuracies['top_3_accuracy']\n          val_top_5_accuracy += top_k_accuracies['top_5_accuracy']\n          val_top_10_accuracy += top_k_accuracies['top_10_accuracy']\n\n      avg_val_loss = val_loss / len(val_loader)\n      avg_val_accuracy = val_accuracy / len(val_loader)\n      avg_top_3_accuracy = val_top_3_accuracy / len(val_loader)\n      avg_top_5_accuracy = val_top_5_accuracy / len(val_loader)\n      avg_top_10_accuracy = val_top_10_accuracy / len(val_loader)\n      val_losses.append(avg_val_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Validation Loss: {avg_val_loss:.4f}\")\n      print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n      print(f\"Top-3 Accuracy: {avg_top_3_accuracy:.4f}\")\n      print(f\"Top-5 Accuracy: {avg_top_5_accuracy:.4f}\")\n      print(f\"Top-10 Accuracy: {avg_top_10_accuracy:.4f}\")\n\n      if abs(avg_val_loss - best_val_loss) < 1e-3:\n        print(\"Early Stopping Triggered\")\n        break\n      elif avg_val_loss > best_val_loss:\n        print(\"Early Stopping Triggered\")\n        break\n      else:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), \"best_cbow_weighted_model_half.pth\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    results.append({\n        'params': params,\n        'val_accuracy': avg_val_accuracy,\n        'val_top_3_accuracy': avg_top_3_accuracy,\n        'val_top_5_accuracy': avg_top_5_accuracy,\n        'val_top_10_accuracy': avg_top_10_accuracy,\n        'total time': total_time\n    })\n  return results\n\ntrain_pairs_half, val_test_pairs_half = train_test_split(encoded_pairs_half, test_size = 0.70, random_state = 42)\nval_pairs_half, test_pairs_half = train_test_split(val_test_pairs_half, test_size = 0.5, random_state = 42)\n\ntrain_dataset_half = CBOWDataset(train_pairs_half, len(vocab_half), window_size = 5)\nval_dataset_half = CBOWDataset(val_pairs_half, len(vocab_half), window_size = 5)\ntest_dataset_half = CBOWDataset(test_pairs_half, len(vocab_half), window_size = 5)\n\nparam_grid = {\n    'batch_size': [64, 128],\n    'embedding_dim': [50, 100],\n    'learning_rate': [0.001, 0.01],\n    'num_epochs': [10, 20]\n}\n\nparam_combs = list(itertools.product(\n    param_grid['batch_size'],\n    param_grid['embedding_dim'],\n    param_grid['learning_rate'],\n    param_grid['num_epochs']\n))\n\nparam_grid_dicts = [\n    {'batch_size': bs, 'embedding_dim': ed, 'learning_rate': lr, 'num_epochs': ne}\n    for bs, ed, lr, ne in param_combs\n]\n\nresults_half_weighted = tune_hyperparams_half_weighted(param_grid_dicts, train_dataset_half, val_dataset_half)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T15:02:13.719937Z","iopub.execute_input":"2025-01-20T15:02:13.720240Z","iopub.status.idle":"2025-01-20T15:02:18.265785Z","shell.execute_reply.started":"2025-01-20T15:02:13.720207Z","shell.execute_reply":"2025-01-20T15:02:18.264532Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-db83f58f513f>\u001b[0m in \u001b[0;36m<cell line: 120>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m ]\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mresults_half_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparams_half_weighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_half\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset_half\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-db83f58f513f>\u001b[0m in \u001b[0;36mtune_hyperparams_half_weighted\u001b[0;34m(param_grid, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOWModelWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_half\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcode_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcode_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformat_traceback_short\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompilerFn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbytecode_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_dead_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_pointless_jumps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;31m# skip common third party libs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTHIRDPARTY_SKIPLIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3467\u001b[0;31m     \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[0m_recompile_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(import_name)\u001b[0m\n\u001b[1;32m   3341\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3342\u001b[0m     \u001b[0mSKIP_DIRS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_strip_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3343\u001b[0;31m     \u001b[0m_recompile_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m_recompile_re\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3325\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_recompile_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mSKIP_DIRS_RE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m     \u001b[0mSKIP_DIRS_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"^[^\\s<]*({'|'.join(map(re.escape, SKIP_DIRS))})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    445\u001b[0m                            not nested and not items))\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    839\u001b[0m             sub_verbose = ((verbose or (add_flags & SRE_FLAG_VERBOSE)) and\n\u001b[1;32m    840\u001b[0m                            not (del_flags & SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 raise source.error(\"missing ), unterminated subpattern\",\n","\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    445\u001b[0m                            not nested and not items))\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;31m# unpack non-capturing groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubpattern\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mSUBPATTERN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_flags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_flags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/sre_parse.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSubPattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"results_half_weighted_df = pd.DataFrame(results_half_weighted)\nresults_half_weighted_df.to_csv('weighted_model_half_hyperparam_results.csv', index = False)\nprint(results_half_weighted_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:41:47.775256Z","iopub.execute_input":"2024-12-21T04:41:47.775515Z","iopub.status.idle":"2024-12-21T04:41:47.791006Z","shell.execute_reply.started":"2024-12-21T04:41:47.775494Z","shell.execute_reply":"2024-12-21T04:41:47.790311Z"}},"outputs":[{"name":"stdout","text":"                                              params  val_accuracy  \\\n0  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.122419   \n1  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.122408   \n2  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.115742   \n3  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.114636   \n4  {'batch_size': 64, 'embedding_dim': 100, 'lear...      0.125488   \n\n   val_top_3_accuracy  val_top_5_accuracy  val_top_10_accuracy   total time  \n0            0.227599            0.288879             0.380460  1132.265152  \n1            0.227408            0.288740             0.380246  1163.120735  \n2            0.217964            0.277909             0.368647   258.736813  \n3            0.216128            0.276872             0.368416   259.857931  \n4            0.232994            0.295295             0.388821  1146.043770  \n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def tune_hyperparams_full_weighted(param_grid, train_dataset, val_dataset):\n  results = []\n  for params in param_grid:\n    start_time = time.time()\n    batch_size = params['batch_size']\n    embedding_dim = params['embedding_dim']\n    learning_rate = params['learning_rate']\n    num_epochs = params['num_epochs']\n\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n\n    model = CBOWModelWeighted(len(vocab_full), embedding_dim, 5).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n      model.train()\n      total_loss = 0.0\n      for context, target in train_loader:\n        context, target = context.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(context)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n      avg_train_loss = total_loss / len(train_loader)\n      train_losses.append(avg_train_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Training Loss: {avg_train_loss:.4f}\")\n\n      model.eval()\n      val_loss = 0.0\n      val_accuracy = 0.0\n      val_top_3_accuracy = 0.0\n      val_top_5_accuracy = 0.0\n      val_top_10_accuracy = 0.0\n      with torch.no_grad():\n        for context, target in val_loader:\n          context, target = context.to(device), target.to(device)\n          output = model(context)\n          loss = criterion(output, target)\n          val_loss += loss.item()\n\n          predictions = torch.argmax(output, dim = 1)\n          val_accuracy += accuracy_score(target.cpu(), predictions.cpu())\n\n          top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n          val_top_3_accuracy += top_k_accuracies['top_3_accuracy']\n          val_top_5_accuracy += top_k_accuracies['top_5_accuracy']\n          val_top_10_accuracy += top_k_accuracies['top_10_accuracy']\n\n      avg_val_loss = val_loss / len(val_loader)\n      avg_val_accuracy = val_accuracy / len(val_loader)\n      avg_top_3_accuracy = val_top_3_accuracy / len(val_loader)\n      avg_top_5_accuracy = val_top_5_accuracy / len(val_loader)\n      avg_top_10_accuracy = val_top_10_accuracy / len(val_loader)\n      val_losses.append(avg_val_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Validation Loss: {avg_val_loss:.4f}\")\n      print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n      print(f\"Top-3 Accuracy: {avg_top_3_accuracy:.4f}\")\n      print(f\"Top-5 Accuracy: {avg_top_5_accuracy:.4f}\")\n      print(f\"Top-10 Accuracy; {avg_top_10_accuracy:.4f}\")\n\n      if abs(avg_val_loss - best_val_loss) < 1e-3:\n        print(\"Early Stopping Triggered\")\n        break\n      elif avg_val_loss > best_val_loss:\n        print(\"Early Stopping Triggered\")\n        break\n      else:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), \"best_cbow_weighted_model_full.pth\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    results.append({\n        'params': params,\n        'val_accuracy': avg_val_accuracy,\n        'val_top_3_accuracy': avg_top_3_accuracy,\n        'val_top_5_accuracy': avg_top_5_accuracy,\n        'val_top_10_accuracy': avg_top_10_accuracy,\n        'total time': total_time\n    })\n  return results\n\ntrain_pairs_full, val_test_pairs_full = train_test_split(encoded_pairs_full, test_size = 0.70, random_state = 42)\nval_pairs_full, test_pairs_full = train_test_split(val_test_pairs_full, test_size = 0.5, random_state = 42)\n\ntrain_dataset_full= CBOWDataset(train_pairs_full, len(vocab_full), window_size = 5)\nval_dataset_full = CBOWDataset(val_pairs_full, len(vocab_full), window_size = 5)\ntest_dataset_full = CBOWDataset(test_pairs_full, len(vocab_full), window_size = 5)\n\nresults_full_weighted = tune_hyperparams_full_weighted(param_grid_dicts, train_dataset_full, val_dataset_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:44:11.219421Z","iopub.execute_input":"2024-12-21T04:44:11.219752Z","iopub.status.idle":"2024-12-21T09:52:27.726714Z","shell.execute_reply.started":"2024-12-21T04:44:11.219723Z","shell.execute_reply":"2024-12-21T09:52:27.725731Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:\nTraining Loss: 6.1885\nEpoch 1:\nValidation Loss: 6.0146\nValidation Accuracy: 0.1079\nTop-3 Accuracy: 0.2056\nTop-5 Accuracy: 0.2640\nTop-10 Accuracy; 0.3533\nEpoch 2:\nTraining Loss: 5.9073\nEpoch 2:\nValidation Loss: 5.9238\nValidation Accuracy: 0.1163\nTop-3 Accuracy: 0.2184\nTop-5 Accuracy: 0.2785\nTop-10 Accuracy; 0.3699\nEpoch 3:\nTraining Loss: 5.8111\nEpoch 3:\nValidation Loss: 5.8759\nValidation Accuracy: 0.1211\nTop-3 Accuracy: 0.2259\nTop-5 Accuracy: 0.2868\nTop-10 Accuracy; 0.3785\nEpoch 4:\nTraining Loss: 5.7480\nEpoch 4:\nValidation Loss: 5.8541\nValidation Accuracy: 0.1232\nTop-3 Accuracy: 0.2295\nTop-5 Accuracy: 0.2910\nTop-10 Accuracy; 0.3835\nEpoch 5:\nTraining Loss: 5.6989\nEpoch 5:\nValidation Loss: 5.8549\nValidation Accuracy: 0.1252\nTop-3 Accuracy: 0.2321\nTop-5 Accuracy: 0.2937\nTop-10 Accuracy; 0.3861\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1907\nEpoch 1:\nValidation Loss: 6.0188\nValidation Accuracy: 0.1082\nTop-3 Accuracy: 0.2058\nTop-5 Accuracy: 0.2639\nTop-10 Accuracy; 0.3528\nEpoch 2:\nTraining Loss: 5.9118\nEpoch 2:\nValidation Loss: 5.9281\nValidation Accuracy: 0.1165\nTop-3 Accuracy: 0.2182\nTop-5 Accuracy: 0.2779\nTop-10 Accuracy; 0.3686\nEpoch 3:\nTraining Loss: 5.8164\nEpoch 3:\nValidation Loss: 5.8800\nValidation Accuracy: 0.1210\nTop-3 Accuracy: 0.2254\nTop-5 Accuracy: 0.2861\nTop-10 Accuracy; 0.3775\nEpoch 4:\nTraining Loss: 5.7539\nEpoch 4:\nValidation Loss: 5.8575\nValidation Accuracy: 0.1231\nTop-3 Accuracy: 0.2288\nTop-5 Accuracy: 0.2905\nTop-10 Accuracy; 0.3828\nEpoch 5:\nTraining Loss: 5.7043\nEpoch 5:\nValidation Loss: 5.8608\nValidation Accuracy: 0.1247\nTop-3 Accuracy: 0.2316\nTop-5 Accuracy: 0.2933\nTop-10 Accuracy; 0.3857\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.0791\nEpoch 1:\nValidation Loss: 5.9814\nValidation Accuracy: 0.1159\nTop-3 Accuracy: 0.2187\nTop-5 Accuracy: 0.2794\nTop-10 Accuracy; 0.3708\nEpoch 2:\nTraining Loss: 5.8342\nEpoch 2:\nValidation Loss: 5.9921\nValidation Accuracy: 0.1173\nTop-3 Accuracy: 0.2202\nTop-5 Accuracy: 0.2805\nTop-10 Accuracy; 0.3711\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.0802\nEpoch 1:\nValidation Loss: 5.9791\nValidation Accuracy: 0.1169\nTop-3 Accuracy: 0.2189\nTop-5 Accuracy: 0.2793\nTop-10 Accuracy; 0.3704\nEpoch 2:\nTraining Loss: 5.8360\nEpoch 2:\nValidation Loss: 5.9911\nValidation Accuracy: 0.1159\nTop-3 Accuracy: 0.2193\nTop-5 Accuracy: 0.2805\nTop-10 Accuracy; 0.3712\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1038\nEpoch 1:\nValidation Loss: 5.9290\nValidation Accuracy: 0.1149\nTop-3 Accuracy: 0.2162\nTop-5 Accuracy: 0.2759\nTop-10 Accuracy; 0.3667\nEpoch 2:\nTraining Loss: 5.7964\nEpoch 2:\nValidation Loss: 5.8435\nValidation Accuracy: 0.1227\nTop-3 Accuracy: 0.2282\nTop-5 Accuracy: 0.2901\nTop-10 Accuracy; 0.3829\nEpoch 3:\nTraining Loss: 5.6833\nEpoch 3:\nValidation Loss: 5.8024\nValidation Accuracy: 0.1259\nTop-3 Accuracy: 0.2338\nTop-5 Accuracy: 0.2964\nTop-10 Accuracy; 0.3899\nEpoch 4:\nTraining Loss: 5.6026\nEpoch 4:\nValidation Loss: 5.7901\nValidation Accuracy: 0.1280\nTop-3 Accuracy: 0.2371\nTop-5 Accuracy: 0.3002\nTop-10 Accuracy; 0.3940\nEpoch 5:\nTraining Loss: 5.5326\nEpoch 5:\nValidation Loss: 5.8012\nValidation Accuracy: 0.1286\nTop-3 Accuracy: 0.2384\nTop-5 Accuracy: 0.3020\nTop-10 Accuracy; 0.3964\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1046\nEpoch 1:\nValidation Loss: 5.9314\nValidation Accuracy: 0.1150\nTop-3 Accuracy: 0.2166\nTop-5 Accuracy: 0.2769\nTop-10 Accuracy; 0.3680\nEpoch 2:\nTraining Loss: 5.7956\nEpoch 2:\nValidation Loss: 5.8449\nValidation Accuracy: 0.1223\nTop-3 Accuracy: 0.2285\nTop-5 Accuracy: 0.2902\nTop-10 Accuracy; 0.3826\nEpoch 3:\nTraining Loss: 5.6825\nEpoch 3:\nValidation Loss: 5.8013\nValidation Accuracy: 0.1263\nTop-3 Accuracy: 0.2344\nTop-5 Accuracy: 0.2971\nTop-10 Accuracy; 0.3901\nEpoch 4:\nTraining Loss: 5.6025\nEpoch 4:\nValidation Loss: 5.7900\nValidation Accuracy: 0.1281\nTop-3 Accuracy: 0.2375\nTop-5 Accuracy: 0.3007\nTop-10 Accuracy; 0.3941\nEpoch 5:\nTraining Loss: 5.5312\nEpoch 5:\nValidation Loss: 5.8020\nValidation Accuracy: 0.1291\nTop-3 Accuracy: 0.2388\nTop-5 Accuracy: 0.3023\nTop-10 Accuracy; 0.3963\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1100\nEpoch 1:\nValidation Loss: 6.0285\nValidation Accuracy: 0.1126\nTop-3 Accuracy: 0.2164\nTop-5 Accuracy: 0.2781\nTop-10 Accuracy; 0.3700\nEpoch 2:\nTraining Loss: 5.8099\nEpoch 2:\nValidation Loss: 6.0616\nValidation Accuracy: 0.1162\nTop-3 Accuracy: 0.2179\nTop-5 Accuracy: 0.2781\nTop-10 Accuracy; 0.3689\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1091\nEpoch 1:\nValidation Loss: 6.0300\nValidation Accuracy: 0.1145\nTop-3 Accuracy: 0.2162\nTop-5 Accuracy: 0.2770\nTop-10 Accuracy; 0.3688\nEpoch 2:\nTraining Loss: 5.8090\nEpoch 2:\nValidation Loss: 6.0542\nValidation Accuracy: 0.1160\nTop-3 Accuracy: 0.2183\nTop-5 Accuracy: 0.2787\nTop-10 Accuracy; 0.3698\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2102\nEpoch 1:\nValidation Loss: 5.9936\nValidation Accuracy: 0.1046\nTop-3 Accuracy: 0.2003\nTop-5 Accuracy: 0.2579\nTop-10 Accuracy; 0.3476\nEpoch 2:\nTraining Loss: 5.8669\nEpoch 2:\nValidation Loss: 5.8928\nValidation Accuracy: 0.1139\nTop-3 Accuracy: 0.2140\nTop-5 Accuracy: 0.2736\nTop-10 Accuracy; 0.3638\nEpoch 3:\nTraining Loss: 5.7616\nEpoch 3:\nValidation Loss: 5.8442\nValidation Accuracy: 0.1185\nTop-3 Accuracy: 0.2212\nTop-5 Accuracy: 0.2813\nTop-10 Accuracy; 0.3721\nEpoch 4:\nTraining Loss: 5.6984\nEpoch 4:\nValidation Loss: 5.8148\nValidation Accuracy: 0.1209\nTop-3 Accuracy: 0.2248\nTop-5 Accuracy: 0.2858\nTop-10 Accuracy; 0.3774\nEpoch 5:\nTraining Loss: 5.6528\nEpoch 5:\nValidation Loss: 5.7941\nValidation Accuracy: 0.1228\nTop-3 Accuracy: 0.2282\nTop-5 Accuracy: 0.2895\nTop-10 Accuracy; 0.3813\nEpoch 6:\nTraining Loss: 5.6167\nEpoch 6:\nValidation Loss: 5.7801\nValidation Accuracy: 0.1244\nTop-3 Accuracy: 0.2302\nTop-5 Accuracy: 0.2921\nTop-10 Accuracy; 0.3842\nEpoch 7:\nTraining Loss: 5.5858\nEpoch 7:\nValidation Loss: 5.7678\nValidation Accuracy: 0.1249\nTop-3 Accuracy: 0.2318\nTop-5 Accuracy: 0.2934\nTop-10 Accuracy; 0.3857\nEpoch 8:\nTraining Loss: 5.5589\nEpoch 8:\nValidation Loss: 5.7592\nValidation Accuracy: 0.1256\nTop-3 Accuracy: 0.2327\nTop-5 Accuracy: 0.2946\nTop-10 Accuracy; 0.3870\nEpoch 9:\nTraining Loss: 5.5347\nEpoch 9:\nValidation Loss: 5.7527\nValidation Accuracy: 0.1261\nTop-3 Accuracy: 0.2337\nTop-5 Accuracy: 0.2959\nTop-10 Accuracy; 0.3889\nEpoch 10:\nTraining Loss: 5.5112\nEpoch 10:\nValidation Loss: 5.7483\nValidation Accuracy: 0.1266\nTop-3 Accuracy: 0.2345\nTop-5 Accuracy: 0.2966\nTop-10 Accuracy; 0.3893\nEpoch 1:\nTraining Loss: 6.2165\nEpoch 1:\nValidation Loss: 5.9981\nValidation Accuracy: 0.1038\nTop-3 Accuracy: 0.1995\nTop-5 Accuracy: 0.2574\nTop-10 Accuracy; 0.3465\nEpoch 2:\nTraining Loss: 5.8715\nEpoch 2:\nValidation Loss: 5.8967\nValidation Accuracy: 0.1131\nTop-3 Accuracy: 0.2136\nTop-5 Accuracy: 0.2732\nTop-10 Accuracy; 0.3636\nEpoch 3:\nTraining Loss: 5.7650\nEpoch 3:\nValidation Loss: 5.8458\nValidation Accuracy: 0.1178\nTop-3 Accuracy: 0.2207\nTop-5 Accuracy: 0.2809\nTop-10 Accuracy; 0.3721\nEpoch 4:\nTraining Loss: 5.7005\nEpoch 4:\nValidation Loss: 5.8169\nValidation Accuracy: 0.1204\nTop-3 Accuracy: 0.2247\nTop-5 Accuracy: 0.2853\nTop-10 Accuracy; 0.3770\nEpoch 5:\nTraining Loss: 5.6544\nEpoch 5:\nValidation Loss: 5.7963\nValidation Accuracy: 0.1222\nTop-3 Accuracy: 0.2273\nTop-5 Accuracy: 0.2883\nTop-10 Accuracy; 0.3805\nEpoch 6:\nTraining Loss: 5.6177\nEpoch 6:\nValidation Loss: 5.7802\nValidation Accuracy: 0.1233\nTop-3 Accuracy: 0.2295\nTop-5 Accuracy: 0.2909\nTop-10 Accuracy; 0.3835\nEpoch 7:\nTraining Loss: 5.5866\nEpoch 7:\nValidation Loss: 5.7687\nValidation Accuracy: 0.1250\nTop-3 Accuracy: 0.2314\nTop-5 Accuracy: 0.2931\nTop-10 Accuracy; 0.3855\nEpoch 8:\nTraining Loss: 5.5594\nEpoch 8:\nValidation Loss: 5.7598\nValidation Accuracy: 0.1255\nTop-3 Accuracy: 0.2325\nTop-5 Accuracy: 0.2943\nTop-10 Accuracy; 0.3870\nEpoch 9:\nTraining Loss: 5.5348\nEpoch 9:\nValidation Loss: 5.7527\nValidation Accuracy: 0.1260\nTop-3 Accuracy: 0.2334\nTop-5 Accuracy: 0.2956\nTop-10 Accuracy; 0.3884\nEpoch 10:\nTraining Loss: 5.5123\nEpoch 10:\nValidation Loss: 5.7488\nValidation Accuracy: 0.1267\nTop-3 Accuracy: 0.2343\nTop-5 Accuracy: 0.2966\nTop-10 Accuracy; 0.3895\nEpoch 11:\nTraining Loss: 5.4908\nEpoch 11:\nValidation Loss: 5.7471\nValidation Accuracy: 0.1267\nTop-3 Accuracy: 0.2345\nTop-5 Accuracy: 0.2969\nTop-10 Accuracy; 0.3898\nEpoch 12:\nTraining Loss: 5.4700\nEpoch 12:\nValidation Loss: 5.7498\nValidation Accuracy: 0.1269\nTop-3 Accuracy: 0.2349\nTop-5 Accuracy: 0.2976\nTop-10 Accuracy; 0.3903\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 5.9956\nEpoch 1:\nValidation Loss: 5.8618\nValidation Accuracy: 0.1196\nTop-3 Accuracy: 0.2243\nTop-5 Accuracy: 0.2853\nTop-10 Accuracy; 0.3766\nEpoch 2:\nTraining Loss: 5.6920\nEpoch 2:\nValidation Loss: 5.8646\nValidation Accuracy: 0.1197\nTop-3 Accuracy: 0.2244\nTop-5 Accuracy: 0.2863\nTop-10 Accuracy; 0.3782\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 5.9987\nEpoch 1:\nValidation Loss: 5.8639\nValidation Accuracy: 0.1195\nTop-3 Accuracy: 0.2245\nTop-5 Accuracy: 0.2854\nTop-10 Accuracy; 0.3766\nEpoch 2:\nTraining Loss: 5.6938\nEpoch 2:\nValidation Loss: 5.8649\nValidation Accuracy: 0.1201\nTop-3 Accuracy: 0.2247\nTop-5 Accuracy: 0.2860\nTop-10 Accuracy; 0.3780\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1147\nEpoch 1:\nValidation Loss: 5.9009\nValidation Accuracy: 0.1123\nTop-3 Accuracy: 0.2122\nTop-5 Accuracy: 0.2716\nTop-10 Accuracy; 0.3625\nEpoch 2:\nTraining Loss: 5.7455\nEpoch 2:\nValidation Loss: 5.8070\nValidation Accuracy: 0.1205\nTop-3 Accuracy: 0.2249\nTop-5 Accuracy: 0.2856\nTop-10 Accuracy; 0.3775\nEpoch 3:\nTraining Loss: 5.6215\nEpoch 3:\nValidation Loss: 5.7637\nValidation Accuracy: 0.1243\nTop-3 Accuracy: 0.2312\nTop-5 Accuracy: 0.2932\nTop-10 Accuracy; 0.3861\nEpoch 4:\nTraining Loss: 5.5420\nEpoch 4:\nValidation Loss: 5.7391\nValidation Accuracy: 0.1263\nTop-3 Accuracy: 0.2345\nTop-5 Accuracy: 0.2970\nTop-10 Accuracy; 0.3905\nEpoch 5:\nTraining Loss: 5.4811\nEpoch 5:\nValidation Loss: 5.7232\nValidation Accuracy: 0.1276\nTop-3 Accuracy: 0.2368\nTop-5 Accuracy: 0.2998\nTop-10 Accuracy; 0.3933\nEpoch 6:\nTraining Loss: 5.4308\nEpoch 6:\nValidation Loss: 5.7137\nValidation Accuracy: 0.1286\nTop-3 Accuracy: 0.2384\nTop-5 Accuracy: 0.3016\nTop-10 Accuracy; 0.3955\nEpoch 7:\nTraining Loss: 5.3867\nEpoch 7:\nValidation Loss: 5.7066\nValidation Accuracy: 0.1290\nTop-3 Accuracy: 0.2393\nTop-5 Accuracy: 0.3028\nTop-10 Accuracy; 0.3971\nEpoch 8:\nTraining Loss: 5.3469\nEpoch 8:\nValidation Loss: 5.7035\nValidation Accuracy: 0.1296\nTop-3 Accuracy: 0.2398\nTop-5 Accuracy: 0.3034\nTop-10 Accuracy; 0.3980\nEpoch 9:\nTraining Loss: 5.3095\nEpoch 9:\nValidation Loss: 5.7031\nValidation Accuracy: 0.1301\nTop-3 Accuracy: 0.2406\nTop-5 Accuracy: 0.3044\nTop-10 Accuracy; 0.3987\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.1130\nEpoch 1:\nValidation Loss: 5.9003\nValidation Accuracy: 0.1117\nTop-3 Accuracy: 0.2119\nTop-5 Accuracy: 0.2712\nTop-10 Accuracy; 0.3621\nEpoch 2:\nTraining Loss: 5.7433\nEpoch 2:\nValidation Loss: 5.8066\nValidation Accuracy: 0.1203\nTop-3 Accuracy: 0.2250\nTop-5 Accuracy: 0.2862\nTop-10 Accuracy; 0.3784\nEpoch 3:\nTraining Loss: 5.6201\nEpoch 3:\nValidation Loss: 5.7645\nValidation Accuracy: 0.1235\nTop-3 Accuracy: 0.2301\nTop-5 Accuracy: 0.2923\nTop-10 Accuracy; 0.3851\nEpoch 4:\nTraining Loss: 5.5404\nEpoch 4:\nValidation Loss: 5.7386\nValidation Accuracy: 0.1263\nTop-3 Accuracy: 0.2342\nTop-5 Accuracy: 0.2970\nTop-10 Accuracy; 0.3907\nEpoch 5:\nTraining Loss: 5.4803\nEpoch 5:\nValidation Loss: 5.7236\nValidation Accuracy: 0.1276\nTop-3 Accuracy: 0.2365\nTop-5 Accuracy: 0.2995\nTop-10 Accuracy; 0.3933\nEpoch 6:\nTraining Loss: 5.4307\nEpoch 6:\nValidation Loss: 5.7134\nValidation Accuracy: 0.1285\nTop-3 Accuracy: 0.2375\nTop-5 Accuracy: 0.3010\nTop-10 Accuracy; 0.3953\nEpoch 7:\nTraining Loss: 5.3868\nEpoch 7:\nValidation Loss: 5.7080\nValidation Accuracy: 0.1294\nTop-3 Accuracy: 0.2392\nTop-5 Accuracy: 0.3026\nTop-10 Accuracy; 0.3969\nEpoch 8:\nTraining Loss: 5.3469\nEpoch 8:\nValidation Loss: 5.7050\nValidation Accuracy: 0.1294\nTop-3 Accuracy: 0.2394\nTop-5 Accuracy: 0.3032\nTop-10 Accuracy; 0.3978\nEpoch 9:\nTraining Loss: 5.3100\nEpoch 9:\nValidation Loss: 5.7038\nValidation Accuracy: 0.1299\nTop-3 Accuracy: 0.2401\nTop-5 Accuracy: 0.3036\nTop-10 Accuracy; 0.3984\nEpoch 10:\nTraining Loss: 5.2743\nEpoch 10:\nValidation Loss: 5.7059\nValidation Accuracy: 0.1293\nTop-3 Accuracy: 0.2400\nTop-5 Accuracy: 0.3043\nTop-10 Accuracy; 0.3993\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 5.9921\nEpoch 1:\nValidation Loss: 5.8692\nValidation Accuracy: 0.1190\nTop-3 Accuracy: 0.2233\nTop-5 Accuracy: 0.2854\nTop-10 Accuracy; 0.3790\nEpoch 2:\nTraining Loss: 5.6036\nEpoch 2:\nValidation Loss: 5.8995\nValidation Accuracy: 0.1191\nTop-3 Accuracy: 0.2246\nTop-5 Accuracy: 0.2863\nTop-10 Accuracy; 0.3794\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 5.9945\nEpoch 1:\nValidation Loss: 5.8712\nValidation Accuracy: 0.1188\nTop-3 Accuracy: 0.2238\nTop-5 Accuracy: 0.2858\nTop-10 Accuracy; 0.3788\nEpoch 2:\nTraining Loss: 5.6040\nEpoch 2:\nValidation Loss: 5.9030\nValidation Accuracy: 0.1194\nTop-3 Accuracy: 0.2240\nTop-5 Accuracy: 0.2861\nTop-10 Accuracy; 0.3794\nEarly Stopping Triggered\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"results_full_weighted_df = pd.DataFrame(results_full_weighted)\nresults_full_weighted_df.to_csv('weighted_model_full_hyperparam_results.csv', index = False)\nprint(results_full_weighted_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T09:52:27.728178Z","iopub.execute_input":"2024-12-21T09:52:27.728572Z","iopub.status.idle":"2024-12-21T09:52:27.739555Z","shell.execute_reply.started":"2024-12-21T09:52:27.728535Z","shell.execute_reply":"2024-12-21T09:52:27.738733Z"}},"outputs":[{"name":"stdout","text":"                                              params  val_accuracy  \\\n0  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.125187   \n1  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.124740   \n2  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.117284   \n3  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.115852   \n4  {'batch_size': 64, 'embedding_dim': 100, 'lear...      0.128640   \n\n   val_top_3_accuracy  val_top_5_accuracy  val_top_10_accuracy   total time  \n0            0.232135            0.293705             0.386132  1363.852431  \n1            0.231576            0.293288             0.385661  1382.852483  \n2            0.220186            0.280531             0.371106   559.355721  \n3            0.219265            0.280504             0.371204   552.773860  \n4            0.238421            0.302015             0.396428  1526.194981  \n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def tune_hyperparams_quarter_weighted(param_grid, train_dataset, val_dataset):\n  results = []\n  for params in param_grid:\n    start_time = time.time()\n    batch_size = params['batch_size']\n    embedding_dim = params['embedding_dim']\n    learning_rate = params['learning_rate']\n    num_epochs = params['num_epochs']\n\n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n\n    model = CBOWModelWeighted(len(vocab_quarter), embedding_dim, 5).to(device)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_loss = float('inf')\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n      model.train()\n      total_loss = 0.0\n      for context, target in train_loader:\n        context, target = context.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(context)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n      avg_train_loss = total_loss / len(train_loader)\n      train_losses.append(avg_train_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Training Loss: {avg_train_loss:.4f}\")\n\n      model.eval()\n      val_loss = 0.0\n      val_accuracy = 0.0\n      val_top_3_accuracy = 0.0\n      val_top_5_accuracy = 0.0\n      val_top_10_accuracy = 0.0\n      with torch.no_grad():\n        for context, target in val_loader:\n          context, target = context.to(device), target.to(device)\n          output = model(context)\n          loss = criterion(output, target)\n          val_loss += loss.item()\n\n          predictions = torch.argmax(output, dim = 1)\n          val_accuracy += accuracy_score(target.cpu(), predictions.cpu())\n\n          top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n          val_top_3_accuracy += top_k_accuracies['top_3_accuracy']\n          val_top_5_accuracy += top_k_accuracies['top_5_accuracy']\n          val_top_10_accuracy += top_k_accuracies['top_10_accuracy']\n\n      avg_val_loss = val_loss / len(val_loader)\n      avg_val_accuracy = val_accuracy / len(val_loader)\n      avg_top_3_accuracy = val_top_3_accuracy / len(val_loader)\n      avg_top_5_accuracy = val_top_5_accuracy / len(val_loader)\n      avg_top_10_accuracy = val_top_10_accuracy / len(val_loader)\n      val_losses.append(avg_val_loss)\n\n      print(f\"Epoch {epoch+1}:\")\n      print(f\"Validation Loss: {avg_val_loss:.4f}\")\n      print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n      print(f\"Top-3 Accuracy: {avg_top_3_accuracy:.4f}\")\n      print(f\"Top-5 Accuracy: {avg_top_5_accuracy:.4f}\")\n      print(f\"Top-10 Accuracy: {avg_top_10_accuracy:.4f}\")\n\n      if abs(avg_val_loss - best_val_loss) < 1e-3:\n        print(\"Early Stopping Triggered\")\n        break\n      elif avg_val_loss > best_val_loss:\n        print(\"Early Stopping Triggered\")\n        break\n      else:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), \"best_cbow_weighted_model_quarter.pth\")\n\n    end_time = time.time()\n    total_time = end_time - start_time\n    results.append({\n        'params': params,\n        'val_accuracy': avg_val_accuracy,\n        'val_top_3_accuracy': avg_top_3_accuracy,\n        'val_top_5_accuracy': avg_top_5_accuracy,\n        'val_top_10_accuracy': avg_top_10_accuracy,\n        'total time': total_time\n    })\n  return results\n\ntrain_pairs_quarter, val_test_pairs_quarter = train_test_split(encoded_pairs_quarter, test_size = 0.70, random_state = 42)\nval_pairs_quarter, test_pairs_quarter = train_test_split(val_test_pairs_quarter, test_size = 0.5, random_state = 42)\n\ntrain_dataset_quarter = CBOWDataset(train_pairs_quarter, len(vocab_quarter), window_size = 5)\nval_dataset_quarter = CBOWDataset(val_pairs_quarter, len(vocab_quarter), window_size = 5)\ntest_dataset_quarter = CBOWDataset(test_pairs_quarter, len(vocab_quarter), window_size = 5)\n\nresults_quarter_weighted = tune_hyperparams_quarter_weighted(param_grid_dicts, train_dataset_quarter, val_dataset_quarter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T09:52:27.740816Z","iopub.execute_input":"2024-12-21T09:52:27.741138Z","iopub.status.idle":"2024-12-21T11:12:36.722021Z","shell.execute_reply.started":"2024-12-21T09:52:27.741113Z","shell.execute_reply":"2024-12-21T11:12:36.721199Z"}},"outputs":[{"name":"stdout","text":"Epoch 1:\nTraining Loss: 6.4614\nEpoch 1:\nValidation Loss: 6.2059\nValidation Accuracy: 0.0883\nTop-3 Accuracy: 0.1756\nTop-5 Accuracy: 0.2301\nTop-10 Accuracy: 0.3187\nEpoch 2:\nTraining Loss: 5.9935\nEpoch 2:\nValidation Loss: 6.1234\nValidation Accuracy: 0.0989\nTop-3 Accuracy: 0.1913\nTop-5 Accuracy: 0.2479\nTop-10 Accuracy: 0.3370\nEpoch 3:\nTraining Loss: 5.8432\nEpoch 3:\nValidation Loss: 6.0787\nValidation Accuracy: 0.1044\nTop-3 Accuracy: 0.1997\nTop-5 Accuracy: 0.2576\nTop-10 Accuracy: 0.3467\nEpoch 4:\nTraining Loss: 5.7439\nEpoch 4:\nValidation Loss: 6.0486\nValidation Accuracy: 0.1083\nTop-3 Accuracy: 0.2059\nTop-5 Accuracy: 0.2640\nTop-10 Accuracy: 0.3532\nEpoch 5:\nTraining Loss: 5.6696\nEpoch 5:\nValidation Loss: 6.0306\nValidation Accuracy: 0.1110\nTop-3 Accuracy: 0.2097\nTop-5 Accuracy: 0.2684\nTop-10 Accuracy: 0.3581\nEpoch 6:\nTraining Loss: 5.6100\nEpoch 6:\nValidation Loss: 6.0188\nValidation Accuracy: 0.1126\nTop-3 Accuracy: 0.2123\nTop-5 Accuracy: 0.2716\nTop-10 Accuracy: 0.3613\nEpoch 7:\nTraining Loss: 5.5607\nEpoch 7:\nValidation Loss: 6.0110\nValidation Accuracy: 0.1142\nTop-3 Accuracy: 0.2143\nTop-5 Accuracy: 0.2730\nTop-10 Accuracy: 0.3633\nEpoch 8:\nTraining Loss: 5.5175\nEpoch 8:\nValidation Loss: 6.0039\nValidation Accuracy: 0.1154\nTop-3 Accuracy: 0.2163\nTop-5 Accuracy: 0.2751\nTop-10 Accuracy: 0.3656\nEpoch 9:\nTraining Loss: 5.4791\nEpoch 9:\nValidation Loss: 6.0020\nValidation Accuracy: 0.1162\nTop-3 Accuracy: 0.2172\nTop-5 Accuracy: 0.2763\nTop-10 Accuracy: 0.3669\nEpoch 10:\nTraining Loss: 5.4443\nEpoch 10:\nValidation Loss: 5.9991\nValidation Accuracy: 0.1167\nTop-3 Accuracy: 0.2179\nTop-5 Accuracy: 0.2779\nTop-10 Accuracy: 0.3684\nEpoch 1:\nTraining Loss: 6.4591\nEpoch 1:\nValidation Loss: 6.2041\nValidation Accuracy: 0.0885\nTop-3 Accuracy: 0.1757\nTop-5 Accuracy: 0.2312\nTop-10 Accuracy: 0.3203\nEpoch 2:\nTraining Loss: 5.9918\nEpoch 2:\nValidation Loss: 6.1198\nValidation Accuracy: 0.0987\nTop-3 Accuracy: 0.1914\nTop-5 Accuracy: 0.2489\nTop-10 Accuracy: 0.3375\nEpoch 3:\nTraining Loss: 5.8401\nEpoch 3:\nValidation Loss: 6.0750\nValidation Accuracy: 0.1046\nTop-3 Accuracy: 0.2003\nTop-5 Accuracy: 0.2584\nTop-10 Accuracy: 0.3475\nEpoch 4:\nTraining Loss: 5.7404\nEpoch 4:\nValidation Loss: 6.0471\nValidation Accuracy: 0.1086\nTop-3 Accuracy: 0.2059\nTop-5 Accuracy: 0.2646\nTop-10 Accuracy: 0.3539\nEpoch 5:\nTraining Loss: 5.6661\nEpoch 5:\nValidation Loss: 6.0289\nValidation Accuracy: 0.1114\nTop-3 Accuracy: 0.2096\nTop-5 Accuracy: 0.2687\nTop-10 Accuracy: 0.3581\nEpoch 6:\nTraining Loss: 5.6072\nEpoch 6:\nValidation Loss: 6.0166\nValidation Accuracy: 0.1131\nTop-3 Accuracy: 0.2124\nTop-5 Accuracy: 0.2717\nTop-10 Accuracy: 0.3612\nEpoch 7:\nTraining Loss: 5.5578\nEpoch 7:\nValidation Loss: 6.0084\nValidation Accuracy: 0.1144\nTop-3 Accuracy: 0.2147\nTop-5 Accuracy: 0.2740\nTop-10 Accuracy: 0.3641\nEpoch 8:\nTraining Loss: 5.5146\nEpoch 8:\nValidation Loss: 6.0036\nValidation Accuracy: 0.1152\nTop-3 Accuracy: 0.2161\nTop-5 Accuracy: 0.2756\nTop-10 Accuracy: 0.3658\nEpoch 9:\nTraining Loss: 5.4768\nEpoch 9:\nValidation Loss: 6.0005\nValidation Accuracy: 0.1162\nTop-3 Accuracy: 0.2178\nTop-5 Accuracy: 0.2771\nTop-10 Accuracy: 0.3673\nEpoch 10:\nTraining Loss: 5.4419\nEpoch 10:\nValidation Loss: 5.9995\nValidation Accuracy: 0.1165\nTop-3 Accuracy: 0.2176\nTop-5 Accuracy: 0.2776\nTop-10 Accuracy: 0.3678\nEpoch 11:\nTraining Loss: 5.4095\nEpoch 11:\nValidation Loss: 5.9980\nValidation Accuracy: 0.1174\nTop-3 Accuracy: 0.2197\nTop-5 Accuracy: 0.2791\nTop-10 Accuracy: 0.3697\nEpoch 12:\nTraining Loss: 5.3796\nEpoch 12:\nValidation Loss: 5.9986\nValidation Accuracy: 0.1175\nTop-3 Accuracy: 0.2194\nTop-5 Accuracy: 0.2793\nTop-10 Accuracy: 0.3694\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2470\nEpoch 1:\nValidation Loss: 6.0468\nValidation Accuracy: 0.1114\nTop-3 Accuracy: 0.2103\nTop-5 Accuracy: 0.2693\nTop-10 Accuracy: 0.3588\nEpoch 2:\nTraining Loss: 5.6804\nEpoch 2:\nValidation Loss: 6.0781\nValidation Accuracy: 0.1129\nTop-3 Accuracy: 0.2130\nTop-5 Accuracy: 0.2730\nTop-10 Accuracy: 0.3632\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2461\nEpoch 1:\nValidation Loss: 6.0493\nValidation Accuracy: 0.1083\nTop-3 Accuracy: 0.2071\nTop-5 Accuracy: 0.2671\nTop-10 Accuracy: 0.3578\nEpoch 2:\nTraining Loss: 5.6824\nEpoch 2:\nValidation Loss: 6.0787\nValidation Accuracy: 0.1128\nTop-3 Accuracy: 0.2127\nTop-5 Accuracy: 0.2724\nTop-10 Accuracy: 0.3628\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.3676\nEpoch 1:\nValidation Loss: 6.1221\nValidation Accuracy: 0.0953\nTop-3 Accuracy: 0.1871\nTop-5 Accuracy: 0.2443\nTop-10 Accuracy: 0.3346\nEpoch 2:\nTraining Loss: 5.8590\nEpoch 2:\nValidation Loss: 6.0451\nValidation Accuracy: 0.1058\nTop-3 Accuracy: 0.2020\nTop-5 Accuracy: 0.2605\nTop-10 Accuracy: 0.3507\nEpoch 3:\nTraining Loss: 5.6694\nEpoch 3:\nValidation Loss: 6.0079\nValidation Accuracy: 0.1117\nTop-3 Accuracy: 0.2104\nTop-5 Accuracy: 0.2697\nTop-10 Accuracy: 0.3609\nEpoch 4:\nTraining Loss: 5.5365\nEpoch 4:\nValidation Loss: 5.9872\nValidation Accuracy: 0.1150\nTop-3 Accuracy: 0.2148\nTop-5 Accuracy: 0.2748\nTop-10 Accuracy: 0.3660\nEpoch 5:\nTraining Loss: 5.4313\nEpoch 5:\nValidation Loss: 5.9794\nValidation Accuracy: 0.1173\nTop-3 Accuracy: 0.2184\nTop-5 Accuracy: 0.2784\nTop-10 Accuracy: 0.3697\nEpoch 6:\nTraining Loss: 5.3441\nEpoch 6:\nValidation Loss: 5.9787\nValidation Accuracy: 0.1187\nTop-3 Accuracy: 0.2202\nTop-5 Accuracy: 0.2807\nTop-10 Accuracy: 0.3724\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.3662\nEpoch 1:\nValidation Loss: 6.1196\nValidation Accuracy: 0.0966\nTop-3 Accuracy: 0.1881\nTop-5 Accuracy: 0.2450\nTop-10 Accuracy: 0.3340\nEpoch 2:\nTraining Loss: 5.8575\nEpoch 2:\nValidation Loss: 6.0427\nValidation Accuracy: 0.1070\nTop-3 Accuracy: 0.2040\nTop-5 Accuracy: 0.2619\nTop-10 Accuracy: 0.3514\nEpoch 3:\nTraining Loss: 5.6673\nEpoch 3:\nValidation Loss: 6.0055\nValidation Accuracy: 0.1122\nTop-3 Accuracy: 0.2117\nTop-5 Accuracy: 0.2706\nTop-10 Accuracy: 0.3611\nEpoch 4:\nTraining Loss: 5.5340\nEpoch 4:\nValidation Loss: 5.9862\nValidation Accuracy: 0.1153\nTop-3 Accuracy: 0.2163\nTop-5 Accuracy: 0.2757\nTop-10 Accuracy: 0.3665\nEpoch 5:\nTraining Loss: 5.4283\nEpoch 5:\nValidation Loss: 5.9776\nValidation Accuracy: 0.1171\nTop-3 Accuracy: 0.2190\nTop-5 Accuracy: 0.2793\nTop-10 Accuracy: 0.3705\nEpoch 6:\nTraining Loss: 5.3406\nEpoch 6:\nValidation Loss: 5.9773\nValidation Accuracy: 0.1184\nTop-3 Accuracy: 0.2211\nTop-5 Accuracy: 0.2816\nTop-10 Accuracy: 0.3726\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2506\nEpoch 1:\nValidation Loss: 6.0643\nValidation Accuracy: 0.1113\nTop-3 Accuracy: 0.2099\nTop-5 Accuracy: 0.2698\nTop-10 Accuracy: 0.3609\nEpoch 2:\nTraining Loss: 5.4939\nEpoch 2:\nValidation Loss: 6.1867\nValidation Accuracy: 0.1098\nTop-3 Accuracy: 0.2091\nTop-5 Accuracy: 0.2690\nTop-10 Accuracy: 0.3596\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2542\nEpoch 1:\nValidation Loss: 6.0686\nValidation Accuracy: 0.1104\nTop-3 Accuracy: 0.2092\nTop-5 Accuracy: 0.2687\nTop-10 Accuracy: 0.3597\nEpoch 2:\nTraining Loss: 5.4936\nEpoch 2:\nValidation Loss: 6.1820\nValidation Accuracy: 0.1103\nTop-3 Accuracy: 0.2094\nTop-5 Accuracy: 0.2688\nTop-10 Accuracy: 0.3600\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.5552\nEpoch 1:\nValidation Loss: 6.2382\nValidation Accuracy: 0.0821\nTop-3 Accuracy: 0.1659\nTop-5 Accuracy: 0.2204\nTop-10 Accuracy: 0.3106\nEpoch 2:\nTraining Loss: 6.0422\nEpoch 2:\nValidation Loss: 6.1301\nValidation Accuracy: 0.0946\nTop-3 Accuracy: 0.1842\nTop-5 Accuracy: 0.2404\nTop-10 Accuracy: 0.3290\nEpoch 3:\nTraining Loss: 5.8778\nEpoch 3:\nValidation Loss: 6.0852\nValidation Accuracy: 0.1010\nTop-3 Accuracy: 0.1942\nTop-5 Accuracy: 0.2517\nTop-10 Accuracy: 0.3401\nEpoch 4:\nTraining Loss: 5.7626\nEpoch 4:\nValidation Loss: 6.0549\nValidation Accuracy: 0.1054\nTop-3 Accuracy: 0.2011\nTop-5 Accuracy: 0.2590\nTop-10 Accuracy: 0.3472\nEpoch 5:\nTraining Loss: 5.6709\nEpoch 5:\nValidation Loss: 6.0331\nValidation Accuracy: 0.1083\nTop-3 Accuracy: 0.2054\nTop-5 Accuracy: 0.2639\nTop-10 Accuracy: 0.3526\nEpoch 6:\nTraining Loss: 5.5950\nEpoch 6:\nValidation Loss: 6.0167\nValidation Accuracy: 0.1107\nTop-3 Accuracy: 0.2090\nTop-5 Accuracy: 0.2678\nTop-10 Accuracy: 0.3570\nEpoch 7:\nTraining Loss: 5.5302\nEpoch 7:\nValidation Loss: 6.0070\nValidation Accuracy: 0.1122\nTop-3 Accuracy: 0.2115\nTop-5 Accuracy: 0.2704\nTop-10 Accuracy: 0.3596\nEpoch 8:\nTraining Loss: 5.4737\nEpoch 8:\nValidation Loss: 6.0029\nValidation Accuracy: 0.1132\nTop-3 Accuracy: 0.2128\nTop-5 Accuracy: 0.2720\nTop-10 Accuracy: 0.3618\nEpoch 9:\nTraining Loss: 5.4237\nEpoch 9:\nValidation Loss: 5.9999\nValidation Accuracy: 0.1145\nTop-3 Accuracy: 0.2143\nTop-5 Accuracy: 0.2736\nTop-10 Accuracy: 0.3636\nEpoch 10:\nTraining Loss: 5.3788\nEpoch 10:\nValidation Loss: 5.9980\nValidation Accuracy: 0.1156\nTop-3 Accuracy: 0.2157\nTop-5 Accuracy: 0.2752\nTop-10 Accuracy: 0.3656\nEpoch 1:\nTraining Loss: 6.5593\nEpoch 1:\nValidation Loss: 6.2430\nValidation Accuracy: 0.0832\nTop-3 Accuracy: 0.1674\nTop-5 Accuracy: 0.2223\nTop-10 Accuracy: 0.3117\nEpoch 2:\nTraining Loss: 6.0439\nEpoch 2:\nValidation Loss: 6.1298\nValidation Accuracy: 0.0951\nTop-3 Accuracy: 0.1851\nTop-5 Accuracy: 0.2408\nTop-10 Accuracy: 0.3295\nEpoch 3:\nTraining Loss: 5.8756\nEpoch 3:\nValidation Loss: 6.0846\nValidation Accuracy: 0.1014\nTop-3 Accuracy: 0.1949\nTop-5 Accuracy: 0.2519\nTop-10 Accuracy: 0.3409\nEpoch 4:\nTraining Loss: 5.7589\nEpoch 4:\nValidation Loss: 6.0540\nValidation Accuracy: 0.1058\nTop-3 Accuracy: 0.2011\nTop-5 Accuracy: 0.2588\nTop-10 Accuracy: 0.3477\nEpoch 5:\nTraining Loss: 5.6670\nEpoch 5:\nValidation Loss: 6.0317\nValidation Accuracy: 0.1085\nTop-3 Accuracy: 0.2057\nTop-5 Accuracy: 0.2637\nTop-10 Accuracy: 0.3529\nEpoch 6:\nTraining Loss: 5.5909\nEpoch 6:\nValidation Loss: 6.0183\nValidation Accuracy: 0.1105\nTop-3 Accuracy: 0.2088\nTop-5 Accuracy: 0.2674\nTop-10 Accuracy: 0.3564\nEpoch 7:\nTraining Loss: 5.5257\nEpoch 7:\nValidation Loss: 6.0083\nValidation Accuracy: 0.1123\nTop-3 Accuracy: 0.2116\nTop-5 Accuracy: 0.2700\nTop-10 Accuracy: 0.3594\nEpoch 8:\nTraining Loss: 5.4693\nEpoch 8:\nValidation Loss: 6.0027\nValidation Accuracy: 0.1134\nTop-3 Accuracy: 0.2131\nTop-5 Accuracy: 0.2723\nTop-10 Accuracy: 0.3614\nEpoch 9:\nTraining Loss: 5.4196\nEpoch 9:\nValidation Loss: 5.9997\nValidation Accuracy: 0.1145\nTop-3 Accuracy: 0.2145\nTop-5 Accuracy: 0.2735\nTop-10 Accuracy: 0.3634\nEpoch 10:\nTraining Loss: 5.3752\nEpoch 10:\nValidation Loss: 6.0001\nValidation Accuracy: 0.1155\nTop-3 Accuracy: 0.2158\nTop-5 Accuracy: 0.2753\nTop-10 Accuracy: 0.3652\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2235\nEpoch 1:\nValidation Loss: 6.0076\nValidation Accuracy: 0.1113\nTop-3 Accuracy: 0.2095\nTop-5 Accuracy: 0.2683\nTop-10 Accuracy: 0.3585\nEpoch 2:\nTraining Loss: 5.6236\nEpoch 2:\nValidation Loss: 5.9949\nValidation Accuracy: 0.1157\nTop-3 Accuracy: 0.2161\nTop-5 Accuracy: 0.2757\nTop-10 Accuracy: 0.3657\nEpoch 3:\nTraining Loss: 5.3138\nEpoch 3:\nValidation Loss: 6.0663\nValidation Accuracy: 0.1146\nTop-3 Accuracy: 0.2145\nTop-5 Accuracy: 0.2733\nTop-10 Accuracy: 0.3629\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2277\nEpoch 1:\nValidation Loss: 6.0123\nValidation Accuracy: 0.1107\nTop-3 Accuracy: 0.2089\nTop-5 Accuracy: 0.2672\nTop-10 Accuracy: 0.3569\nEpoch 2:\nTraining Loss: 5.6299\nEpoch 2:\nValidation Loss: 5.9949\nValidation Accuracy: 0.1149\nTop-3 Accuracy: 0.2156\nTop-5 Accuracy: 0.2753\nTop-10 Accuracy: 0.3663\nEpoch 3:\nTraining Loss: 5.3207\nEpoch 3:\nValidation Loss: 6.0642\nValidation Accuracy: 0.1146\nTop-3 Accuracy: 0.2147\nTop-5 Accuracy: 0.2738\nTop-10 Accuracy: 0.3635\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.4441\nEpoch 1:\nValidation Loss: 6.1426\nValidation Accuracy: 0.0919\nTop-3 Accuracy: 0.1804\nTop-5 Accuracy: 0.2362\nTop-10 Accuracy: 0.3255\nEpoch 2:\nTraining Loss: 5.9007\nEpoch 2:\nValidation Loss: 6.0419\nValidation Accuracy: 0.1023\nTop-3 Accuracy: 0.1970\nTop-5 Accuracy: 0.2550\nTop-10 Accuracy: 0.3445\nEpoch 3:\nTraining Loss: 5.6961\nEpoch 3:\nValidation Loss: 6.0011\nValidation Accuracy: 0.1088\nTop-3 Accuracy: 0.2071\nTop-5 Accuracy: 0.2653\nTop-10 Accuracy: 0.3555\nEpoch 4:\nTraining Loss: 5.5427\nEpoch 4:\nValidation Loss: 5.9783\nValidation Accuracy: 0.1124\nTop-3 Accuracy: 0.2121\nTop-5 Accuracy: 0.2714\nTop-10 Accuracy: 0.3617\nEpoch 5:\nTraining Loss: 5.4170\nEpoch 5:\nValidation Loss: 5.9660\nValidation Accuracy: 0.1151\nTop-3 Accuracy: 0.2158\nTop-5 Accuracy: 0.2755\nTop-10 Accuracy: 0.3664\nEpoch 6:\nTraining Loss: 5.3093\nEpoch 6:\nValidation Loss: 5.9635\nValidation Accuracy: 0.1170\nTop-3 Accuracy: 0.2181\nTop-5 Accuracy: 0.2781\nTop-10 Accuracy: 0.3695\nEpoch 7:\nTraining Loss: 5.2143\nEpoch 7:\nValidation Loss: 5.9649\nValidation Accuracy: 0.1177\nTop-3 Accuracy: 0.2198\nTop-5 Accuracy: 0.2799\nTop-10 Accuracy: 0.3718\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.4436\nEpoch 1:\nValidation Loss: 6.1410\nValidation Accuracy: 0.0920\nTop-3 Accuracy: 0.1812\nTop-5 Accuracy: 0.2371\nTop-10 Accuracy: 0.3266\nEpoch 2:\nTraining Loss: 5.8987\nEpoch 2:\nValidation Loss: 6.0401\nValidation Accuracy: 0.1034\nTop-3 Accuracy: 0.1989\nTop-5 Accuracy: 0.2559\nTop-10 Accuracy: 0.3444\nEpoch 3:\nTraining Loss: 5.6936\nEpoch 3:\nValidation Loss: 6.0015\nValidation Accuracy: 0.1085\nTop-3 Accuracy: 0.2065\nTop-5 Accuracy: 0.2650\nTop-10 Accuracy: 0.3553\nEpoch 4:\nTraining Loss: 5.5402\nEpoch 4:\nValidation Loss: 5.9789\nValidation Accuracy: 0.1127\nTop-3 Accuracy: 0.2123\nTop-5 Accuracy: 0.2713\nTop-10 Accuracy: 0.3621\nEpoch 5:\nTraining Loss: 5.4143\nEpoch 5:\nValidation Loss: 5.9675\nValidation Accuracy: 0.1152\nTop-3 Accuracy: 0.2159\nTop-5 Accuracy: 0.2753\nTop-10 Accuracy: 0.3658\nEpoch 6:\nTraining Loss: 5.3065\nEpoch 6:\nValidation Loss: 5.9636\nValidation Accuracy: 0.1175\nTop-3 Accuracy: 0.2188\nTop-5 Accuracy: 0.2783\nTop-10 Accuracy: 0.3695\nEpoch 7:\nTraining Loss: 5.2115\nEpoch 7:\nValidation Loss: 5.9674\nValidation Accuracy: 0.1179\nTop-3 Accuracy: 0.2197\nTop-5 Accuracy: 0.2796\nTop-10 Accuracy: 0.3714\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2099\nEpoch 1:\nValidation Loss: 5.9997\nValidation Accuracy: 0.1125\nTop-3 Accuracy: 0.2126\nTop-5 Accuracy: 0.2729\nTop-10 Accuracy: 0.3643\nEpoch 2:\nTraining Loss: 5.4060\nEpoch 2:\nValidation Loss: 6.0665\nValidation Accuracy: 0.1141\nTop-3 Accuracy: 0.2146\nTop-5 Accuracy: 0.2748\nTop-10 Accuracy: 0.3657\nEarly Stopping Triggered\nEpoch 1:\nTraining Loss: 6.2136\nEpoch 1:\nValidation Loss: 5.9971\nValidation Accuracy: 0.1130\nTop-3 Accuracy: 0.2136\nTop-5 Accuracy: 0.2734\nTop-10 Accuracy: 0.3640\nEpoch 2:\nTraining Loss: 5.4088\nEpoch 2:\nValidation Loss: 6.0643\nValidation Accuracy: 0.1142\nTop-3 Accuracy: 0.2159\nTop-5 Accuracy: 0.2757\nTop-10 Accuracy: 0.3659\nEarly Stopping Triggered\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"results_quarter_weighted_df = pd.DataFrame(results_quarter_weighted)\nresults_quarter_weighted_df.to_csv('weighted_model_quarter_hyperparam_results.csv', index = False)\nprint(results_quarter_weighted_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:12:36.723041Z","iopub.execute_input":"2024-12-21T11:12:36.723382Z","iopub.status.idle":"2024-12-21T11:12:36.734137Z","shell.execute_reply.started":"2024-12-21T11:12:36.723358Z","shell.execute_reply":"2024-12-21T11:12:36.733306Z"}},"outputs":[{"name":"stdout","text":"                                              params  val_accuracy  \\\n0  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.116672   \n1  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.117533   \n2  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.112865   \n3  {'batch_size': 64, 'embedding_dim': 50, 'learn...      0.112762   \n4  {'batch_size': 64, 'embedding_dim': 100, 'lear...      0.118743   \n\n   val_top_3_accuracy  val_top_5_accuracy  val_top_10_accuracy  total time  \n0            0.217880            0.277880             0.368394  638.042042  \n1            0.219373            0.279266             0.369360  764.510987  \n2            0.213017            0.272950             0.363215  125.983661  \n3            0.212697            0.272405             0.362845  125.847549  \n4            0.220188            0.280671             0.372426  401.873167  \n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Now that we have completed the hyperparameter tuning across all 6 variations\nof the model, we can compare the results of each parameter combination for each model, and pick the best combination for each model, taking into account all accuracy metrics as well as the total time taken.","metadata":{}},{"cell_type":"code","source":"batch_size_full = 128\nembedding_dim_full = 100\nlearning_rate_full = 0.01\nnum_epochs_full = 20\n\nbatch_size_half = 128\nembedding_dim_half = 100\nlearning_rate_half = 0.01\nnum_epochs_half = 20\n\nbatch_size_quarter = 128\nembedding_dim_quarter = 100\nlearning_rate_quarter = 0.01\nnum_epochs_quarter = 10\n\nbatch_size_weighted_full = 128\nembedding_dim_weighted_full = 100\nlearning_rate_weighted_full = 0.001\nnum_epochs_weighted_full = 10\n\nbatch_size_weighted_half = 64\nembedding_dim_weighted_half = 100\nlearning_rate_weighted_half = 0.001\nnum_epochs_weighted_half = 10\n\nbatch_size_weighted_quarter = 64\nembedding_dim_weighted_quarter = 100\nlearning_rate_weighted_quarter = 0.001\nnum_epochs_weighted_quarter = 20\n\nvocab_size_full = len(vocab_full)\nvocab_size_half = len(vocab_half)\nvocab_size_quarter = len(vocab_quarter)\n\ntrain_pairs_full, val_test_pairs_full = train_test_split(encoded_pairs_full, test_size = 0.7, random_state = 42)\nval_pairs_full, test_pairs_full = train_test_split(val_test_pairs_full, test_size = 0.5, random_state = 42)\n\ntrain_pairs_half, val_test_pairs_half = train_test_split(encoded_pairs_half, test_size = 0.7, random_state = 42)\nval_pairs_half, test_pairs_half = train_test_split(val_test_pairs_half, test_size = 0.5, random_state = 42)\n\ntrain_pairs_quarter, val_test_pairs_quarter = train_test_split(encoded_pairs_quarter, test_size = 0.7, random_state = 42)\nval_pairs_quarter, test_pairs_quarter = train_test_split(val_test_pairs_quarter, test_size = 0.5, random_state = 42)\n\ntrain_dataset_full = CBOWDataset(train_pairs_full, vocab_size_full, 5)\nval_dataset_full = CBOWDataset(val_pairs_full, vocab_size_full, 5)\ntest_dataset_full = CBOWDataset(test_pairs_full, vocab_size_full, 5)\n\ntrain_dataset_half = CBOWDataset(train_pairs_half, vocab_size_half, 5)\nval_dataset_half = CBOWDataset(val_pairs_half, vocab_size_half, 5)\ntest_dataset_half = CBOWDataset(test_pairs_half, vocab_size_half, 5)\n\ntrain_dataset_quarter = CBOWDataset(train_pairs_quarter, vocab_size_quarter, 5)\nval_dataset_quarter = CBOWDataset(val_pairs_quarter, vocab_size_quarter, 5)\ntest_dataset_quarter = CBOWDataset(test_pairs_quarter, vocab_size_quarter, 5)\n\ntrain_loader_full = DataLoader(train_dataset_full, batch_size = batch_size_full, shuffle = True)\nval_loader_full = DataLoader(val_dataset_full, batch_size = batch_size_full, shuffle = False)\ntest_loader_full = DataLoader(test_dataset_full, batch_size = batch_size_full, shuffle = False)\n\ntrain_loader_half = DataLoader(train_dataset_half, batch_size = batch_size_half, shuffle = True)\nval_loader_half = DataLoader(val_dataset_half, batch_size = batch_size_half, shuffle = False)\ntest_loader_half = DataLoader(test_dataset_half, batch_size = batch_size_half, shuffle = False)\n\ntrain_loader_quarter = DataLoader(train_dataset_quarter, batch_size = batch_size_quarter, shuffle = True)\nval_loader_quarter = DataLoader(val_dataset_quarter, batch_size = batch_size_quarter, shuffle = False)\ntest_loader_quarter = DataLoader(test_dataset_quarter, batch_size = batch_size_quarter, shuffle = False)\n\ntrain_loader_weighted_full = DataLoader(train_dataset_full, batch_size = batch_size_weighted_full, shuffle = True)\nval_loader_weighted_full = DataLoader(val_dataset_full, batch_size = batch_size_weighted_full, shuffle = False)\ntest_loader_weighted_full = DataLoader(test_dataset_full, batch_size = batch_size_weighted_full, shuffle = False)\n\ntrain_loader_weighted_half = DataLoader(train_dataset_half, batch_size = batch_size_weighted_half, shuffle = True)\nval_loader_weighted_half = DataLoader(val_dataset_half, batch_size = batch_size_weighted_half, shuffle = False)\ntest_loader_weighted_half = DataLoader(test_dataset_half, batch_size = batch_size_weighted_half, shuffle = False)\n\ntrain_loader_weighted_quarter = DataLoader(train_dataset_quarter, batch_size = batch_size_weighted_quarter, shuffle = True)\nval_loader_weighted_quarter = DataLoader(val_dataset_quarter, batch_size = batch_size_weighted_quarter, shuffle = False)\ntest_loader_weighted_quarter = DataLoader(test_dataset_quarter, batch_size = batch_size_weighted_quarter, shuffle = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:16:01.556227Z","iopub.execute_input":"2025-01-21T11:16:01.556521Z","iopub.status.idle":"2025-01-21T11:16:12.649359Z","shell.execute_reply.started":"2025-01-21T11:16:01.556499Z","shell.execute_reply":"2025-01-21T11:16:12.648705Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Next, we evaluate all 6 of our models on the test data that was not involved in the training and validation loops. This can be achieved by using the same random_state in our definition of train_test_split.","metadata":{}},{"cell_type":"code","source":"model_full = CBOWModel(vocab_size_full, embedding_dim_full).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_full.parameters(), lr = learning_rate_full)\n\nbest_val_loss = float('inf')\ntrain_losses_full, val_losses_full = [], []\n\nstart_time = time.time()\nfor epoch in range(num_epochs_full):\n  model_full.train()\n  total_loss = 0.0\n  for context, target in train_loader_full:\n    context, target = context.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model_full(context)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n  avg_train_loss = total_loss / len(train_loader_full)\n  train_losses_full.append(avg_train_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_full}: Training Loss =  {avg_train_loss:.4f}\")\n\n  model_full.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for context, target in val_loader_full:\n      context, target = context.to(device), target.to(device)\n      output = model_full(context)\n      loss = criterion(output, target)\n      val_loss += loss.item()\n\n  avg_val_loss = val_loss / len(val_loader_full)\n  val_losses_full.append(avg_val_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_full}: Validation Loss = {avg_val_loss:.4f}\")\n\n  if abs(best_val_loss - avg_val_loss) < 1e-3:\n    print(\"Early stopping triggered\")\n    break\n  elif avg_val_loss > best_val_loss:\n    print(\"Early stopping triggered\")\n    break\n  else:\n    best_val_loss = avg_val_loss\n    torch.save(model_full.state_dict(), \"best_cbow_model_full.pth\") #important to save this model path so we can reuse it for future evaluations\n\nmodel_full.load_state_dict(torch.load('best_cbow_model_full.pth'))\nmodel_full.eval()\n\ntest_loss_full = 0.0\ntest_accuracy_full = 0.0\ntop_3_accuracy_full, top_5_accuracy_full, top_10_accuracy_full = 0.0, 0.0, 0.0\n\nwith torch.no_grad():\n  for context, target in test_loader_full:\n    context, target = context.to(device), target.to(device)\n    output = model_full(context)\n    loss = criterion(output, target)\n    test_loss_full += loss.item()\n\n    predictions = torch.argmax(output, dim = 1)\n    test_accuracy_full += accuracy_score(target.cpu(), predictions.cpu())\n\n    top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n    top_3_accuracy_full += top_k_accuracies['top_3_accuracy']\n    top_5_accuracy_full += top_k_accuracies['top_5_accuracy']\n    top_10_accuracy_full += top_k_accuracies['top_10_accuracy']\n\navg_test_loss_full = test_loss_full / len(test_loader_full)\navg_test_accuracy_full = test_accuracy_full / len(test_loader_full)\navg_top_3_accuracy_full = top_3_accuracy_full / len(test_loader_full)\navg_top_5_accuracy_full = top_5_accuracy_full / len(test_loader_full)\navg_top_10_accuracy_full = top_10_accuracy_full / len(test_loader_full)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(f\"Test Loss: {avg_test_loss_full:.4f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy_full:.4f}\")\nprint(f\"Top 3 Test Accuracy: {avg_top_3_accuracy_full:.4f}\")\nprint(f\"Top 5 Test Accuracy: {avg_top_5_accuracy_full:.4f}\")\nprint(f\"Top 10 Test Accuracy: {avg_top_10_accuracy_full:.4f}\")\nprint(f\"Total time taken to train and test: {total_time:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:30:06.387152Z","iopub.execute_input":"2024-12-27T06:30:06.387473Z","iopub.status.idle":"2024-12-27T06:43:04.138502Z","shell.execute_reply.started":"2024-12-27T06:30:06.387447Z","shell.execute_reply":"2024-12-27T06:43:04.137723Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 / 20: Training Loss =  10.6476\nEpoch 1 / 20: Validation Loss = 10.6352\nEpoch 2 / 20: Training Loss =  10.6305\nEpoch 2 / 20: Validation Loss = 10.6312\nEpoch 3 / 20: Training Loss =  10.6259\nEpoch 3 / 20: Validation Loss = 10.6277\nEpoch 4 / 20: Training Loss =  10.6230\nEpoch 4 / 20: Validation Loss = 10.6272\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-23-7f65b8f7be61>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_full.load_state_dict(torch.load('best_cbow_model_full.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 10.6274\nTest Accuracy: 0.0990\nTop 3 Test Accuracy: 0.1574\nTop 5 Test Accuracy: 0.1834\nTop 10 Test Accuracy: 0.2398\nTotal time taken to train and test: 776.29\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"model_half = CBOWModel(vocab_size_half, embedding_dim_half).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_half.parameters(), lr = learning_rate_half)\n\nbest_val_loss = float('inf')\ntrain_losses_half, val_losses_half = [], []\n\nstart_time = time.time()\nfor epoch in range(num_epochs_half):\n  model_half.train()\n  total_loss = 0.0\n  for context, target in train_loader_half:\n    context, target = context.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model_half(context)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n  avg_train_loss = total_loss / len(train_loader_half)\n  train_losses_half.append(avg_train_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_half}: Training Loss =  {avg_train_loss:.4f}\")\n\n  model_half.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for context, target in val_loader_half:\n      context, target = context.to(device), target.to(device)\n      output = model_half(context)\n      loss = criterion(output, target)\n      val_loss += loss.item()\n\n  avg_val_loss = val_loss / len(val_loader_half)\n  val_losses_half.append(avg_val_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_half}: Validation Loss = {avg_val_loss:.4f}\")\n\n  if abs(best_val_loss - avg_val_loss) < 1e-3:\n    print(\"Early stopping triggered\")\n    break\n  elif avg_val_loss > best_val_loss:\n    print(\"Early stopping triggered\")\n    break\n  else:\n    best_val_loss = avg_val_loss\n    torch.save(model_half.state_dict(), \"best_cbow_model_half.pth\")\n\nmodel_half.load_state_dict(torch.load('best_cbow_model_half.pth'))\nmodel_half.eval()\n\ntest_loss_half = 0.0\ntest_accuracy_half = 0.0\ntop_3_accuracy_half, top_5_accuracy_half, top_10_accuracy_half = 0.0, 0.0, 0.0\n\nwith torch.no_grad():\n  for context, target in test_loader_half:\n    context, target = context.to(device), target.to(device)\n    output = model_half(context)\n    loss = criterion(output, target)\n    test_loss_half += loss.item()\n\n    predictions = torch.argmax(output, dim = 1)\n    test_accuracy_half += accuracy_score(target.cpu(), predictions.cpu())\n\n    top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n    top_3_accuracy_half += top_k_accuracies['top_3_accuracy']\n    top_5_accuracy_half += top_k_accuracies['top_5_accuracy']\n    top_10_accuracy_half += top_k_accuracies['top_10_accuracy']\n\navg_test_loss_half = test_loss_half / len(test_loader_half)\navg_test_accuracy_half = test_accuracy_half / len(test_loader_half)\navg_top_3_accuracy_half = top_3_accuracy_half / len(test_loader_half)\navg_top_5_accuracy_half = top_5_accuracy_half / len(test_loader_half)\navg_top_10_accuracy_half = top_10_accuracy_half / len(test_loader_half)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(f\"Test Loss: {avg_test_loss_half:.4f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy_half:.4f}\")\nprint(f\"Top 3 Test Accuracy: {avg_top_3_accuracy_half:.4f}\")\nprint(f\"Top 5 Test Accuracy: {avg_top_5_accuracy_half:.4f}\")\nprint(f\"Top 10 Test Accuracy: {avg_top_10_accuracy_half:.4f}\")\nprint(f\"Total time taken to train and test: {total_time:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:46:08.263825Z","iopub.execute_input":"2024-12-27T06:46:08.264259Z","iopub.status.idle":"2024-12-27T06:53:24.704562Z","shell.execute_reply.started":"2024-12-27T06:46:08.264227Z","shell.execute_reply":"2024-12-27T06:53:24.703567Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 / 20: Training Loss =  10.4651\nEpoch 1 / 20: Validation Loss = 10.4600\nEpoch 2 / 20: Training Loss =  10.4526\nEpoch 2 / 20: Validation Loss = 10.4579\nEpoch 3 / 20: Training Loss =  10.4472\nEpoch 3 / 20: Validation Loss = 10.4545\nEpoch 4 / 20: Training Loss =  10.4437\nEpoch 4 / 20: Validation Loss = 10.4533\nEpoch 5 / 20: Training Loss =  10.4411\nEpoch 5 / 20: Validation Loss = 10.4532\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-25-47429aa15866>:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_half.load_state_dict(torch.load('best_cbow_model_half.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 10.4528\nTest Accuracy: 0.1030\nTop 3 Test Accuracy: 0.1720\nTop 5 Test Accuracy: 0.2034\nTop 10 Test Accuracy: 0.2502\nTotal time taken to train and test: 436.35\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model_quarter = CBOWModel(vocab_size_quarter, embedding_dim_quarter).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_quarter.parameters(), lr = learning_rate_quarter)\n\nbest_val_loss = float('inf')\ntrain_losses_quarter, val_losses_quarter = [], []\n\nstart_time = time.time()\nfor epoch in range(num_epochs_quarter):\n  model_quarter.train()\n  total_loss = 0.0\n  for context, target in train_loader_quarter:\n    context, target = context.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model_quarter(context)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n  avg_train_loss = total_loss / len(train_loader_quarter)\n  train_losses_quarter.append(avg_train_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_quarter}: Training Loss =  {avg_train_loss:.4f}\")\n\n  model_quarter.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for context, target in val_loader_quarter:\n      context, target = context.to(device), target.to(device)\n      output = model_quarter(context)\n      loss = criterion(output, target)\n      val_loss += loss.item()\n\n  avg_val_loss = val_loss / len(val_loader_quarter)\n  val_losses_quarter.append(avg_val_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_quarter}: Validation Loss = {avg_val_loss:.4f}\")\n\n  if abs(best_val_loss - avg_val_loss) < 1e-3:\n    print(\"Early stopping triggered\")\n    break\n  elif avg_val_loss > best_val_loss:\n    print(\"Early stopping triggered\")\n    break\n  else:\n    best_val_loss = avg_val_loss\n    torch.save(model_quarter.state_dict(), \"best_cbow_model_quarter.pth\")\n\nmodel_quarter.load_state_dict(torch.load('best_cbow_model_quarter.pth'))\nmodel_quarter.eval()\n\ntest_loss_quarter = 0.0\ntest_accuracy_quarter = 0.0\ntop_3_accuracy_quarter, top_5_accuracy_quarter, top_10_accuracy_quarter = 0.0, 0.0, 0.0\n\nwith torch.no_grad():\n  for context, target in test_loader_quarter:\n    context, target = context.to(device), target.to(device)\n    output = model_quarter(context)\n    loss = criterion(output, target)\n    test_loss_quarter += loss.item()\n\n    predictions = torch.argmax(output, dim = 1)\n    test_accuracy_quarter += accuracy_score(target.cpu(), predictions.cpu())\n\n    top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n    top_3_accuracy_quarter += top_k_accuracies['top_3_accuracy']\n    top_5_accuracy_quarter += top_k_accuracies['top_5_accuracy']\n    top_10_accuracy_quarter += top_k_accuracies['top_10_accuracy']\n\navg_test_loss_quarter = test_loss_quarter / len(test_loader_quarter)\navg_test_accuracy_quarter = test_accuracy_quarter / len(test_loader_quarter)\navg_top_3_accuracy_quarter = top_3_accuracy_quarter / len(test_loader_quarter)\navg_top_5_accuracy_quarter = top_5_accuracy_quarter / len(test_loader_quarter)\navg_top_10_accuracy_quarter = top_10_accuracy_quarter / len(test_loader_quarter)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(f\"Test Loss: {avg_test_loss_quarter:.4f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy_quarter:.4f}\")\nprint(f\"Top 3 Test Accuracy: {avg_top_3_accuracy_quarter:.4f}\")\nprint(f\"Top 5 Test Accuracy: {avg_top_5_accuracy_quarter:.4f}\")\nprint(f\"Top 10 Test Accuracy: {avg_top_10_accuracy_quarter:.4f}\")\nprint(f\"Total time taken to train and test: {total_time:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:53:24.705970Z","iopub.execute_input":"2024-12-27T06:53:24.706306Z","iopub.status.idle":"2024-12-27T06:57:56.218515Z","shell.execute_reply.started":"2024-12-27T06:53:24.706273Z","shell.execute_reply":"2024-12-27T06:57:56.217629Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 / 10: Training Loss =  10.2820\nEpoch 1 / 10: Validation Loss = 10.2710\nEpoch 2 / 10: Training Loss =  10.2622\nEpoch 2 / 10: Validation Loss = 10.2663\nEpoch 3 / 10: Training Loss =  10.2528\nEpoch 3 / 10: Validation Loss = 10.2620\nEpoch 4 / 10: Training Loss =  10.2468\nEpoch 4 / 10: Validation Loss = 10.2606\nEpoch 5 / 10: Training Loss =  10.2420\nEpoch 5 / 10: Validation Loss = 10.2577\nEpoch 6 / 10: Training Loss =  10.2362\nEpoch 6 / 10: Validation Loss = 10.2557\nEpoch 7 / 10: Training Loss =  10.2323\nEpoch 7 / 10: Validation Loss = 10.2551\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-26-b591f767b255>:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_quarter.load_state_dict(torch.load('best_cbow_model_quarter.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 10.2560\nTest Accuracy: 0.1050\nTop 3 Test Accuracy: 0.1780\nTop 5 Test Accuracy: 0.2123\nTop 10 Test Accuracy: 0.2514\nTotal time taken to train and test: 271.45\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"weighted_model_full = CBOWModelWeighted(vocab_size_full, embedding_dim_weighted_full, 5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(weighted_model_full.parameters(), lr = learning_rate_weighted_full)\n\nbest_val_loss = float('inf')\ntrain_losses_full, val_losses_full = [], []\n\nstart_time = time.time()\nfor epoch in range(num_epochs_weighted_full):\n  weighted_model_full.train()\n  total_loss = 0.0\n  for context, target in train_loader_weighted_full:\n    context, target = context.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = weighted_model_full(context)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n  avg_train_loss = total_loss / len(train_loader_weighted_full)\n  train_losses_full.append(avg_train_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_weighted_full}: Training Loss =  {avg_train_loss:.4f}\")\n\n  weighted_model_full.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for context, target in val_loader_weighted_full:\n      context, target = context.to(device), target.to(device)\n      output = weighted_model_full(context)\n      loss = criterion(output, target)\n      val_loss += loss.item()\n\n  avg_val_loss = val_loss / len(val_loader_weighted_full)\n  val_losses_full.append(avg_val_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_weighted_full}: Validation Loss = {avg_val_loss:.4f}\")\n\n  if abs(best_val_loss - avg_val_loss) < 1e-3:\n    print(\"Early stopping triggered\")\n    break\n  elif avg_val_loss > best_val_loss:\n    print(\"Early stopping triggered\")\n    break\n  else:\n    best_val_loss = avg_val_loss\n    torch.save(weighted_model_full.state_dict(), \"best_cbow_model_weighted_full.pth\")\n\nweighted_model_full.load_state_dict(torch.load('best_cbow_model_weighted_full.pth'))\nweighted_model_full.eval()\n\ntest_loss_full = 0.0\ntest_accuracy_full = 0.0\ntop_3_accuracy_full, top_5_accuracy_full, top_10_accuracy_full = 0.0, 0.0, 0.0\n\nwith torch.no_grad():\n  for context, target in test_loader_weighted_full:\n    context, target = context.to(device), target.to(device)\n    output = weighted_model_full(context)\n    loss = criterion(output, target)\n    test_loss_full += loss.item()\n\n    predictions = torch.argmax(output, dim = 1)\n    test_accuracy_full += accuracy_score(target.cpu(), predictions.cpu())\n\n    top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n    top_3_accuracy_full += top_k_accuracies['top_3_accuracy']\n    top_5_accuracy_full += top_k_accuracies['top_5_accuracy']\n    top_10_accuracy_full += top_k_accuracies['top_10_accuracy']\n\navg_test_loss_full = test_loss_full / len(test_loader_weighted_full)\navg_test_accuracy_full = test_accuracy_full / len(test_loader_weighted_full)\navg_top_3_accuracy_full = top_3_accuracy_full / len(test_loader_weighted_full)\navg_top_5_accuracy_full = top_5_accuracy_full / len(test_loader_weighted_full)\navg_top_10_accuracy_full = top_10_accuracy_full / len(test_loader_weighted_full)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(f\"Test Loss: {avg_test_loss_full:.4f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy_full:.4f}\")\nprint(f\"Top 3 Test Accuracy: {avg_top_3_accuracy_full:.4f}\")\nprint(f\"Top 5 Test Accuracy: {avg_top_5_accuracy_full:.4f}\")\nprint(f\"Top 10 Test Accuracy: {avg_top_10_accuracy_full:.4f}\")\nprint(f\"Total time taken to train and test: {total_time:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T07:03:54.651731Z","iopub.execute_input":"2024-12-27T07:03:54.652201Z","iopub.status.idle":"2024-12-27T07:28:12.984253Z","shell.execute_reply.started":"2024-12-27T07:03:54.652167Z","shell.execute_reply":"2024-12-27T07:28:12.983336Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 / 10: Training Loss =  6.1107\nEpoch 1 / 10: Validation Loss = 5.8991\nEpoch 2 / 10: Training Loss =  5.7425\nEpoch 2 / 10: Validation Loss = 5.8051\nEpoch 3 / 10: Training Loss =  5.6193\nEpoch 3 / 10: Validation Loss = 5.7630\nEpoch 4 / 10: Training Loss =  5.5403\nEpoch 4 / 10: Validation Loss = 5.7396\nEpoch 5 / 10: Training Loss =  5.4800\nEpoch 5 / 10: Validation Loss = 5.7236\nEpoch 6 / 10: Training Loss =  5.4302\nEpoch 6 / 10: Validation Loss = 5.7140\nEpoch 7 / 10: Training Loss =  5.3862\nEpoch 7 / 10: Validation Loss = 5.7079\nEpoch 8 / 10: Training Loss =  5.3461\nEpoch 8 / 10: Validation Loss = 5.7040\nEpoch 9 / 10: Training Loss =  5.3095\nEpoch 9 / 10: Validation Loss = 5.7036\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-28-864e245a1081>:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weighted_model_full.load_state_dict(torch.load('best_cbow_model_weighted_full.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 5.6974\nTest Accuracy: 0.1297\nTop 3 Test Accuracy: 0.2403\nTop 5 Test Accuracy: 0.3041\nTop 10 Test Accuracy: 0.3989\nTotal time taken to train and test: 1458.17\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"weighted_model_half = CBOWModelWeighted(vocab_size_half, embedding_dim_weighted_half, 5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(weighted_model_half.parameters(), lr = learning_rate_weighted_half)\n\nbest_val_loss = float('inf')\ntrain_losses_half, val_losses_half = [], []\n\nstart_time = time.time()\nfor epoch in range(num_epochs_weighted_half):\n  weighted_model_half.train()\n  total_loss = 0.0\n  for context, target in train_loader_weighted_half:\n    context, target = context.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = weighted_model_half(context)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n  avg_train_loss = total_loss / len(train_loader_weighted_half)\n  train_losses_half.append(avg_train_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_weighted_half}: Training Loss =  {avg_train_loss:.4f}\")\n\n  weighted_model_half.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for context, target in val_loader_weighted_half:\n      context, target = context.to(device), target.to(device)\n      output = weighted_model_half(context)\n      loss = criterion(output, target)\n      val_loss += loss.item()\n\n  avg_val_loss = val_loss / len(val_loader_weighted_half)\n  val_losses_half.append(avg_val_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_weighted_half}: Validation Loss = {avg_val_loss:.4f}\")\n\n  if abs(best_val_loss - avg_val_loss) < 1e-3:\n    print(\"Early stopping triggered\")\n    break\n  elif avg_val_loss > best_val_loss:\n    print(\"Early stopping triggered\")\n    break\n  else:\n    best_val_loss = avg_val_loss\n    torch.save(weighted_model_half.state_dict(), \"best_cbow_model_weighted_half.pth\")\n\nweighted_model_half.load_state_dict(torch.load('best_cbow_model_weighted_half.pth'))\nweighted_model_half.eval()\n\ntest_loss_half = 0.0\ntest_accuracy_half = 0.0\ntop_3_accuracy_half, top_5_accuracy_half, top_10_accuracy_half = 0.0, 0.0, 0.0\n\nwith torch.no_grad():\n  for context, target in test_loader_weighted_half:\n    context, target = context.to(device), target.to(device)\n    output = weighted_model_half(context)\n    loss = criterion(output, target)\n    test_loss_half += loss.item()\n\n    predictions = torch.argmax(output, dim = 1)\n    test_accuracy_half += accuracy_score(target.cpu(), predictions.cpu())\n\n    top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n    top_3_accuracy_half += top_k_accuracies['top_3_accuracy']\n    top_5_accuracy_half += top_k_accuracies['top_5_accuracy']\n    top_10_accuracy_half += top_k_accuracies['top_10_accuracy']\n\navg_test_loss_half = test_loss_half / len(test_loader_weighted_half)\navg_test_accuracy_half = test_accuracy_half / len(test_loader_weighted_half)\navg_top_3_accuracy_half = top_3_accuracy_half / len(test_loader_weighted_half)\navg_top_5_accuracy_half = top_5_accuracy_half / len(test_loader_weighted_half)\navg_top_10_accuracy_half = top_10_accuracy_half / len(test_loader_weighted_half)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(f\"Test Loss: {avg_test_loss_half:.4f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy_half:.4f}\")\nprint(f\"Top 3 Test Accuracy: {avg_top_3_accuracy_half:.4f}\")\nprint(f\"Top 5 Test Accuracy: {avg_top_5_accuracy_half:.4f}\")\nprint(f\"Top 10 Test Accuracy: {avg_top_10_accuracy_half:.4f}\")\nprint(f\"Total time taken to train and test: {total_time:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T07:44:31.139607Z","iopub.execute_input":"2024-12-27T07:44:31.139942Z","iopub.status.idle":"2024-12-27T07:58:24.274554Z","shell.execute_reply.started":"2024-12-27T07:44:31.139916Z","shell.execute_reply":"2024-12-27T07:58:24.273665Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 / 10: Training Loss =  6.2224\nEpoch 1 / 10: Validation Loss = 6.0271\nEpoch 2 / 10: Training Loss =  5.8303\nEpoch 2 / 10: Validation Loss = 5.9431\nEpoch 3 / 10: Training Loss =  5.6908\nEpoch 3 / 10: Validation Loss = 5.9052\nEpoch 4 / 10: Training Loss =  5.5937\nEpoch 4 / 10: Validation Loss = 5.8815\nEpoch 5 / 10: Training Loss =  5.5166\nEpoch 5 / 10: Validation Loss = 5.8685\nEpoch 6 / 10: Training Loss =  5.4488\nEpoch 6 / 10: Validation Loss = 5.8639\nEpoch 7 / 10: Training Loss =  5.3877\nEpoch 7 / 10: Validation Loss = 5.8586\nEpoch 8 / 10: Training Loss =  5.3299\nEpoch 8 / 10: Validation Loss = 5.8633\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-31-b99d6751068a>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weighted_model_half.load_state_dict(torch.load('best_cbow_model_weighted_half.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 5.8531\nTest Accuracy: 0.1253\nTop 3 Test Accuracy: 0.2326\nTop 5 Test Accuracy: 0.2953\nTop 10 Test Accuracy: 0.3882\nTotal time taken to train and test: 833.07\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"weighted_model_quarter = CBOWModelWeighted(vocab_size_quarter, embedding_dim_weighted_quarter, 5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(weighted_model_quarter.parameters(), lr = learning_rate_weighted_quarter)\n\nbest_val_loss = float('inf')\ntrain_losses_quarter, val_losses_quarter = [], []\n\nstart_time = time.time()\nfor epoch in range(num_epochs_weighted_quarter):\n  weighted_model_quarter.train()\n  total_loss = 0.0\n  for context, target in train_loader_weighted_quarter:\n    context, target = context.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = weighted_model_quarter(context)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n  avg_train_loss = total_loss / len(train_loader_weighted_quarter)\n  train_losses_quarter.append(avg_train_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_weighted_quarter}: Training Loss =  {avg_train_loss:.4f}\")\n\n  weighted_model_quarter.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for context, target in val_loader_weighted_quarter:\n      context, target = context.to(device), target.to(device)\n      output = weighted_model_quarter(context)\n      loss = criterion(output, target)\n      val_loss += loss.item()\n\n  avg_val_loss = val_loss / len(val_loader_weighted_quarter)\n  val_losses_quarter.append(avg_val_loss)\n  print(f\"Epoch {epoch + 1} / {num_epochs_weighted_quarter}: Validation Loss = {avg_val_loss:.4f}\")\n\n  if abs(best_val_loss - avg_val_loss) < 1e-3:\n    print(\"Early stopping triggered\")\n    break\n  elif avg_val_loss > best_val_loss:\n    print(\"Early stopping triggered\")\n    break\n  else:\n    best_val_loss = avg_val_loss\n    torch.save(weighted_model_quarter.state_dict(), \"best_cbow_model_weighted_quarter.pth\")\n\nweighted_model_quarter.load_state_dict(torch.load('best_cbow_model_weighted_quarter.pth'))\nweighted_model_quarter.eval()\n\ntest_loss_quarter = 0.0\ntest_accuracy_quarter = 0.0\ntop_3_accuracy_quarter, top_5_accuracy_quarter, top_10_accuracy_quarter = 0.0, 0.0, 0.0\n\nwith torch.no_grad():\n  for context, target in test_loader_weighted_quarter:\n    context, target = context.to(device), target.to(device)\n    output = weighted_model_quarter(context)\n    loss = criterion(output, target)\n    test_loss_quarter += loss.item()\n\n    predictions = torch.argmax(output, dim = 1)\n    test_accuracy_quarter += accuracy_score(target.cpu(), predictions.cpu())\n\n    top_k_accuracies = calculate_metrics(output, target, top_ks = [3, 5, 10])\n    top_3_accuracy_quarter += top_k_accuracies['top_3_accuracy']\n    top_5_accuracy_quarter += top_k_accuracies['top_5_accuracy']\n    top_10_accuracy_quarter += top_k_accuracies['top_10_accuracy']\n\navg_test_loss_quarter = test_loss_quarter / len(test_loader_weighted_quarter)\navg_test_accuracy_quarter = test_accuracy_quarter / len(test_loader_weighted_quarter)\navg_top_3_accuracy_quarter = top_3_accuracy_quarter / len(test_loader_weighted_quarter)\navg_top_5_accuracy_quarter = top_5_accuracy_quarter / len(test_loader_weighted_quarter)\navg_top_10_accuracy_quarter = top_10_accuracy_quarter / len(test_loader_weighted_quarter)\n\nend_time = time.time()\ntotal_time = end_time - start_time\n\nprint(f\"Test Loss: {avg_test_loss_quarter:.4f}\")\nprint(f\"Test Accuracy: {avg_test_accuracy_quarter:.4f}\")\nprint(f\"Top 3 Test Accuracy: {avg_top_3_accuracy_quarter:.4f}\")\nprint(f\"Top 5 Test Accuracy: {avg_top_5_accuracy_quarter:.4f}\")\nprint(f\"Top 10 Test Accuracy: {avg_top_10_accuracy_quarter:.4f}\")\nprint(f\"Total time taken to train and test: {total_time:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T07:59:14.334874Z","iopub.execute_input":"2024-12-27T07:59:14.335190Z","iopub.status.idle":"2024-12-27T08:04:12.700550Z","shell.execute_reply.started":"2024-12-27T07:59:14.335163Z","shell.execute_reply":"2024-12-27T08:04:12.699476Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 / 20: Training Loss =  6.3681\nEpoch 1 / 20: Validation Loss = 6.1191\nEpoch 2 / 20: Training Loss =  5.8572\nEpoch 2 / 20: Validation Loss = 6.0426\nEpoch 3 / 20: Training Loss =  5.6668\nEpoch 3 / 20: Validation Loss = 6.0033\nEpoch 4 / 20: Training Loss =  5.5332\nEpoch 4 / 20: Validation Loss = 5.9856\nEpoch 5 / 20: Training Loss =  5.4279\nEpoch 5 / 20: Validation Loss = 5.9758\nEpoch 6 / 20: Training Loss =  5.3400\nEpoch 6 / 20: Validation Loss = 5.9757\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-33-86c014116eb7>:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weighted_model_quarter.load_state_dict(torch.load('best_cbow_model_weighted_quarter.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 5.9860\nTest Accuracy: 0.1166\nTop 3 Test Accuracy: 0.2191\nTop 5 Test Accuracy: 0.2788\nTop 10 Test Accuracy: 0.3691\nTotal time taken to train and test: 298.31\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"Now we can move on to testing some intrinsic and extrinsic evaluators on our various models, and compare their respective performances against each of the evaluators.","metadata":{}},{"cell_type":"markdown","source":"# Intrinsic Evaluation #","metadata":{}},{"cell_type":"code","source":"model_full = CBOWModel(vocab_size_full, embedding_dim_full).to(device)\nmodel_half = CBOWModel(vocab_size_half, embedding_dim_half).to(device)\nmodel_quarter = CBOWModel(vocab_size_quarter, embedding_dim_quarter).to(device)\nweighted_model_full = CBOWModelWeighted(vocab_size_full, embedding_dim_weighted_full, 5).to(device)\nweighted_model_half = CBOWModelWeighted(vocab_size_half, embedding_dim_weighted_half, 5).to(device)\nweighted_model_quarter = CBOWModelWeighted(vocab_size_quarter, embedding_dim_weighted_quarter, 5).to(device)\n\nmodel_full.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_full.pth'))\nmodel_half.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_half.pth'))\nmodel_quarter.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_quarter.pth'))\nweighted_model_full.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_weighted_full.pth'))\nweighted_model_half.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_weighted_half.pth'))\nweighted_model_quarter.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_weighted_quarter.pth'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:16:15.479539Z","iopub.execute_input":"2025-01-21T11:16:15.479847Z","iopub.status.idle":"2025-01-21T11:16:20.003228Z","shell.execute_reply.started":"2025-01-21T11:16:15.479820Z","shell.execute_reply":"2025-01-21T11:16:20.002364Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-ff6fcdf9ba58>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_full.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_full.pth'))\n<ipython-input-14-ff6fcdf9ba58>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_half.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_half.pth'))\n<ipython-input-14-ff6fcdf9ba58>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_quarter.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_quarter.pth'))\n<ipython-input-14-ff6fcdf9ba58>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weighted_model_full.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_weighted_full.pth'))\n<ipython-input-14-ff6fcdf9ba58>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weighted_model_half.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_weighted_half.pth'))\n<ipython-input-14-ff6fcdf9ba58>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weighted_model_quarter.load_state_dict(torch.load('/kaggle/input/word-embedding-models/other/default/1/best_cbow_model_weighted_quarter.pth'))\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Word Similarity #\n\nFirst, we compare how well can our models recognise the similarity between words. Specific words are chosen based on how similar or dissimilar they are, and the similarities as predicted by the model will be compared with that from a human, to evaluate the performance of a model.","metadata":{}},{"cell_type":"code","source":"\"\"\"\ncommon_vocab is defined so that words can be chosen for our first intrinsic \nevaluator. We do not want to end up choosing a word that does not exist in \na model's vocabulary, as it will not be able to produce a suitable embedding.\n\"\"\"\n\ncommon_vocab = set(vocab_full.keys()) & set(vocab_half.keys()) & set(vocab_quarter.keys())\nprint(list(common_vocab)[1000:1500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T03:54:28.503382Z","iopub.execute_input":"2025-01-21T03:54:28.503669Z","iopub.status.idle":"2025-01-21T03:54:28.526695Z","shell.execute_reply.started":"2025-01-21T03:54:28.503647Z","shell.execute_reply":"2025-01-21T03:54:28.525758Z"}},"outputs":[{"name":"stdout","text":"['velociraptor', 'compressor', 'kneeling', 'rab', 'unspecified', 'dublin', 'prank', 'botched', 'perceptiveness', 'shoe', 'adamantly', 'wayland', 'despairing', 'pelvic', 'schuster', 'warmer', 'fourteenth', 'boise', 'schmidt', 'customize', 'faithful', 'furnish', 'greta', 'rolfe', 'colossus', 'slope', 'ap', 'webster', 'homeopathic', 'masse', 'pte', 'frightening', 'corel', 'fink', 'reconnected', 'stave', 'earthen', 'renege', 'breezy', 'aeroplane', 'gstad', 'masseuse', 'demand', 'helplessly', 'kappa', 'submission', 'frightens', 'tem', 'turns', 'violet', 'whoever', 'network', 'macey', 'habitation', 'chihuahua', 'battleship', 'torturing', 'greenhorn', 'argon', 'mongoose', 'mauro', 'heritage', 'pit', 'ood', 'ranulf', 'disconcert', 'staircase', 'presentable', 'pleasantly', 'portico', 'slocum', 'geopolitical', 'darkest', 'holl', 'riel', 'clich', 'cuesta', 'contrast', 'exterior', 'lsu', 'unsettling', 'vigilant', 'rommel', 'external', 'ceni', 'homey', 'fanfare', 'oppress', 'furze', 'unannounced', 'underestimated', 'ferr', 'cet', 'concrete', 'bath', 'chimney', 'consolation', 'juan', 'prc', 'agnostics', 'headwaiter', 'fre', 'partnership', 'psychopathic', 'swedish', 'ilford', 'harley', 'loosely', 'panoramic', 'sunshade', 'bradbury', 'disapproval', 'javier', 'kidder', 'jawing', 'default', 'kc', 'waistline', 'campagna', 'insinuation', 'ridiculous', 'toothpaste', 'delilah', 'sassy', 'tabby', 'taco', 'grating', 'beaumont', 'jacob', 'pedophile', 'hypnosis', 'interment', 'douche', 'truancy', 'mito', 'obsidian', 'cataract', 'beat', 'housemate', 'metaphor', 'matthia', 'washboard', 'potato', 'baggin', 'blazing', 'standish', 'www', 'uproar', 'confessing', 'wold', 'lightning', 'pre', 'hillbilly', 'shane', 'mossi', 'karat', 'conquistador', 'chivalry', 'office', 'pubis', 'cleanse', 'astronaut', 'queerly', 'sass', 'goddesse', 'vella', 'appease', 'ratchet', 'emily', 'unemployment', 'circuitous', 'skin', 'disconnected', 'uri', 'collateral', 'eld', 'raith', 'aidan', 'stanton', 'almagro', 'conroy', 'seguin', 'wager', 'slovenliness', 'mercantile', 'wiccan', 'claus', 'crackle', 'arrangement', 'spitfire', 'boulevard', 'authorization', 'withdraw', 'capability', 'cartographer', 'gettin', 'garu', 'westlake', 'arrogance', 'pl', 'debunk', 'swimsuit', 'mollie', 'slaughterer', 'glos', 'viejo', 'website', 'spurn', 'cf', 'amiable', 'introspective', 'kush', 'awoke', 'taking', 'conquered', 'erin', 'mandala', 'jacky', 'thir', 'gnaeus', 'ptsd', 'unscrew', 'wish', 'atman', 'mull', 'hazel', 'porno', 'atone', 'aldo', 'elegant', 'rj', 'negotiator', 'seating', 'gaius', 'indianapolis', 'altogether', 'pondering', 'explosion', 'confide', 'warr', 'carefully', 'masochist', 'orchestrate', 'portend', 'barechested', 'florence', 'ominously', 'nojima', 'compact', 'pleasantness', 'artificially', 'genealogy', 'languish', 'benny', 'cervical', 'fira', 'hungover', 'orient', 'tremulous', 'donut', 'sss', 'sentence', 'humour', 'tweaking', 'brandywine', 'pantheon', 'basal', 'kash', 'kac', 'overprotective', 'insistent', 'cursory', 'holstein', 'montez', 'churchyard', 'pilothouse', 'buttermilk', 'hacienda', 'tisza', 'divination', 'organic', 'mik', 'turnoff', 'bison', 'devourer', 'forgive', 'searcher', 'bandwagon', 'naru', 'charming', 'faulkner', 'traynor', 'accuse', 'nice', 'tolerably', 'stiletto', 'tendril', 'shua', 'fatalistic', 'regulator', 'cosmically', 'daffodil', 'deactivate', 'contrive', 'leaden', 'evermore', 'titian', 'sasquatch', 'dutiful', 'joyride', 'sinologist', 'tame', 'corps', 'disability', 'merry', 'distracted', 'aeronautical', 'northeast', 'homesteader', 'thudding', 'haine', 'killed', 'antarctica', 'undecided', 'fetched', 'web', 'through', 'gie', 'truncate', 'kenyan', 'perplexed', 'hestia', 'winner', 'voluptuous', 'experimentation', 'pda', 'stiffen', 'cy', 'offensive', 'curtis', 'mlle', 'surreal', 'pharmacy', 'butler', 'linkin', 'forester', 'hookup', 'logie', 'garter', 'janis', 'mater', 'noonday', 'clothespin', 'indecipherable', 'methodically', 'ghetto', 'rat', 'cranston', 'add', 'shake', 'laborer', 'misconception', 'corporation', 'eatery', 'ensnare', 'limo', 'vintage', 'malay', 'mousy', 'quietness', 'moccasin', 'tumbling', 'potent', 'unreliable', 'colonisation', 'koku', 'expediency', 'miff', 'ditch', 'caroline', 'definite', 'mace', 'coca', 'discouragement', 'abuse', 'naval', 'rockaway', 'dh', 'gator', 'pon', 'gage', 'mellon', 'upsetting', 'bolshevik', 'manpower', 'typo', 'startling', 'trois', 'godzilla', 'hillock', 'cc', 'edwina', 'insider', 'embassy', 'should', 'orphan', 'bevy', 'ferro', 'mtdna', 'austere', 'fodder', 'abroad', 'dupree', 'tertiary', 'nuke', 'reddish', 'coupe', 'surely', 'negate', 'familiar', 'oink', 'toyota', 'tubercular', 'bother', 'diego', 'thebe', 'lerner', 'seer', 'stays', 'compensation', 'bubonic', 'beverage', 'ncaa', 'lago', 'something', 'crosse', 'bacon', 'excavate', 'malnourished', 'quizzically', 'unconditionally', 'coldstream', 'martinez', 'bunt', 'chuckle', 'marihuana', 'crushed', 'didi', 'awfully', 'daly', 'vig', 'pout', 'tweet', 'spork', 'porridge', 'cryptic', 'featured', 'isha', 'peppermint', 'cornfield', 'ambrose', 'hypnotist', 'hombres', 'bull', 'basso', 'pillowcase', 'cantonment', 'paralyze', 'weir', 'facilitate', 'forked', 'snorkel', 'mentality', 'scorching', 'kincaid', 'recapture', 'mast', 'bareness', 'unhooked', 'space', 'steroid', 'dynasty', 'corpus', 'sworn', 'schmooze', 'utilize', 'rapt', 'coroner', 'survivable', 'politic', 'enon', 'ad', 'soothingly', 'santiago', 'ary', 'infinitesimal', 'gingerly', 'homeowner', 'brazil', 'thetis']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"custom_dataset = {\n    'word1': ['hot', 'fast', 'european', 'fedora', 'warrior', 'legitimate',\n             'alchemy', 'attendant', 'alligator', 'blogger'],\n    'word2': ['cold', 'swift', 'romanian', 'indefinitely', 'heroic', \n              'stench', 'treachery', 'foreman', 'viper', 'recurring'],\n    'similarity': [3.0, 9.0, 7.5, 0.5, 8.5, 1.0, 1.5, 5.0, 6.0, 1.0]\n}\ncustom_df = pd.DataFrame(custom_dataset)\n\ndef cosine_similarity(vec1, vec2):\n    return 1 - cosine(vec1, vec2)\n\nmodel_full_sims, model_half_sims, model_quarter_sims = [], [], []\nweighted_model_full_sims, weighted_model_half_sims, weighted_model_quarter_sims = [], [], []\nhuman_similarities = custom_df['similarity'].tolist()\n\nfor _, row in custom_df.iterrows():\n    word1, word2 = row['word1'], row['word2']\n\n    vec1_full = model_full.embeddings.weight.data[vocab_full[word1]].detach().cpu().numpy()\n    vec2_full = model_full.embeddings.weight.data[vocab_full[word2]].detach().cpu().numpy()\n\n    vec1_half = model_half.embeddings.weight.data[vocab_half[word1]].detach().cpu().numpy()\n    vec2_half = model_half.embeddings.weight.data[vocab_half[word2]].detach().cpu().numpy()\n\n    vec1_quarter = model_quarter.embeddings.weight.data[vocab_quarter[word1]].detach().cpu().numpy()\n    vec2_quarter = model_quarter.embeddings.weight.data[vocab_quarter[word2]].detach().cpu().numpy()\n\n    vec1_weighted_full = weighted_model_full.embeddings.weight.data[vocab_full[word1]].detach().cpu().numpy()\n    vec2_weighted_full = weighted_model_full.embeddings.weight.data[vocab_full[word2]].detach().cpu().numpy()\n\n    vec1_weighted_half = weighted_model_half.embeddings.weight.data[vocab_half[word1]].detach().cpu().numpy()\n    vec2_weighted_half = weighted_model_half.embeddings.weight.data[vocab_half[word2]].detach().cpu().numpy()\n\n    vec1_weighted_quarter = weighted_model_quarter.embeddings.weight.data[vocab_quarter[word1]].detach().cpu().numpy()\n    vec2_weighted_quarter = weighted_model_quarter.embeddings.weight.data[vocab_quarter[word2]].detach().cpu().numpy()\n\n    model_full_sims.append(cosine_similarity(vec1_full, vec2_full))\n    model_half_sims.append(cosine_similarity(vec1_half, vec2_half))\n    model_quarter_sims.append(cosine_similarity(vec1_quarter, vec2_quarter))\n    weighted_model_full_sims.append(cosine_similarity(vec1_weighted_full, vec2_weighted_full))\n    weighted_model_half_sims.append(cosine_similarity(vec1_weighted_half, vec2_weighted_half))\n    weighted_model_quarter_sims.append(cosine_similarity(vec1_weighted_quarter, vec2_weighted_quarter))\n\ncor_full, _ = spearmanr(human_similarities, model_full_sims)\ncor_half, _ = spearmanr(human_similarities, model_half_sims)\ncor_quarter, _ = spearmanr(human_similarities, model_quarter_sims)\ncor_weighted_full, _ = spearmanr(human_similarities, weighted_model_full_sims)\ncor_weighted_half, _ = spearmanr(human_similarities, weighted_model_half_sims)\ncor_weighted_quarter, _ = spearmanr(human_similarities, weighted_model_quarter_sims)\n\nprint(f\"Full Model Similarities: {model_full_sims}\")\nprint(f\"Full Model Correlation: {cor_full}\")\nprint(f\"Half Model Similarities: {model_half_sims}\")\nprint(f\"Half Model Correlation: {cor_half}\")\nprint(f\"Quarter Model Similarities: {model_quarter_sims}\")\nprint(f\"Quarter Model Correlation: {cor_quarter}\")\nprint(f\"Weighted Full Model Similarities: {weighted_model_full_sims}\")\nprint(f\"Weighted Full Model Correlation: {cor_weighted_full}\")\nprint(f\"Weighted Half Model Similarities: {weighted_model_half_sims}\")\nprint(f\"Weighted Half Model Correlation: {cor_weighted_half}\")\nprint(f\"Weighted Quarter Model Similarities: {weighted_model_quarter_sims}\")\nprint(f\"Weighted Quarter Model Correlation: {cor_weighted_quarter}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T16:10:39.277979Z","iopub.execute_input":"2025-01-20T16:10:39.278264Z","iopub.status.idle":"2025-01-20T16:10:39.313686Z","shell.execute_reply.started":"2025-01-20T16:10:39.278242Z","shell.execute_reply":"2025-01-20T16:10:39.312868Z"}},"outputs":[{"name":"stdout","text":"Full Model Similarities: [0.3399430909931195, 0.28245043586529617, 0.023994129743122206, 0.03516878845311133, 0.4297824273014339, -0.0038491602192494145, 0.11120815355511826, -0.24181547755315913, -0.006461446180797914, -0.06458320150433217]\nFull Model Correlation: 0.34650615989763683\nHalf Model Similarities: [0.34770953474812694, 0.28559212393206357, 0.04068693640485588, 0.07360373043302926, -0.07592316930435494, -0.11798983177013533, -0.10614448265349918, -0.005776699458077905, 0.15430099283416976, -0.08609664769202974]\nHalf Model Correlation: 0.395138603392042\nQuarter Model Similarities: [0.2846130109633893, 0.08480810775543524, -0.009338049132698645, -0.08692384261580832, 0.07326200578619846, -0.2855609230546936, -0.19492230520074538, 0.22572628631766056, 0.11301451113667849, -0.003532530839936321]\nQuarter Model Correlation: 0.480245379507251\nWeighted Full Model Similarities: [0.32550454944511964, 0.03939329796905766, 0.15609808905173783, -0.07950767559493999, -0.06753427974964987, -0.15546854793205056, 0.0668722757100102, 0.33151560412928816, 0.10645371436906592, 0.018045528134687516]\nWeighted Full Model Correlation: 0.37690143708164003\nWeighted Half Model Similarities: [0.3085783179562087, 0.011355610334337074, 0.09866761831219795, -0.10142812228537323, -0.04435557416304481, 0.07124510301401843, 0.03606027562890357, 0.17215041862690483, 0.05350108656452168, 0.014796300466240808]\nWeighted Half Model Correlation: 0.048632443494405174\nWeighted Quarter Model Similarities: [0.22695801203756816, 0.05236453468792179, -0.2067474281701389, -0.13548163095526355, 0.132681671648457, 0.0017029630211988778, -0.15749158893684134, 0.0014430933007077984, -0.0022186043241538833, 0.012513783444996651]\nWeighted Quarter Model Correlation: 0.21884599572482327\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Concept Categorisation #\n\nWe compare how well our models can categorise similar words into clusters, evaluating based on the homogeneity and completeness of clusters formed, using two metrics, V Measure score and Adjusted Rand Index (ARI).","metadata":{}},{"cell_type":"code","source":"words = ['alligator', 'viper', 'foreman', 'attendant', 'hallway', 'defibrillator',\n        'earthworm', 'lavatory', 'appraiser', 'nightstand']\ncategories = ['animal', 'animal', 'occupation', 'occupation', 'location',\n             'item', 'animal', 'location', 'occupation', 'item']\n\nembeddings_full = [model_full.embeddings.weight.data[vocab_full[word]].detach().cpu().numpy() for word in words]\nembeddings_half = [model_half.embeddings.weight.data[vocab_half[word]].detach().cpu().numpy() for word in words]\nembeddings_quarter = [model_quarter.embeddings.weight.data[vocab_quarter[word]].detach().cpu().numpy() for word in words]\nweighted_embeddings_full = [weighted_model_full.embeddings.weight.data[vocab_full[word]].detach().cpu().numpy() for word in words]\nweighted_embeddings_half = [weighted_model_half.embeddings.weight.data[vocab_half[word]].detach().cpu().numpy() for word in words]\nweighted_embeddings_quarter = [weighted_model_quarter.embeddings.weight.data[vocab_quarter[word]].detach().cpu().numpy() for word in words]\n\nn_clusters = len(set(categories))\nkmeans = KMeans(n_clusters = n_clusters, random_state = 42)\n\npredicted_labels_full = kmeans.fit_predict(embeddings_full)\npredicted_labels_half = kmeans.fit_predict(embeddings_half)\npredicted_labels_quarter = kmeans.fit_predict(embeddings_quarter)\npredicted_labels_weighted_full = kmeans.fit_predict(weighted_embeddings_full)\npredicted_labels_weighted_half = kmeans.fit_predict(weighted_embeddings_half)\npredicted_labels_weighted_quarter = kmeans.fit_predict(weighted_embeddings_quarter)\n\nvmeasure_full = v_measure_score(categories, predicted_labels_full)\nari_full = adjusted_rand_score(categories, predicted_labels_full)\nprint(f\"V Measure Full: {vmeasure_full}\")\nprint(f\"ARI Full: {ari_full}\")\n\nvmeasure_half = v_measure_score(categories, predicted_labels_half)\nari_half = adjusted_rand_score(categories, predicted_labels_half)\nprint(f\"V Measure Half: {vmeasure_half}\")\nprint(f\"ARI Half: {ari_half}\")\n\nvmeasure_quarter = v_measure_score(categories, predicted_labels_quarter)\nari_quarter = adjusted_rand_score(categories, predicted_labels_quarter)\nprint(f\"V Measure Quarter: {vmeasure_quarter}\")\nprint(f\"ARI Quarter: {ari_quarter}\")\n\nvmeasure_weighted_full = v_measure_score(categories, predicted_labels_weighted_full)\nari_weighted_full = adjusted_rand_score(categories, predicted_labels_weighted_full)\nprint(f\"V Measure Weighted Full: {vmeasure_weighted_full}\")\nprint(f\"ARI Weighted Full: {ari_weighted_full}\")\n\nvmeasure_weighted_half = v_measure_score(categories, predicted_labels_weighted_half)\nari_weighted_half = adjusted_rand_score(categories, predicted_labels_weighted_half)\nprint(f\"V Measure Weighted Half: {vmeasure_weighted_half}\")\nprint(f\"ARI Weighted Half: {ari_weighted_half}\")\n\nvmeasure_weighted_quarter = v_measure_score(categories, predicted_labels_weighted_quarter)\nari_weighted_quarter = adjusted_rand_score(categories, predicted_labels_weighted_quarter)\nprint(f\"V Measure Weighted Quarter: {vmeasure_weighted_quarter}\")\nprint(f\"ARI Weighted Quarter: {ari_weighted_quarter}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T16:26:55.313354Z","iopub.execute_input":"2025-01-20T16:26:55.313626Z","iopub.status.idle":"2025-01-20T16:26:55.397155Z","shell.execute_reply.started":"2025-01-20T16:26:55.313606Z","shell.execute_reply":"2025-01-20T16:26:55.396226Z"}},"outputs":[{"name":"stdout","text":"V Measure Full: 0.35007927152981044\nARI Full: -0.09223300970873786\nV Measure Half: 0.40946281657246997\nARI Half: 0.02476780185758514\nV Measure Quarter: 0.40946281657246997\nARI Quarter: 0.02476780185758514\nV Measure Weighted Full: 0.7031321488843297\nARI Weighted Full: 0.45047489823609227\nV Measure Weighted Half: 0.4160292481856192\nARI Weighted Half: -0.06418918918918919\nV Measure Weighted Quarter: 0.5739323787756515\nARI Weighted Quarter: 0.16923076923076924\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Extrinsic Evaluation # ","metadata":{}},{"cell_type":"markdown","source":"# Sentiment Analysis #\n\nFirst, we conduct some simple sentiment analysis, with a pre-trained dataset, the popular IMDB reviews dataset. This dataset come with pre-defined classes of positive and negative sentiment, and our models are tested on the accuracy at which they can predict the sentiment of the text.","metadata":{}},{"cell_type":"code","source":"url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\nresponse = requests.get(url, stream=True)\n\noutput_file = \"/kaggle/working/aclImdb_v1.tar.gz\"\nwith open(output_file, \"wb\") as f:\n    f.write(response.content)\n\nextract_to = \"/kaggle/working/aclImdb\"\nwith tarfile.open(output_file, \"r:gz\") as tar:\n    tar.extractall(path=extract_to)\n\nprint(\"Dataset downloaded and extracted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T03:54:41.041562Z","iopub.execute_input":"2025-01-21T03:54:41.041877Z","iopub.status.idle":"2025-01-21T03:55:14.010623Z","shell.execute_reply.started":"2025-01-21T03:54:41.041850Z","shell.execute_reply":"2025-01-21T03:55:14.009722Z"}},"outputs":[{"name":"stdout","text":"Dataset downloaded and extracted!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"data = load_files('/kaggle/working/aclImdb/aclImdb/train', categories = ['pos', 'neg'],\n                 encoding = 'utf-8', decode_error = 'ignore')\n\npos_revs = [data.data[i] for i in range(len(data.data)) if data.target[i] == 1]\nneg_revs = [data.data[i] for i in range(len(data.data)) if data.target[i] == 0]\n\nsamp_pos_revs = random.sample(pos_revs, 10000)\nsamp_neg_revs = random.sample(neg_revs, 10000)\n\ndf_sentiment = pd.DataFrame({\n    'review': samp_pos_revs + samp_neg_revs,\n    'label': [1] * 10000 + [0] * 10000\n})\ndf_sentiment = df_sentiment.sample(frac = 1).reset_index(drop = True)\n\nprint(df_sentiment.shape)\nprint(df_sentiment.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T03:55:22.744422Z","iopub.execute_input":"2025-01-21T03:55:22.744836Z","iopub.status.idle":"2025-01-21T03:55:23.648611Z","shell.execute_reply.started":"2025-01-21T03:55:22.744799Z","shell.execute_reply":"2025-01-21T03:55:23.647691Z"}},"outputs":[{"name":"stdout","text":"(20000, 2)\n                                              review  label\n0  Four teenage girlfriends drive to Fort Laurdal...      1\n1  I found 'Time At The Top' an entertaining and ...      1\n2  Got to be one of the best political satires I ...      1\n3  Another too bad the lowest they can go here is...      0\n4  American Pie has gone a long distance from the...      0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"X = df_sentiment['review']\ny = df_sentiment['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n\ndef encode_text(text, model, vocab):\n  tokens = text.split()\n  vectors = [\n      model.embeddings.weight.data[vocab[word]].detach().cpu().numpy() for word in tokens if word in vocab\n  ]\n  if len(vectors) == 0:\n    return np.zeros(100)\n  return np.mean(vectors, axis = 0)\n\nX_train_encoded_full = np.array([encode_text(review, model_full, vocab_full) for review in X_train])\nX_test_encoded_full = np.array([encode_text(review, model_full, vocab_full) for review in X_test])\nX_train_encoded_half = np.array([encode_text(review, model_half, vocab_half) for review in X_train])\nX_test_encoded_half = np.array([encode_text(review, model_half, vocab_half) for review in X_test])\nX_train_encoded_quarter = np.array([encode_text(review, model_quarter, vocab_quarter) for review in X_train])\nX_test_encoded_quarter = np.array([encode_text(review, model_quarter, vocab_quarter) for review in X_test])\nX_train_encoded_weighted_full = np.array([encode_text(review, weighted_model_full, vocab_full) for review in X_train])\nX_test_encoded_weighted_full = np.array([encode_text(review, weighted_model_full, vocab_full) for review in X_test])\nX_train_encoded_weighted_half = np.array([encode_text(review, weighted_model_half, vocab_half) for review in X_train])\nX_test_encoded_weighted_half = np.array([encode_text(review, weighted_model_half, vocab_half) for review in X_test])\nX_train_encoded_weighted_quarter = np.array([encode_text(review, weighted_model_quarter, vocab_quarter) for review in X_train])\nX_test_encoded_weighted_quarter = np.array([encode_text(review, weighted_model_quarter, vocab_quarter) for review in X_test])\n\nclassifier_full = LogisticRegression(random_state = 42)\nclassifier_full.fit(X_train_encoded_full, y_train)\ny_pred_full = classifier_full.predict(X_test_encoded_full)\nprint(f\"Full Model Accuracy: {accuracy_score(y_test, y_pred_full):.4f}\")\nprint(classification_report(y_test, y_pred_full))\n\nclassifier_half = LogisticRegression(random_state = 42)\nclassifier_half.fit(X_train_encoded_half, y_train)\ny_pred_half = classifier_full.predict(X_test_encoded_half)\nprint(f\"Half Model Accuracy: {accuracy_score(y_test, y_pred_half):.4f}\")\nprint(classification_report(y_test, y_pred_half))\n\nclassifier_quarter = LogisticRegression(random_state = 42)\nclassifier_quarter.fit(X_train_encoded_quarter, y_train)\ny_pred_quarter = classifier_quarter.predict(X_test_encoded_quarter)\nprint(f\"Quarter Model Accuracy: {accuracy_score(y_test, y_pred_quarter):.4f}\")\nprint(classification_report(y_test, y_pred_quarter))\n\nclassifier_weighted_full = LogisticRegression(random_state = 42)\nclassifier_weighted_full.fit(X_train_encoded_weighted_full, y_train)\ny_pred_weighted_full = classifier_weighted_full.predict(X_test_encoded_weighted_full)\nprint(f\"Weighted Full Model Accuracy: {accuracy_score(y_test, y_pred_weighted_full):.4f}\")\nprint(classification_report(y_test, y_pred_weighted_full))\n\nclassifier_weighted_half = LogisticRegression(random_state = 42)\nclassifier_weighted_half.fit(X_train_encoded_weighted_half, y_train)\ny_pred_weighted_half = classifier_weighted_half.predict(X_test_encoded_weighted_half)\nprint(f\"Weighted Half Model Accuracy: {accuracy_score(y_test, y_pred_weighted_half):.4f}\")\nprint(classification_report(y_test, y_pred_weighted_half))\n\nclassifier_weighted_quarter = LogisticRegression(random_state = 42)\nclassifier_weighted_quarter.fit(X_train_encoded_weighted_quarter, y_train)\ny_pred_weighted_quarter = classifier_weighted_quarter.predict(X_test_encoded_weighted_quarter)\nprint(f\"Weighted Quarter Model Accuracy: {accuracy_score(y_test, y_pred_weighted_quarter):.4f}\")\nprint(classification_report(y_test, y_pred_weighted_quarter))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T03:55:27.272767Z","iopub.execute_input":"2025-01-21T03:55:27.273055Z","iopub.status.idle":"2025-01-21T04:05:21.900571Z","shell.execute_reply.started":"2025-01-21T03:55:27.273034Z","shell.execute_reply":"2025-01-21T04:05:21.899017Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Full Model Accuracy: 0.7045\n              precision    recall  f1-score   support\n\n           0       0.70      0.70      0.70      2000\n           1       0.70      0.70      0.70      2000\n\n    accuracy                           0.70      4000\n   macro avg       0.70      0.70      0.70      4000\nweighted avg       0.70      0.70      0.70      4000\n\nHalf Model Accuracy: 0.5065\n              precision    recall  f1-score   support\n\n           0       0.50      0.95      0.66      2000\n           1       0.56      0.06      0.11      2000\n\n    accuracy                           0.51      4000\n   macro avg       0.53      0.51      0.38      4000\nweighted avg       0.53      0.51      0.38      4000\n\nQuarter Model Accuracy: 0.6893\n              precision    recall  f1-score   support\n\n           0       0.69      0.69      0.69      2000\n           1       0.69      0.69      0.69      2000\n\n    accuracy                           0.69      4000\n   macro avg       0.69      0.69      0.69      4000\nweighted avg       0.69      0.69      0.69      4000\n\nWeighted Full Model Accuracy: 0.7120\n              precision    recall  f1-score   support\n\n           0       0.71      0.71      0.71      2000\n           1       0.71      0.72      0.71      2000\n\n    accuracy                           0.71      4000\n   macro avg       0.71      0.71      0.71      4000\nweighted avg       0.71      0.71      0.71      4000\n\nWeighted Half Model Accuracy: 0.6973\n              precision    recall  f1-score   support\n\n           0       0.69      0.70      0.70      2000\n           1       0.70      0.69      0.70      2000\n\n    accuracy                           0.70      4000\n   macro avg       0.70      0.70      0.70      4000\nweighted avg       0.70      0.70      0.70      4000\n\nWeighted Quarter Model Accuracy: 0.6893\n              precision    recall  f1-score   support\n\n           0       0.69      0.68      0.69      2000\n           1       0.69      0.70      0.69      2000\n\n    accuracy                           0.69      4000\n   macro avg       0.69      0.69      0.69      4000\nweighted avg       0.69      0.69      0.69      4000\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Named Entity Recognition #\n\nWe do some simple checks on the performance of our models on NER tasks, using a pre-trained dataset, CoNLL-2003, another very popular dataset. Our models are then tested on how well they can classify information units.","metadata":{}},{"cell_type":"code","source":"ner_dataset = load_dataset('conll2003')\n\ntrain_data = ner_dataset['train']\ntest_data = ner_dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T11:16:41.280631Z","iopub.execute_input":"2025-01-21T11:16:41.280907Z","iopub.status.idle":"2025-01-21T11:16:50.167761Z","shell.execute_reply.started":"2025-01-21T11:16:41.280887Z","shell.execute_reply":"2025-01-21T11:16:50.167140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afe96be45ce4617b5606735f73d6340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf1d93e3417147e1866f63ba53ed7fb2"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0cbb583ffa4937a70b31b7addcd67b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd9129bc0cf4cbcaea4eb503695ad7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce6e31a8131491e87f574e6b95c5252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8982b799274b4b81bbc1d95a46c2f8"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def encode_sentence(tokens, model, vocab):\n    \"\"\"\n    Encodes each token in a sentence using a pre-trained model.\n    Returns individual token embeddings.\n    \"\"\"\n    embeddings = []\n    for token in tokens:\n        if token in vocab:\n            embedding = model.embeddings.weight.data[vocab[token]].detach().cpu().numpy()\n            embeddings.append(embedding)\n        else:\n            embeddings.append(np.zeros(model.embeddings.embedding_dim))  # Unknown tokens\n\n    return embeddings\n\nX_train_full, y_train_full = [], []\nX_train_half, y_train_half = [], []\nX_train_quarter, y_train_quarter = [], []\nX_train_weighted_full, y_train_weighted_full = [], []\nX_train_weighted_half, y_train_weighted_half = [], []\nX_train_weighted_quarter, y_train_weighted_quarter = [], []\n\nfor example in train_data:\n    tokens = example['tokens']\n    tags = example['ner_tags']\n    embeddings_full = encode_sentence(tokens, model_full, vocab_full)\n    embeddings_half = encode_sentence(tokens, model_half, vocab_half)\n    embeddings_quarter = encode_sentence(tokens, model_quarter, vocab_quarter)\n    embeddings_weighted_full = encode_sentence(tokens, weighted_model_full, vocab_full)\n    embeddings_weighted_half = encode_sentence(tokens, weighted_model_half, vocab_half)\n    embeddings_weighted_quarter = encode_sentence(tokens, weighted_model_quarter, vocab_quarter)\n    \n    X_train_full.extend(embeddings_full)\n    X_train_half.extend(embeddings_half)\n    X_train_quarter.extend(embeddings_quarter)\n    X_train_weighted_full.extend(embeddings_weighted_full)\n    X_train_weighted_half.extend(embeddings_weighted_half)\n    X_train_weighted_quarter.extend(embeddings_weighted_quarter)\n\n    y_train_full.extend(tags)\n    y_train_half.extend(tags)\n    y_train_quarter.extend(tags)\n    y_train_weighted_full.extend(tags)\n    y_train_weighted_half.extend(tags)\n    y_train_weighted_quarter.extend(tags)\n\nX_test_full, y_test_full = [], []\nX_test_half, y_test_half = [], []\nX_test_quarter, y_test_quarter = [], []\nX_test_weighted_full, y_test_weighted_full = [], []\nX_test_weighted_half, y_test_weighted_half = [], []\nX_test_weighted_quarter, y_test_weighted_quarter = [], []\n\nfor example in test_data:\n    tokens = example['tokens']\n    tags = example['ner_tags']\n    embeddings_full = encode_sentence(tokens, model_full, vocab_full)\n    embeddings_half = encode_sentence(tokens, model_half, vocab_half)\n    embeddings_quarter = encode_sentence(tokens, model_quarter, vocab_quarter)\n    embeddings_weighted_full = encode_sentence(tokens, weighted_model_full, vocab_full)\n    embeddings_weighted_half = encode_sentence(tokens, weighted_model_half, vocab_half)\n    embeddings_weighted_quarter = encode_sentence(tokens, weighted_model_quarter, vocab_quarter)\n    \n    X_test_full.extend(embeddings_full)\n    X_test_half.extend(embeddings_half)\n    X_test_quarter.extend(embeddings_quarter)\n    X_test_weighted_full.extend(embeddings_weighted_full)\n    X_test_weighted_half.extend(embeddings_weighted_half)\n    X_test_weighted_quarter.extend(embeddings_weighted_quarter)\n\n    y_test_full.extend(tags)\n    y_test_half.extend(tags)\n    y_test_quarter.extend(tags)\n    y_test_weighted_full.extend(tags)\n    y_test_weighted_half.extend(tags)\n    y_test_weighted_quarter.extend(tags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:26:01.636681Z","iopub.execute_input":"2025-01-21T12:26:01.637067Z","iopub.status.idle":"2025-01-21T12:26:28.293437Z","shell.execute_reply.started":"2025-01-21T12:26:01.637039Z","shell.execute_reply":"2025-01-21T12:26:28.292526Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"classifier_full = LogisticRegression(random_state = 42)\nclassifier_full.fit(X_train_full, y_train_full)\ny_pred_full = classifier_full.predict(X_test_full)\nprint(f\"Full Model Accuracy: {accuracy_score(y_test_full, y_pred_full):.4f}\")\nprint(classification_report(y_test_full, y_pred_full, zero_division = 0))\n\nclassifier_half = LogisticRegression(random_state = 42)\nclassifier_half.fit(X_train_half, y_train_half)\ny_pred_half = classifier_half.predict(X_test_half)\nprint(f\"Half Model Accuracy: {accuracy_score(y_test_half, y_pred_half):.4f}\")\nprint(classification_report(y_test_half, y_pred_half, zero_division = 0))\n\nclassifier_quarter = LogisticRegression(random_state = 42)\nclassifier_quarter.fit(X_train_quarter, y_train_quarter)\ny_pred_quarter = classifier_quarter.predict(X_test_quarter)\nprint(f\"Quarter Model Accuracy: {accuracy_score(y_test_quarter, y_pred_quarter):.4f}\")\nprint(classification_report(y_test_quarter, y_pred_quarter, zero_division = 0))\n\nclassifier_weighted_full = LogisticRegression(random_state = 42)\nclassifier_weighted_full.fit(X_train_weighted_full, y_train_weighted_full)\ny_pred_weighted_full = classifier_weighted_full.predict(X_test_weighted_full)\nprint(f\"Weighted Full Model Accuracy: {accuracy_score(y_test_weighted_full, y_pred_weighted_full):.4f}\")\nprint(classification_report(y_test_weighted_full, y_pred_weighted_full, zero_division = 0))\n\nclassifier_weighted_half = LogisticRegression(random_state = 42)\nclassifier_weighted_half.fit(X_train_weighted_half, y_train_weighted_half)\ny_pred_weighted_half = classifier_weighted_half.predict(X_test_weighted_half)\nprint(f\"Weighted Half Model Accuracy: {accuracy_score(y_test_weighted_half, y_pred_weighted_half):.4f}\")\nprint(classification_report(y_test_weighted_half, y_pred_weighted_half, zero_division = 0))\n\nclassifier_weighted_quarter = LogisticRegression(random_state = 42)\nclassifier_weighted_quarter.fit(X_train_weighted_quarter, y_train_weighted_quarter)\ny_pred_weighted_quarter = classifier_weighted_quarter.predict(X_test_weighted_quarter)\nprint(f\"Weighted Quarter Model Accuracy: {accuracy_score(y_test_weighted_quarter, y_pred_weighted_quarter):.4f}\")\nprint(classification_report(y_test_weighted_quarter, y_pred_weighted_quarter, zero_division = 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:26:45.445629Z","iopub.execute_input":"2025-01-21T12:26:45.445911Z","iopub.status.idle":"2025-01-21T12:27:54.121641Z","shell.execute_reply.started":"2025-01-21T12:26:45.445891Z","shell.execute_reply":"2025-01-21T12:27:54.120663Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Full Model Accuracy: 0.8247\n              precision    recall  f1-score   support\n\n           0       0.83      1.00      0.90     38323\n           1       0.00      0.00      0.00      1617\n           2       0.00      0.00      0.00      1156\n           3       0.00      0.00      0.00      1661\n           4       0.00      0.00      0.00       835\n           5       0.00      0.00      0.00      1668\n           6       0.00      0.00      0.00       257\n           7       0.00      0.00      0.00       702\n           8       0.00      0.00      0.00       216\n\n    accuracy                           0.82     46435\n   macro avg       0.09      0.11      0.10     46435\nweighted avg       0.68      0.82      0.75     46435\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Half Model Accuracy: 0.8247\n              precision    recall  f1-score   support\n\n           0       0.83      1.00      0.90     38323\n           1       0.00      0.00      0.00      1617\n           2       0.00      0.00      0.00      1156\n           3       0.00      0.00      0.00      1661\n           4       0.00      0.00      0.00       835\n           5       0.00      0.00      0.00      1668\n           6       0.00      0.00      0.00       257\n           7       0.00      0.00      0.00       702\n           8       0.00      0.00      0.00       216\n\n    accuracy                           0.82     46435\n   macro avg       0.09      0.11      0.10     46435\nweighted avg       0.68      0.82      0.75     46435\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Quarter Model Accuracy: 0.8246\n              precision    recall  f1-score   support\n\n           0       0.83      1.00      0.90     38323\n           1       0.00      0.00      0.00      1617\n           2       0.67      0.00      0.00      1156\n           3       0.00      0.00      0.00      1661\n           4       0.00      0.00      0.00       835\n           5       0.00      0.00      0.00      1668\n           6       0.00      0.00      0.00       257\n           7       0.00      0.00      0.00       702\n           8       0.00      0.00      0.00       216\n\n    accuracy                           0.82     46435\n   macro avg       0.17      0.11      0.10     46435\nweighted avg       0.70      0.82      0.75     46435\n\nWeighted Full Model Accuracy: 0.8213\n              precision    recall  f1-score   support\n\n           0       0.82      1.00      0.90     38323\n           1       0.00      0.00      0.00      1617\n           2       0.00      0.00      0.00      1156\n           3       0.00      0.00      0.00      1661\n           4       0.00      0.00      0.00       835\n           5       0.00      0.00      0.00      1668\n           6       0.00      0.00      0.00       257\n           7       0.00      0.00      0.00       702\n           8       0.00      0.00      0.00       216\n\n    accuracy                           0.82     46435\n   macro avg       0.09      0.11      0.10     46435\nweighted avg       0.68      0.82      0.74     46435\n\nWeighted Half Model Accuracy: 0.8219\n              precision    recall  f1-score   support\n\n           0       0.82      1.00      0.90     38323\n           1       0.00      0.00      0.00      1617\n           2       0.00      0.00      0.00      1156\n           3       0.00      0.00      0.00      1661\n           4       0.00      0.00      0.00       835\n           5       0.00      0.00      0.00      1668\n           6       0.00      0.00      0.00       257\n           7       0.00      0.00      0.00       702\n           8       0.00      0.00      0.00       216\n\n    accuracy                           0.82     46435\n   macro avg       0.09      0.11      0.10     46435\nweighted avg       0.68      0.82      0.74     46435\n\nWeighted Quarter Model Accuracy: 0.8211\n              precision    recall  f1-score   support\n\n           0       0.82      0.99      0.90     38323\n           1       0.00      0.00      0.00      1617\n           2       0.13      0.00      0.00      1156\n           3       0.00      0.00      0.00      1661\n           4       0.00      0.00      0.00       835\n           5       0.00      0.00      0.00      1668\n           6       0.00      0.00      0.00       257\n           7       0.00      0.00      0.00       702\n           8       0.00      0.00      0.00       216\n\n    accuracy                           0.82     46435\n   macro avg       0.11      0.11      0.10     46435\nweighted avg       0.68      0.82      0.74     46435\n\n","output_type":"stream"}],"execution_count":25}]}